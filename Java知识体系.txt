 Java:Java是一个面向对象高级编程语言，通过Java编译器生成字节码文件在计算机里运行。
与C++不同，Java运用Java Virtual Machine（JVM）执行Java字节码（二进制形式），并通过JVM在多台机器上运行。
正因为Java运用了JVM在计算机上运行，并弱化了指针，因此Java不适合操作系统层面的源码编写，Java的应用在应用程序和web层面比较广泛。

Java官方文档：https://www.w3cschool.cn/java/dict

基本语法：关键字和保留字、标识符、变量、运算符、程序流程控制 
关键字：Java关键字是电脑语言里事先定义的，有特别意义的标识符，有时又叫保留字，还有特别意义的变量。
Java的关键字对Java的编译器有特殊的意义，他们用来表示一种数据类型，或者表示程序的结构等，关键字不能用作变量名、方法名、类名、包名和参数。
1、48个关键字：
用于定义类、函数、变量修饰符的关键字：
abstract、final、static、synchronized
用于定义类与类之间关系的关键字：
extends、implements
用于定义建立实例及引用实例、判断实例的关键字：
new、this、super、instanceof
用于异常处理的关键字：
try catch finally throw throws
用于包的关键字：
package import
其他修饰符关键字：
native、assert、transient、volatile、strictfp 
*用于定义数据类型值的字面值：
true、false、null。

2、Java保留字是指现有Java版本尚未使用 但以后版本可能会作为关键字使用，是C语言里用到但是Java里没有用到的关键字。
goto、const。

3、3个特殊直接量：true、false、null。

4、标识符：Java标识符由数字，字母和下划线（_），美元符号（$）或人民币符号（￥）组成。
在Java中是区分大小写的，而且还要求首位不能是数字。最重要的是，Java关键字不能当作Java标识符。
简单来说就是，在Java中凡是可以自己起名字的地方都叫标识符。

4.1 Java中的名称命名规范：
包名：多单词组成时所有字母都小写：xxxyyyzzz
类名、接口名：多单词组成时，所有单词的首字母大写：XxxYyyZzz
变量名、方法名：多单词组成时，第一个单词首字母小写，
第二个单词开始每个单词首字母大写：xxxYyyZzz
常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ

5、变量
概念：本质上来说变量是内存中的一小块区域，通过变量名来访问这块区域。因此，使用每一个变量前必须要先申请（声明）然后必须对其进行赋值，才能使用

按照数据类型来分：
基本数据类型（primitive type,在栈stack中）：
整数类型：byte（1字节）、short（2字节）、int（4字节）、long（8字节）
浮点类型：float（4字节）、double（8字节）
整数类型默认为int、浮点类型默认为double。
字符型： char（使用Unicode编码、每个字符占两个字节，8个bite一个字节）
布尔型： boolean（与C语言布尔类型取0和1不同，只能取true和false）
引用数据类型（reference type,在堆heap中）：
类（字符串定义在类里）
接口
数组
（ *若想让基础类型存放在堆中，可以将基础类型包装为一个对象。将会用到基础类型的包装类）

按照声明的位置来分：
局部变量：方法体内，或者语句块内部定义的变量（局部变量不会默认初始化、必须先声明并赋值）
成员变量：方法体之外，类体中（需要先声明、赋值，若没有先赋值、则默认初始化后再使用）

基本数据类型大小：
类型 占用存储空间     表数范围
byte  1字节 = 8bit位   -128~127
short 2字节            -2^15~2^15-1
int   4字节            -2^31~2^31-1
long  8字节            -2^63~2^63-1
float 4字节 		   -3.40E+38 ~ +3.40E+38
double 8字节		   -1.79E+308 ~ +1.79E+308
注意：
bit : 计算机最小存储单位    byte：计算机中基本存储单元
整数类型默认为int、浮点类型默认为double。

字符型数据类型：
char用单引号'',String用双引号"";
String类型是字符串数据类型，用来存储多个字符，char类型只能存储一个字符。
String可以和8种数据类型做运算，运算的结果仍为String类型。
String是一个引用数据类型，但同时是一个final类，可以用字面量的定义方式去赋值。
通过字面量的方式（区别于new）赋值，会声明在常量池中。而常量池中不会存在相同内容的字符串。
所以，用==去比较两个相同内容的字符串，会返回true。

String代表不可变的字符串序列，称作"不可变性"
>当对字符串重新赋值时，需要重新写内存区域
>当对现有的字符串进行连接操作时，需要重新写内存区域赋值，不能在原有的基础上赋值
>当调用String的replace方法时，也需要重新写内存区域赋值，不能在原有的基础上赋值
>String字符串拼接的方式，常量和常量拼接的结果在常量池；只要其中有一个是变量，结果就在堆中；
 如果拼接的结果使用intern()方法，返回值在常量池中

String的实例化方式：
1.通过字面量的方式，声明在方法区中，两个相同的字符串返回true（即使是在对象中通过字面量定义的两个相同的字符串，返回的也是true）
2.通过new的方式，声明在堆中，两个相同的对象对比返回false

String与char之间的转换：
String → char ：使用String中的toCharArray()
char → String ：调用String中的构造器

String与byte之间的转换：
String → char ：使用String中的getByte()
char → String ：调用String中的构造器

不同于String是不可变的字符串序列，StringBuffer和StringBuilder是可变的字符串序列。
String：不可变的字符串序列
StringBuffer ：可变的字符串序列，线程安全，效率低，多线程时用
StringBuilder ：可变的字符串序列，线程不安全，效率高

String可以直接赋值或者创建对象
String s1 = "Hello"; // 通过直接赋值创建String对象  
String s2 = new String("Hello"); // 通过new关键字创建String对象

StringBuffer是Java中的一个可变类，它允许我们在同一个对象上进行多次的字符串操作。
StringBuffer sb = new StringBuffer(); // 创建一个空的StringBuffer对象  
sb.append("Hello"); // 添加字符串  
sb.append(" World"); // 添加另一个字符串  
System.out.println(sb.toString()); // 输出 "Hello World"

StringBuilder：StringBuilder和StringBuffer类似，也是Java中的一个可变类，允许我们在同一个对象上进行多次的字符串操作。
StringBuilder sb = new StringBuilder(); // 创建一个空StringBuilder对象  
sb.append("Hello"); // 添加字符串  
sb.append(" World"); // 添加另一个字符串  
System.out.println(sb.toString()); // 输出 "Hello World"

创建StringBuffer时，底层创建了一个长度为16的字符串char型数组；
如果创建时传了参数，则在此基础上增加一个长度为16的字符串char型数组，默认情况下，扩容为原来的2倍+2，同时将原有数组的元素复制到新的数组中

基础数据类型的转换：
容量小的数据类型与容量大的数据类型运算，结果自动转换为容量大的数据类型。
当byte,short,char做运算时，结果必须是int类型。
byte,short,char -> int -> long -> float -> double

容量大的转换为容量小的：强制类型转换
注意：强制类型转换，可能导致精度损失。
// Widening
// byte<short<int<long<float<double
int i = 10;
long l = i;               // 10
// Narrowing 
double d = 10.02;
long l = (long)d;         // 10
String.valueOf(10);       // "10"
Integer.parseInt("10");   // 10
Double.parseDouble("10"); // 10.0

6、在Java当中，运算符可以分为：算术运算符(+ - * /)、 关系运算符(< > ==)、逻辑运算符、位运算符、移位运算符以及条件运算符等。
一、算术运算符
1，基本的四则运算：加减乘除模(+ - * / %)
注意点：

（1）这些运算符都是二元运算符，使用时必须要有左右两个操作数。

int a = 20;
int b = 10;
System.out.println(a + b); // 30
System.out.println(a - b); // 10
System.out.println(a * b); // 200
System.out.println(a / b); // 2
System.out.println(a % b); // 0 --->模运算相当于数学中除法的余数

（2）同C语言一样，int / int的结果还是int，而且会向下取整。

要出现小数点，那就转成double类型或在最后*1.0。

（3）除法和取模操作时，右操作数不能为0，否则会报出异常。

（4）%在Java中不但可以对整数进行取模，还可以对double进行取模操作。

（5）两边操作数不相同的时候，会发生类型提升。看一个特例：

对两个short类型进行相加，在用short进行接受，发现报错，提示是从int到short可能会有损失。
这是因为为了计算的方便，Java在将小于4个字节的类型进行计算的时候，会将其隐形提升到int类型。
上面两个short均被提升到int，在用short接收，就会报错。解决办法是进行强制类型转换。

2，增量运算符 = += -= *= %=
不会改变变量本身的数据类型。该种类型运算符操作完成后，会将操纵的结果赋值给左操作数。要注意只有变量才可以使用该运算符，常量不允许被修改，不能使用。

3，自增/自减运算符 ++/--
不会改变变量本身的数据类型。这两种运算符有前置和后置之分。如果是单独使用，那么前置和后置是没有区别的，如果是混合使用：

int a = 1;
a++; // 后置++ 表示给a的值加1，此时a的值为2
System.out.println(a++); // 注意：后置++是先使用变量原来值，表示式结束时再给变量+1，因此输出2
System.out.println(a); // 输出3
++a; // 前置++ 表示给a的值加1
System.out.println(++a); // 注意：前置++是先给变量+1，然后使用变量中的值，再使用表达式，因此输出5
System.out.println(a); // 输出5

混合使用，【前置++】先+1，然后使用变量+1之后的值，【后置++】先使用变量原来的值，表达式结束时给变量+1 只有变量才能使用自增/自减运算符，常量不能使用，因为常量不允许被修改。

二、关系运算符
主要有六个: == != < > <= >=，其计算结果是 true 或者 false 。在Java中，只有true和false，不存在0表示假，非0表示真。

当需要多次判断时，不能连着写，比如：3 < a < 5，在C语言当中，是可以运行的，但是在Java当中会报错，需要写成3 < a &&  a < 5。

在验证用户登录时判定用户名不是null而且不是空字符串，应当写为：username != null &&!username.equals(“”)，二者的顺序不能交换。
原因：
在Java中，&&是逻辑与运算符，它有一个重要的特性：短路行为。这意味着如果第一个操作数为false，那么整个表达式的结果就确定为false，而第二个操作数（即右侧的条件）将不会被评估。这个特性在防止NullPointerException时非常有用。
如果我们将条件交换为!username.equals("") && username != null，那么当username为null时，!username.equals("")这个条件会首先被评估，这将导致NullPointerException，因为null不能调用任何方法（包括equals方法）。
而username != null这个条件原本可以阻止这种情况的发生，但由于它现在位于第二个位置，它已经没有机会阻止NullPointerException了。


==和equals的区别？
==：运算符
如果比较的是基本数据类型：比较两个变量保存的数据是否相同（不一定类型要相同）
如果比较的是引用数据类型：比较两个变量的地址值是否相同，是否指向同一个对象实体

用==比较两个String类型的数据：
在Java中，使用==操作符来比较两个String类型的对象时，并不会导致编译错误，因为==是Java的一个有效操作符，它可以用于比较基本数据类型（如int、char等）的值，也可以用于比较引用数据类型的引用值（即，它们是否指向内存中的同一位置）。
然而，对于String类型的比较，使用==通常不是最佳实践，因为String是一个不可变类，并且Java虚拟机（JVM）为了优化字符串的使用，会在字符串常量池中缓存字符串字面量。
这意味着，当你创建两个内容相同的字符串字面量时，JVM可能会让它们指向常量池中的同一个对象。但是，如果你通过new String("...")来创建字符串对象，或者字符串是通过其他非字面量的方式（如字符串连接、字符串函数返回值等）得到的，那么即使内容相同，这些字符串对象也可能位于内存中的不同位置。
因此，当你使用==来比较两个String对象时，你实际上是在比较它们的引用是否相同，而不是它们的内容是否相同。这通常不是你想要的，特别是当你不确定这些字符串是否来自常量池，或者是否通过new关键字显式创建时。
为了比较两个String对象的内容是否相同，应该使用equals()方法。equals()方法被重写（Override）在String类中，以比较两个字符串的内容是否完全一致。这是比较字符串内容的正确方式。

equals(): 方法
只能适用于引用数据类型
Object类中equals（）的定义：Object类中定义的equals()和==的作用是相同的，比较两个对象的地址值
public boolean equals(Object obj){
	return (this == obj)
}
像String、Date、File、包装类等都重写了Object类中的equals()方法，比较两个对象的实体内容

在Java8中String类重写的equals()方法：
public boolean equals(Object anObject) {  
    if (this == anObject) {  
        return true;  
    }  
    if (anObject instanceof String) {  
        String anotherString = (String)anObject;  
        int n = value.length;  
        if (n == anotherString.value.length) {  
            char v1[] = value;  
            char v2[] = anotherString.value;  
            int i = 0;  
            while (n-- != 0) {  
                if (v1[i] != v2[i])  
                    return false;  
                i++;  
            }  
            return true;  
        }  
    }  
    return false;  
}


通常情况下，自定义的类需要重写equals()方法
自定义类重写equals()方法的步骤一般如下：
1、先用“==”判断是否相等。
2、判断equals()方法的参数是否为null，如果为null，则返回false；因为当前对象不可能为null，如果为null，则不能调用其equals()方法，否则抛java.lang.NullPointerException异常。
3、当参数不为null，则如果两个对象的运行时类（通过getClass()获取）不相等，返回false，否则继续判断。


Java比较器：比较对象的大小
Comparable 的使用举例：
1.像String、包装类等实现了Comparable接口，重写了CompareTo()方法，给出了比较两个对象大小的方法
2.像String、包装类重写CompareTo()方法以后，进行了从小到大的排列
3.重写CompareTo(obj)的规则：
	如果当前对象this大于形参对象obj，则返回正整数；
	如果当前对象this小于形参对象obj，则返回负整数；
	如果当前对象this等于形参对象obj，则返回零；

Comparator定制排序：
当元素的类型没有实现Comparable接口而又不方便修改代码，或者实现了Comparable
接口的排序规则不适合当前的操作，可以考虑使用Comparator的对象来排序


三、逻辑运算符
逻辑运算符主要有六个: &  &&  |  || ! ^，运算结果都是boolean类型。

1、逻辑与 & 短路与 &&
语法规则：表达式1 && 表达式2，左右表达式必须是boolean类型的结果。相当于数学中的且。
短路与遵守短路求值的规则。即表达式1为假，表达式2就不再执行。逻辑与继续执行表达式2。

2、逻辑或 | 短路或 ||
语法规则：表达式1 || 表达式2，左右表达式必须是boolean类型的结果。相当于数学中的或。
短路或遵守短路求值的规则。即表达式1为真，表达式2就不再执行。逻辑或继续执行表达式2。

|| 遵守短路求值的规则。即表达式1为真，表达式2就不在执行，否则，执行表达式2。

& 和 | 也可以进行逻辑运算。如果他们的表达式为boolean，也表示逻辑运算，但是和&& ||相比不支持短路求值。

3、逻辑非 !
语法规则：!表达式1，相当于数学中的非。

4、逻辑异或
语法规则：表达式1^表达式2，左右表达式必须是boolean类型的结果。当表达式1和表达式2相同时为true，不同时为false。


四、位运算符
数据存储的最小单位是字节，而数据操作的最小单位是比特位。
字节是最小的存储单位，每个字节是由8个二进制比特位组成的，多个字节组合在一起可以表示各种不同的数据。
位运算表示按照二进制的每一位进行运算。
位运算符的左右两边是数值类型。

1，按位与&
如果两个二进制位都是 1，则结果为 1，否则结果为 0。

int a = 10;
int b = 20;
System.out.println(a & b); // 0


2，按位或|
如果两个二进制位有一个是1，则结果是1，否则都是0，结果是0。 

int a = 10;
int b = 20;
System.out.println(a | b); //30

 3，按位异或^
如果两个二进制位相同，则结果是0，否则是1。

4，按位取反~
如果该二进制位是1，则变成0，是0，变成1。

5，移位运算
    Java和C语言不同的地方在于，Java多了一个>>>，表示无符号右移。但是没有无符号左移。Java的移位运算符有三个: <<，>>，>>> 。都是二元运算符，且都是按照二进制比特位来运算的。

（1）左移：<<最左侧位不要了N位，在最右侧补零。左移 1 位，相当于原数字 * 2。左移 N 位，相当于原数字 * 2 的N次方。

（2）右移：>>最右侧不要了N位，在最左侧如果是正数，补0，负数补1。右移 1 位，相当于原数字 / 2。右移 N 位，相当于原数字 / 2 的N次方。

（3）无符号右移：>>>最右侧位不要了，最左侧补0。被移位二进制最高位无论是0还是1，空缺位都拿0补。

（4）移动负数位或者移位位数过大都没有意义。

    计算机在进行运算的时候，实际上是按照二进制运算的。加减等在运算的时候被转化成二进制的形式进行运算。计算机计算移位效率高于计算乘除，比如当某个代码正好乘除 2 的N次方的时候可以用移位运算代替。

6，条件运算符
条件运算符只有一个：表达式1 ? 表达式2 : 表达式3。
当表达式1为true时，执行表达式2，表达式3不在执行，否则执行表达式2。这个是Java当中唯一的一个三目运算符。表达式2和表达式3的结果需要同类型的，表达式不能单独存在。
int a = 10;
int b = 20;
int max = (a > b) ? a : b;
// 输出: 20
System.out.println(max);

7，在Java程序中，JVM默认总是顺序执行以分号;结束的语句。但是，在实际的代码中，程序经常需要做条件判断、循环，因此，需要有多种流程控制语句，来实现程序的跳转和循环等功能。
流程控制语句是用来控制程序中各语句执行顺序的语句，可以把语句组合成能完成一定功能的小逻辑模块。

if..else if..else：条件判断语句。
int j = 10;
if (j == 10) {
  System.out.println("I get printed");
} else if (j > 10) {
  System.out.println("I don't");
} else {
  System.out.println("I also don't");
}

for：循环语句，限定循环体的执行次数
for (int i = 0; i < 10; i++) {
  System.out.print(i);
}
// 输出: 0123456789

//增强for循环
int[] numbers = {1,2,3,4,5};
for (int number: numbers) {
  System.out.print(number);
}
// 输出: 12345

while：循环语句，反复执行语句或代码块，直到条件不满足时跳出
int count = 0;
while (count < 5) {
  System.out.print(count);
  count++;
}
// 输出: 01234

//do while循环
int count = 0;
do {
  System.out.print(count);
  count++;
} while (count < 5);
// 输出: 01234

switch..case:根据switch表达式中的值，依次匹配各个case中的常量。
一旦匹配成功，进入case结构，调用其执行语句。如果没有break关键字，则继续执行其他case结构的表达式。
int month = 3;
String str;
switch (month) {
  case 1:
    str = "January";
    break;
  case 2:
    str = "February";
    break;
  case 3:
    str = "March";
    break;
  default:
    str = "Some other month";
    break;
}
// 输出: Result March
System.out.println("Result " + str);

break和continue关键字：
1、break用于跳出一个循环体或者完全结束一个循环，不仅可以结束其所在的循环，还可结束其外层循环。
注意：
（1）只能在循环体内和switch语句体内使用break。
（2）不管是哪种循环，一旦在循环体中遇到break，系统将完全结束循环，开始执行循环之后的代码。
（3）当break出现在循环体中的switch语句体内时，起作用只是跳出该switch语句体，并不能终止循环体的执行。若想强行终止循环体的执行，可以在循环体中，但并不在switch语句中设置break语句，满足某种条件则跳出本层循环体。
for (int i = 0; i < 5; i++) {
  System.out.print(i);
  if (i == 3) {
    break;
  }
}
// 输出: 0123

2、continue语句的作用是跳过本次循环体中剩下尚未执行的语句，立即进行下一次的循环条件判定，可以理解为只是中止(跳过)本次循环，接着开始下一次循环。
注意：
（1）continue语句并没有使整个循环终止。
（2）continue 只能在循环语句中使用，即只能在 for、while 和 do…while 语句中使用。
for (int i = 0; i < 5; i++) {
  if (i == 3) {
    continue;
  }
  System.out.print(i);
}
// 输出: 01245

7、注释、文档注释
注释的内容不参与编译。
文档注释：注释内容可以被jdk提供的工具javadoc解析，生成一套网页文件形式体现的该程序的说明文档。
注意：javadoc解析的类要加上public

// 我是单行注释！
 
/*
而我是一个
多行注释！
*/
/**
  * 这个
  * 是
  * 文档
  * 注释
 */

8、注解
Annotation(注解)也被称为元数据(Metadata)是JDK1.5及以后版本引入的，用于修饰解释 包、类、方法、属性、构造器、局部变量等数据信息。
它可以用于创建文档，跟踪代码中的依赖性，甚至执行基本编译时检查。
注解是以‘@注解名’在代码中存在的，根据注解参数的个数，我们可以将注解分为：标记注解、单值注解、完整注解三类。
和注释一样，注解不影响程序逻辑，但注解可以被编译或运行，相当于嵌入在代码中的补充信息。
另外，你可以在编译时选择代码里的注解是否只存在于源代码级，或者它也能在class文件、或者运行时中出现（SOURCE/CLASS/RUNTIME）。
在 JavaSE 中，注解的使用目的比较简单，例如标记过时的功能，忽略警告等。
在 JavaEE 中注解占据了更重要的角色，例如用来配置应用程序的任何切面，代替 java EE 旧版中所遗留的繁冗代码和 XML 配置等。
 
元注解：标注在一个注解上，用于定义一个注解的状态
@Documented - 一个简单的标记注解，它标识了是否将注解添加到 Javadoc 中。

@Retention - 定义应保留注解的时间。
RetentionPolicy.SOURCE 在编译期间丢弃。这些注解在编译完成后没有任何意义，因此它们不会被写入字节码。例子：@Override, @SuppressWarnings
RetentionPolicy.CLASS – 在类加载期间丢弃。应用在进行字节码级别的编译期间。有些令人惊讶的是，这是默认的。
RetentionPolicy.RUNTIME – 不会丢弃。该注解可以在运行时进行反射。这是我们通常用于自定义注解的内容。

@Target - 注解可以使用的地方。如果不指定这一属性，注解可以应用在任何地方。
以下是该注解的有效值。这里的一个要点，它只有包含的形式，这意味着如果您想要对7个属性进行注解，
并且只想排除一个属性，这时需要在定义目标时包含所有7个属性。 

@Documented: 用于指定被该元注解修饰的 Annotation 类将被javadoc 工具提取成文档，即在生成文档时，可以看到该注解。
注意：
定义为@Documented 的注解必须设置Retention值为RUNTIME。

@Inherited 
被@Inherited 修饰的注解 将具有继承性，如果某个类使用了被 @Inherited修饰的注解，则其子类将自动具有该注解

注解的注意事项：
▷ 注解仅存在于源码中，在class字节码文件中不包含
▷ 默认的保留策略，注解会在class字节码文件中存在，但运行时无法获得，
▷ 注解会在class字节码文件中存在，在运行时可以通过反射获取到
▷ 首先要明确生命周期长度 SOURCE < CLASS < RUNTIME ，前者能作用的地方后者一定也能作用。
        ①：一般如果需要在 运行时去动态获取注解信息，那只能用 RUNTIME 注解；
        ②：如果要在 编译时进行一些预处理操作，比如生成一些辅助代码（如 ButterKnife），就用 CLASS注解；
        ③：如果只是做一些 检查性的操作如 @Override 和 @SuppressWarnings，则可选用 SOURCE 注解。


9、Java数组（Array）：数组是多个相同类型的数据按照一定顺序排列的集合，并使用一个名字命名，并通过编号的方式对这些数据进行统一处理。

数组的声明：数组的长度一旦确定，就不能修改。
数组有一维数组和二维数组。
//声明 Declare
int[] a1;
int[] a2 = {1, 2, 3};
int[] a3 = new int[]{1, 2, 3};
int[] a4 = new int[3];
a4[0] = 1;
a4[2] = 2;
a4[3] = 3;

数组的初始化：数组的初始化有两种形式，静态初始化和动态初始化。
静态初始化:数组的初始化和数组的元素赋值同时进行 
ids = new int[]{11,22,12,12,12,123};
ids = {11,22,12,12,12,123};
动态初始化：数组的初始化和数组的元素赋值分开进行
String[] names = new String[5];

注意：数组一旦初始化完成，数组长度就确定了

数组元素通过角标调用。从第一个角标0开始，一直到-1。
通过数组的属性length获取数组的长度。
数组元素的默认初始化值：
>数组元素是整型：0
>数组元素是浮点型：0.0
>数组元素是char型：0或'\u0000' 而非'0'
>数组元素是Boolean型：false
>数组元素是引用数据类型: null

二维数组的初始化：
静态初始化：
int [][] arr = new int [][]{{1,2,3},{4,5},{6,7,8}};
int []arr [] = new int [][]{{1,2,3},{4,5},{6,7,8}};
int [][] arr = {{1,2,3},{4,5},{6,7,8}};
动态初始化：
String [][] arr2 = new String[5][3]; 
String [][] arr2 = new String[5][];

10、Java异常处理
Java的异常处理是Java编程语言中用于处理运行时错误的一个重要机制。它允许程序员定义在程序中可能发生的异常情况，并在这些情况发生时提供相应的处理逻辑。
Java的异常处理基于三个关键字：try、catch 和 finally，以及 throw 和 throws。

异常类
在Java中，异常是一种对象，它是Throwable类或其子类的实例。Throwable类有两个主要的子类：Error和Exception。

Error：表示系统级错误或资源耗尽的情况，通常应用程序无法处理这些错误。例如，OutOfMemoryError。
Exception：表示可以由应用程序捕获并处理的异常情况。Exception类又进一步分为RuntimeException（运行时异常）和检查型异常（checked exceptions）。

异常处理的基本结构
异常处理的基本结构包括try、catch和finally块。

try块：包含可能引发异常的代码。
catch块：用于捕获并处理try块中抛出的异常。可以有多个catch块来处理不同类型的异常。

finally块：无论是否发生异常，finally块中的代码都会被执行。它通常用于释放资源，如关闭文件或数据库连接。
示例：
try {  
    // 尝试执行的代码，可能会抛出异常  
    int result = 10 / 0; // 这将引发ArithmeticException  
} catch (ArithmeticException e) {  
    // 处理ArithmeticException  
    System.out.println("除数不能为0");  
} catch (Exception e) {  
    // 处理其他类型的异常（如果有的话）  
    System.out.println("发生了一个异常：" + e.getMessage());  
} finally {  
    // 无论是否发生异常，都会执行的代码  
    System.out.println("这是finally块");  
}

抛出异常
使用throw关键字可以手动抛出一个异常。这通常用于在检测到错误情况时通知调用者。

示例：

public void checkInput(int input) throws IllegalArgumentException {  
    if (input < 0) {  
        throw new IllegalArgumentException("输入不能为负数");  
    }  
    // 其他代码...  
}

声明异常
如果一个方法可能会抛出异常，但该方法本身不处理该异常，那么它应该使用throws关键字声明它可能会抛出的异常。这样，调用该方法的代码就需要处理这些异常，或者继续声明它们。


示例：
public void readFile(String filePath) throws IOException {  
    // 读取文件的代码，可能会抛出IOException  
    // ...  
}

自定义异常
除了使用Java内置的异常类外，程序员还可以定义自己的异常类。这通常用于表示特定于应用程序的错误情况。自定义异常类应该继承自Exception类或其子类。


Java编程范式-面向对象：
"万事万物皆对象"
1.在Java语言中，我们把功能、结构等封装到类中，通过类的实例化，来调用具体的功能结构
2.涉及到Java语言与前端HTML或者后端数据库做交互时，都体现为用类、对象去做交互
面向对象特点：封装、继承、多态
面向对象包括：类、对象、属性、方法、成员变量、局部变量、修饰符、方法的重载、继承、方法的重写、多态、static关键字、abstract关键字、接口 → JVM调优，设计模式，web和大数据框架

类：是一组相关属性和行为的集合。可以看成是一类事物的模板，使用事物的属性特征和行为特征来描述该类事物；如：人类

实例对象：是一类事物的具体体现。对象是类的一个实例（对象并不是找个女朋友），必然具备该类事物的属性和行为；如：小红、小明、小南

Scanner scanner = new Scanner();
具有静态描述的特征称之为属性
具有动态动作的行为(做事情)称之为方法
创建类的对象 = 类的实例化 = 实例化类
匿名对象：创建的对象没有显式的赋给一个变量名。匿名对象只能调用一次。
new Phone().sendEmail();
匿名对象的使用：将匿名对象的值赋值给方法的形参
mall.show(new Phone())

成员变量（属性，非static的）：
位置：在方法外部，直接写在类当中
作用范围：整个类全都可以通用
默认值：根据其类型，有默认值
内存位置：位于堆内存
生命周期：随着对象创建而诞生，随着对象被垃圾回收而消失
可以使用权限修饰符

局部变量：
位置：在方法内部
作用范围：只有方法当中才可以使用，出了方法就不能再用
默认值：无默认值
内存位置：位于栈内存
生命周期：随着方法进栈而诞生，随着方法出栈而消失
不可以使用权限修饰符

public class MyClass {  
    // 成员变量  
    public String myMemberVariable;  
      
    public void myMethod() {  
        // 局部变量  
        String myLocalVariable = "Hello, World!";  
    }  
}

修饰符：private：同类下访问  → 缺省：同包下访问 → protected：子包下可以访问，同包也可以访问。不同包下的子类，访问protected修饰的，需要使用子类的对象访问，不能使用父类的对象访问  → public ：项目中所有类都可以访问
PS:修饰类的时候，只能用缺省和public。

方法的重载：在 Java 类中可以定义 多个名称相同的方法 ，若只是参数不同，功能类似，那么可以将这类方法称为同名方法，也可以叫做方法重载
例如：sort(byte[] a) 与 sort(char[] a) 构成了方法的重载
Java中可变个数的形参 jdk5.0以上 格式：public void show(String ... strs){}
可变个数的形参与相同的数组之间不构成重载，二者不能共存，编译器认为这两种方法没有区别。

Java里面值传递机制只有一种：值传递。即将实际参数值的副本（复制）传入方法内，而参数本身不受影响。
>形参是基本数据类型，将实参的数据值传递给形参；形参是引用数据类型，将实参的地址值（含变量的数据类型）传递给形参。

Java中的值传递机制是指在传递基本数据类型的参数时，实际是将参数的值复制一份传递给方法或函数，而不是传递参数本身。这意味着在方法或函数内部对参数的修改不会影响到原始变量的值。
具体来说，当我们将基本类型的变量传递给方法时，实际上是将变量的值复制一份传递给方法，而不是传递变量本身。这意味着在方法内部对参数的修改不会影响到原始变量的值。

例如，如果我们定义一个方法来交换两个整数的值：
public static void swap(int x, int y) {  
    int temp = x;  
    x = y;  
    y = temp;  
}
当我们调用swap(a, b)时，实际传递的是a和b的值，而不是a和b本身。这意味着在swap方法内部对x和y的修改不会影响到原始的a和b的值。因此，调用swap(a, b)后，a的值不会改变，而b的值会被交换。

PS：String很奇特，虽然是引用数据类型，但是采用的却是值传递！！！


封装（数据的隐藏）：
通常，应禁止直接访问一个对象中数据的实际表示，而是应该通过操作接口来访问，这叫信息隐藏。
程序设计中的“高内聚，低耦合”：我们在程序设计的过程中要追求“高内聚，低耦合”。
高内聚就是类的内部数据操作细节自己来完成，不允许外部干涉，低耦合：就是， 仅暴露少量的方法给外部使用

封装的体现：
①将类的属性私有化，同时提供公共的(public)方法来获取(get)和设置(set)
②将类的方法私有化，仅用于内部方法调用
③单例模式

以下是一个示例，展示如何在Java中编写一个私有类的get和set方法，体现Java的封装性：

public class Person {  
    private String name;  
    private int age;  
  
    public Person(String name, int age) {  
        this.name = name;  
        this.age = age;  
    }  
  
    // Getter方法  
    public String getName() {  
        return name;  
    }  
  
    public int getAge() {  
        return age;  
    }  
  
    // Setter方法  
    public void setName(String name) {  
        this.name = name;  
    }  
  
    public void setAge(int age) {  
        if (age > 0) {  
            this.age = age;  
        } else {  
            System.out.println("年龄必须大于0");  
        }  
    }  
}
在上面的示例中，我们定义了一个名为Person的类，其中包含两个私有属性：name和age。我们使用构造函数来初始化这两个属性。然后，我们提供了getter方法来获取属性的值，以及setter方法来设置属性的值。
注意，setter方法只允许我们设置年龄为大于0的值，否则会输出一条错误消息。这就是Java封装性的体现，我们将属性封装在类内部，并通过getter和setter方法来控制对属性的访问。

构造器（构造方法）：
在Java中，构造器（Constructor）是一种特殊的方法，用于在创建对象时初始化对象的状态。构造器与类同名，并且没有返回类型（包括void）。当使用new关键字创建类的一个新对象时，构造器会自动被调用。
构造器的主要目的是为对象分配内存并初始化其状态。它可以接收参数，并根据这些参数设置对象的属性。

构造器的声明：
1.如果没有显式的定义的构造器，则系统默认定义空参的构造器
2.定义构造器的格式：权限修饰符 类名（形参列表）{}
3.一个类中可以有多个形参构造器，彼此构成重载
4.一旦我们显式的定义了构造器之后，系统就不再提供默认的空参构造器
5.一个类中，至少要有一个构造器

构造器的作用：
1.创建对象
2.初始化对象的属性

空参构造器：Person person = new Person();

带参构造器：
public Person(String name, int age) {  
    this.name = name;  
    this.age = age;  
}  


Java属性赋值的先后顺序：
1.默认初始化
2.显式初始化
3.构造器中初始化
4.通过 "对象.方法" 或 "对象.属性" 的方式初始化

在Java中，new一个对象是给怎么样的一个过程？

在Java中，创建一个对象主要经历了以下几个步骤：

1.加载类：首先，Java虚拟机（JVM）需要加载类。这个过程包括解析类的字节码，确定类的字段和方法，并为类的静态变量分配内存。
为实例变量分配内存：在类被加载后，JVM会为实例变量（即非静态字段）分配内存。

2.初始化实例变量：这个步骤会为实例变量赋予默认值。所有的数值类型默认值为0，所有的引用类型默认值为null。
如果类中有实例变量的初始化语句，或者从父类或接口中继承的实例变量初始化，那么这些初始化语句会被执行。

3.创建对象：接下来，使用new关键字来创建对象。这个过程包括为对象分配内存，复制构造函数的参数值到相应的实例变量，
调用构造函数来初始化对象，最后返回新创建对象的引用。

4.使用对象：现在，你可以使用这个新创建的对象来进行各种操作，比如调用方法、访问字段等。

Java之this关键字：
在Java中，this 关键字是一个引用变量，它引用当前对象（即调用方法或构造器的那个对象）。

1.this可以修饰属性、方法、构造器
2.this修饰属性和方法
>this可以理解为当前对象
>在类的方法或类的构造器中，this修饰属性或方法时，一般情况下省略。特殊情况下，如果方法的形参和类的属性同名时，
必须显式使用"this.属性"声明此变量是属性，而非形参。
>this可以修饰构造器，结构为"this.形参列表"
>构造器中，不能通过方式"this.形参列表"调用自己
>如果一个类中有n个构造器，最多有n-1个构造器使用了"this.形参列表"方式
>规定："this.形参列表"必须声明在构造器的首行
>构造器内部，只能声明一个"this.形参列表"调用其他构造器

public class MyClass {  
    private int x;  
    private int y;  
  
    public MyClass() {  
        this(0, 0); // 使用 this 调用另一个构造器  
    }  
  
    public MyClass(int x, int y) {  
        this.x = x;  
        this.y = y;  
    }  
}


public class MyClass {  
    private int x;  
  
    public void setX(int x) {  
        this.x = x; // 使用 this 引用当前对象  
    }  
}

Java之package关键字：
在Java中，package 关键字用于声明一个类所在的包（package）。

1.为了更好的实现类的管理，提供包的概念
2.声明在原文件的首行
3.每"."一次，就代表一层文件目录
注意：相同或不同包下，不能引入同名的类、接口

Java之import关键字：
在Java中，import 关键字用于导入类或其他类型，使得在当前Java文件中可以不必使用完全限定名（即包括包名）就可以引用它们。使用 import 可以使代码更简洁，更容易阅读。

1.在源文件中显式的使用import结构导入指定包下的类、接口
2.声明在包的声明和类的声明之间
3.使用"xxx.*"的形式，表示可以导入xxx包下的所有结构
4.如果使用的类或接口是Java.lang包下定义的，则可以省略import结构
5.如果使用的类或接口是本包下定义的，则可以省略import结构

Java面向对象之继承：在Java中，继承（Inheritance）是一种面向对象编程（OOP）的核心概念，它允许一个类（称为子类或派生类）获取另一个类（称为父类或基类）的属性和方法。
继承已存在的类就是复用这些类的方法，而且可以增加一些新的方法和字段，使新类能够适应新的情况，继承是Java程序设计中一项核心技术，它主要解决的问题是：共性的抽取，实现代码复用。

Java中表现继承的关系需要借助关键字extends，子类继承父类。Class A extends B(){} A:子类、派生类、subclass B：父类、超类、基类、superclass

Java继承的特点：
· 子类将继承父类的成员变量和成员方法。特别的，父类中声明为private的属性或方法，子类继承后，仍然认为子类获取了父类中私有的结构。
· Java中只支持单继承和多继承，不允许多重继承（C++允许多重继承）
· 子类继承父类之后，需要添加自己特有的成员，体现出与基类的不同
· 如果访问的成员变量子类中有，则优先访问子类本身的
· 如果访问的成员变量子类中无，父类中有，则访问继承下来的
· 如果子类与父类中有同名的成员变量，则优先访问子类自己的，即子类将父类的同名变量隐藏
· 如果我们没有显式的声明一个类的父类，则该类继承于Object类。Object类是所有类的父类

// 基类（父类）  
public class Animal {  
    private String name;  
  
    public Animal(String name) {  
        this.name = name;  
    }  
  
    public void eat() {  
        System.out.println(name + " is eating...");  
    }  
}  
  
// 子类（狗类）  
public class Dog extends Animal {  
    private String breed;  
  
    public Dog(String name, String breed) {  
        super(name);  // 调用父类的构造方法  
        this.breed = breed;  
    }  
  
    public void bark() {  
        System.out.println(getName() + " is barking...");  
    }  
  
    public String getBreed() {  
        return breed;  
    }  
}

继承之super关键字：它主要用于引用子类的父类（或称为超类、基类）的成员（包括属性和方法）。
1.super可以用来调用属性、方法、构造器。
2.通过使用"super.属性"或"super.方法"的形式在子类中显式的调用父类的属性和方法
>所有类都继承Object类，任何类用super都可以调用Object类。
3.如果子类重写了父类的方法，此时子类中想调用父类被重写的方法。必须显式的使用"super.方法"的形式调用父类被重写的方法
4.在子类中显式的使用"super(形参列表)"调用父类中声明的构造器
>在子类构造方法中，super(形参列表)调用父类构造时，必须是子类构造方法中的第一条语句
>super(形参列表)只能在子类的构造方法中出现一次，并不能和this同时出现
>在类的多个构造器中，至少有一个子类调用了父类的空参构造器
>若父类显式定义无参或者默认的构造方法，在子类构造方法的第一行默认有隐含的super调用，即调用基类的构造方法
 构造哪个类的对象，就调用哪个类的构造方法，调用构造方法时先调用基类，在调用子类(即在子类中隐藏super())
>如果父类的构造方法是带有参数的，此时编译器不会给子类生成默认的构造方法，此时需要用户在子类中显式定义构造方法，并在子类构造方法中选取合适的父类构造方法调用

// 父类  
public class Parent {  
    protected int num;  
  
    public Parent(int num) {  
        this.num = num;  
    }  
  
    public int getNum() {  
        return num;  
    }  
}  
  
// 子类  
public class Child extends Parent {  
    private int num;  
  
    public Child(int num) {  
        super(num); // 调用父类的构造器  
        this.num = num * 2;  
    }  
  
    public int getNum() {  
        return super.getNum() * 2; // 调用父类的方法  
    }  
}

继承之执行顺序：

无继承关系时的执行顺序：
静态代码块先执行，且只执行一次，在类加载阶段执行
当有对象创建时，才会执行实例代码块，实例代码块执行完后，再执行构造方法

有继承关系时的执行顺序：
· 父类静态代码块优先子类静态代码块执行，都是最早执行的
· 父类实例代码块和父类构造方法紧接着执行
· 子类的实例代码块和子类构造方法在接着执行
· 第二次实例化对象时，父类和子类的静态代码块都不会在执行 

继承方式：单继承、多继承、不同的类继承同一个类（不支持一个类继承不同的父类）
final关键字修饰类不能被继承。

Java中的object类：
Object类是Javajava.lang包下的核心类，Object类是所有类的父类，何一个类时候如果没有明确的继承一个父类的话，那么它就是Object的子类；

1. clone()
保护方法，实现对象的浅复制，只有实现了Cloneable接口才可以调用该方法，否则抛出CloneNotSupportedException异常。

2. getClass()
final方法，返回Class类型的对象，反射来获取对象。

3. toString()
该方法用得比较多，一般子类都有覆盖，来获取对象的信息。

4. finalize()
该方法用于释放资源。因为无法确定该方法什么时候被调用，很少使用。

5. equals()
比较对象的内容是否相等

6. hashCode()
该方法用于哈希查找，重写了equals方法一般都要重写hashCode方法。这个方法在一些具有哈希功能的Collection中用到。

7. wait()
wait方法就是使当前线程等待该对象的锁，当前线程必须是该对象的拥有者，也就是具有该对象的锁。wait()方法一直等待，直到获得锁或者被中断。wait(long timeout)设定一个超时间隔，如果在规定时间内没有获得锁就返回。

调用该方法后当前线程进入睡眠状态，直到以下事件发生。

其他线程调用了该对象的notify方法。
其他线程调用了该对象的notifyAll方法。
其他线程调用了interrupt中断该线程。
时间间隔到了。
此时该线程就可以被调度了，如果是被中断的话就抛出一个InterruptedException异常。

8. notify()
该方法唤醒在该对象上等待的某个线程。

9. notifyAll()
该方法唤醒在该对象上等待的所有线程。


Java方法的重写（Override）：
在Java中，方法的重写（Overriding）是面向对象编程中的一个重要概念，它允许子类提供一个与父类相同方法签名（即方法名、参数列表和返回类型都相同）但实现不同的方法。
当子类对象调用该方法时，将执行子类中的方法实现，而不是父类中的方法。

方法重写的规则：

1.参数列表必须完全与被重写方法相同；也就是说，在被重写的方法里，输入参数必须与父类的参数相同
>父类的成员方法只能被它的子类重写。
>构造方法不能被重写
>如果不能继承一个方法，则不能重写这个方法。

2.返回类型必须完全与被重写方法的返回类型相同

3.子类重写父类的方法，权限修饰符不能大于父类被重写的方法
>父类中的访问权限是protected，那么，重写的的权限不能为public
>父类中的访问权限是private，子类不能重写父类
>子类和父类在同一个包中，那么子类可以重写父类所有除了声明为private和final的方法。
 子类和父类不在同一个包中，那么子类只能够重写父类的声明为public和protect的非final方法。
 
4.重写方法不能抛出新的检查异常或者比被重写方法申明更加宽泛的异常。
例如：父类的一个方法声明了一个检查异常IOException,但是在重写这个方法的时候不能抛出Exception异常，
因为Exception是IOException的父类，只能抛出IOException的子类异常。

>重写的方法能够抛出任何非强制异常，无论被重写的方法是否抛出异常。
但是，重写方法不能抛出新的强制性异常，或者比被重写方法声明的更广泛的强制性异常，反之则可以。

5.声明为static的方法不能被重写，但是能够被再次声明（可以被继承）;声明为final的方法不能被重写。

// 父类  
public class Animal {  
    public void makeSound() {  
        System.out.println("The animal makes a sound");  
    }  
}  
  
// 子类（狗类）  
public class Dog extends Animal {  
    @Override  // 使用@Override注解表示该方法是一个重写方法  
    public void makeSound() {  
        System.out.println("The dog barks");  
    }  
}

Java多态：
在Java中，多态（Polymorphism）是面向对象编程的三大特性之一，它允许不同的对象对同一消息做出不同的响应。
换句话说，多态性意味着一个引用变量（如方法参数、返回值或实例变量）可以指向多种实际类型，并在运行时确定到底应该执行哪个类的方法。这种特性使得代码更加灵活、可扩展和可重用。

在编译期，只能调用父类中声明的方法，但在执行期，实际执行的是子类中重写父类的方法。(编译，看左边；运行，看右边)

>多态性不适用于属性，只适用于方法。(属性的编译和运行都是谁声明，谁调用)
>多态性不能调用子类特有的方法，编译时，对象是父类类型

如何在内存的层面理解多态性？
在 Java 内存层面，多态性中子类对象指向父类对象的行为是通过类型转换实现的。
在内存中，子类对象是父类对象的一个实例，因此它们都存储在堆内存中。
子类对象是父类对象的完整副本，并且拥有自己的方法和属性。由于子类对象是父类对象的完整副本，因此它们都可以被转换为父类对象的引用。
因此，我们可以在程序中使用父类对象的引用指向子类对象，这种行为称为向上转型。

当向上转型时，我们仍然可以使用父类对象的引用访问子类对象中的方法和属性，
但是如果父类对象没有定义这些方法和属性，则不能访问。
多态性允许我们在编译时使用父类对象的引用，而在运行时实际访问的是子类对象中的方法和属性。
这种行为允许我们在不改变现有代码的情况下实现子类特定的功能。

实现多态的条件，缺一不可：
1.子类继承父类
2.子类必须要对父类中方法进行重写

对于重载而言，在方法调用之前，编译器就确定了要调用的方法，这是"早绑定";
对于多态，在方法调用之后，编译器才确定要调用的方法，这是"晚绑定"

Java instanceof 关键字：
判断对象a是否是类A的一个实例，如果是，返回true(如果是多态对象，也会返回true);如果不是，返回false
使用instanceof关键字避免多态对象强制转化出现ClassCastException异常。

// 父类  
public class Shape {  
    public void draw() {  
        System.out.println("Drawing a shape...");  
    }  
}  
  
// 子类（圆形类）  
public class Circle extends Shape {  
    @Override  // 使用@Override注解表示该方法是一个重写方法  
    public void draw() {  
        System.out.println("Drawing a circle...");  
    }  
}  
  
// 子类（矩形类）  
public class Rectangle extends Shape {  
    @Override  // 使用@Override注解表示该方法是一个重写方法  
    public void draw() {  
        System.out.println("Drawing a rectangle...");  
    }  
}  
  
// 主类  
public class Main {  
    public static void main(String[] args) {  
        Shape[] shapes = new Shape[2];  // 创建一个Shape类型的数组，并分配两个空间  
        shapes[0] = new Circle();  // 将第一个空间指向Circle类的对象  
        shapes[1] = new Rectangle();  // 将第二个空间指向Rectangle类的对象  
          
        for (Shape shape : shapes) {  // 循环遍历数组中的每一个对象，并调用它们的draw()方法  
            shape.draw();  // 由于多态的存在，这个方法将调用Circle类和Rectangle类各自的draw()方法，而不是Shape类的draw()方法  
        }  
    }  
}
在上面的例子中，Shape 类是一个父类，它有一个 draw() 方法。Circle 和 Rectangle 类是两个子类，它们分别继承了 Shape 类并重写了 draw() 方法。
在 Main 类的 main() 方法中，首先创建了一个 Shape 类型的数组，并分别将两个子类的对象赋值给数组中的两个空间。
然后，使用一个for-each循环遍历数组中的每一个对象，并调用它们的 draw() 方法。由于多态的存在，这个方法将调用每个子类自己的 draw() 方法，而不是父类的 draw() 方法。


Java 包装类：
针对八种基本数据类型定义相应的引用类型（具有类的特征）

基本数据类型和包装类的转换：
基本数据类型→包装类：调用包装类的构造器;自动装箱
调用方法举例：Integer的valueOf(i)方法

包装类→基本数据类型：调用包装类的xxxValue();自动拆箱
调用方法举例：Integer的intValue()方法

(自动装箱和自动拆箱是jdk5.0以后的特性)

基本数据类型、包装类转换为String类型：
1.连接运算 String str1 = num1 + "";
2.调用String的valueOf()方法 String str2 = String.valueOf();

String类型转换为基本数据类型、包装类：
调用包装类的静态方法 parseXXX

Java static关键字：
Java static关键字是一个修饰符，它用于声明属性（变量）、方法、代码块、内部类和嵌套类，用于定义类级别的变量、方法和代码块，以支持各种与类本身相关但不依赖于类实例的功能和模式。

static修饰方法：
static修饰的方法一般称作静态方法，由于静态方法不依赖于任何对象就可以进行访问，因此对于静态方法来说，是没有this的，因为它不依附于任何对象，既然都没有对象，就谈不上this了。
并且由于这个特性，在静态方法中不能访问类的非静态成员变量和非静态成员方法，因为非静态成员方法/变量都是必须依赖具体的对象才能够被调用。
但是要注意的是，虽然在静态方法中不能访问非静态成员方法和非静态成员变量，但是在非静态成员方法中是可以访问静态成员方法/变量的。

public class StaticMethodExample {  
    // 静态方法  
    public static int add(int a, int b) {  
        return a + b;  
    }  
  
    public static void main(String[] args) {  
        // 直接通过类名调用静态方法  
        int sum = StaticMethodExample.add(5, 3);  
        System.out.println("The sum is: " + sum);  
    }  
}

static变量：
static修饰的属性也称作静态变量，静态变量和非静态变量的区别是：
静态变量被所有的对象所共享，声明在方法区，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化。
而非静态变量是对象所拥有的，在创建对象的时候被初始化，声明在堆中，存在多个副本，各个对象拥有的副本互不影响。
static成员变量的初始化顺序按照定义的顺序进行初始化。

public class SharedVariableExample {  
    // 静态变量  
    public static int sharedVariable;  
  
    public static void main(String[] args) {  
        // 创建两个实例  
        SharedVariableExample example1 = new SharedVariableExample();  
        SharedVariableExample example2 = new SharedVariableExample();  
  
        // 修改静态变量的值  
        example1.sharedVariable = 10;  
  
        // 打印静态变量的值，可以看到它在这两个实例中是相同的  
        System.out.println("example1.sharedVariable: " + example1.sharedVariable);  
        System.out.println("example2.sharedVariable: " + example2.sharedVariable);  
    }  
}

static代码块：
(代码块的作用：用来初始化类、对象）
static关键字还有一个比较关键的作用就是 用来形成静态代码块以优化程序性能。static块可以置于类中的任何地方，类中可以有多个static块。
在类初次被加载的时候，会按照static块的顺序来执行每个static块，并且只会执行一次。
因此，很多时候会将一些只需要进行一次的初始化操作都放在static代码块中进行。静态代码块的执行顺序按照声明的前后顺序执行。
静态代码块的执行要优于非静态代码块的执行，而且静态代码块只能调用静态的属性、方法，不能调用非静态的结构。

static关键字的用法：
1.static用来修饰属性（静态变量）
静态属性属于类本身，而不是类的实例。也就是说，静态属性是类级别的，它们存储在静态存储区中，生命周期与JVM的运行时长一致。
静态属性可以在没有创建类实例的情况下访问，通过类名直接访问，例如 ClassName.staticVariableName。
静态属性在内存中只有一份拷贝（节省内存），无论创建多少个对象，静态属性都只有一份。

2.static用来修饰方法（静态函数）
静态方法属于类本身，而不是类的实例。它们可以在没有创建类实例的情况下调用，通过类名直接调用，例如 ClassName.staticMethodName()。
静态方法只能访问静态属性或其他静态方法，因为它们不能依赖于类的对象状态。
静态方法中不能使用非静态属性和非静态方法（除非在方法内部创建类的实例）。

3.static用来修饰代码块
静态代码块在类加载时执行，且只执行一次。它通常用于初始化静态属性。
静态代码块按照它们在类中出现的顺序执行。
静态代码块可以访问类的静态属性和静态方法，但不能访问非静态属性和非静态方法。

4.static用来修饰内部类和嵌套类
静态内部类（也称为嵌套类）是定义在另一个类中的类，并使用static关键字修饰。静态内部类不需要外部类的实例即可创建实例。
静态内部类可以访问外部类的静态属性和静态方法，但不能直接访问外部类的非静态属性和非静态方法（除非创建外部类的实例）。

public class StaticBlockExample {  
    // 静态变量  
    public static int staticVariable;  
  
    // 静态代码块  
    static {  
        System.out.println("This is a static block.");  
        staticVariable = 10;  
    }  
  
    public static void main(String[] args) {  
        // 创建两个实例  
        StaticBlockExample example1 = new StaticBlockExample();  
        StaticBlockExample example2 = new StaticBlockExample();  
  
        // 打印静态变量的值，可以看到它在这两个实例中是相同的  
        System.out.println("example1.staticVariable: " + example1.staticVariable);  
        System.out.println("example2.staticVariable: " + example2.staticVariable);  
    }  
}

PS：开发中，如何确定一个属性是否要声明为static？
①属性是可以被多个对象所共享的
②操作静态属性的方法，通常设计成static的
③工具类中的方法，习惯上声明为static的


Java final关键字
在Java中，final关键字是一个修饰符，它可以用于变量、方法和类，表示这些实体是“最终的”或“不可变的”。

final关键字的用法：
1.用来修饰类
>此类不能被其他类继承

2.用来修饰方法
>方法不能被重写

3.final用来修饰变量
>此时修饰的"变量"被称为常量
>final修饰属性，可以考虑修饰的位置有：显示初始化、代码块中初始化、构造器中初始化
>final修饰局部变量，修饰形参时，表示此形参是一个常量。一旦赋值此形参，不能重新赋值
>final修饰属性，用来当作全局常量

4.final用来修饰类
>不能被继承：如果一个类被final修饰，那么这个类不能被其他类继承。即，没有子类可以继承这个final类。
>不能被扩展：如果一个类被final修饰，那么这个类的所有子类都不能被其他类继承。
>所有成员变量、方法和内部类默认也是final：当一个类被final修饰时，它的所有成员变量、方法和内部类默认也是final的。
这意味着这些成员不能被覆盖或重写。
>用于安全和性能优化：由于final类的不可变性，它常用于创建工具类或驱动程序，比如驱动器接口或一些不需要修改的基础类。
这样就可以防止其他人更改类的行为或结构，同时也可以提高程序的性能，因为编译器可以对其进行更多优化。

PS:被final修饰的类不允许被继承，但是可以实例化

        final class Foo {
            private final int value;

            public Foo(int value) {
                this.value = value;
            }

            public int getValue() {
                return value;
            }
        }
        Foo foo = new Foo(10);
        System.out.println(foo.getValue()); //输出10


Java abstract关键字
在Java中，abstract关键字用于声明一个类、方法或接口为抽象的。

abstract关键字用法：
1.用来修饰类:被修饰的类称为抽象类,可能包含抽象方法,也可能不包含抽象方法
>此类不能被实例化,即不能创建对象(因为其中包含抽象方法),其他功能与类相同
>抽象类中一定有构造器,便于子类实例化时调用
>开发中,提供抽象类的子类,让子类对象实例化,完成相关操作
>抽象类一般位于体系结构的上层,用来定义功能

注意:如果一个类继承了抽象类,那么这个类要么重写抽象类中所有的抽象方法,要么将此类也抽象化
abstract class Animal {  
    // 抽象方法  
    abstract void makeSound();  
  
    // 非抽象方法  
    void eat() {  
        System.out.println("The animal eats.");  
    }  
}

PS:在Java中被abstract修饰的类不能被final修饰
在Java中，如果一个类被abstract修饰，那么这个类就是一个抽象类。而如果一个类被final修饰，那么这个类就是一个最终类，不能被继承。
由于抽象类被设计为不能被实例化的类，因此它不能同时被final修饰，因为final意味着这个类不能被继承，这与抽象类的设计初衷相矛盾。因此，在Java中，一个类不能同时被abstract和final修饰。


2.用来修饰方法: 被修饰的方法称为抽象方法
>抽象方法只有方法的声明,没有方法体(在一些比较顶层的类中,它的方法实现与子类大多不同,此时没必要在顶层实现具体的方法,只需声明功能即可)
>包含抽象方法的类一定是一个抽象类;反之,抽象类中可无方法体
>若子类重写父类中所有的抽象方法,则子类才可实例化
>若未重写父类抽象类所有的方法,则此子类也是一个抽象类,需要用abstract修饰

abstract class Animal {  
    public abstract void makeSound();  
}
在这个例子中，Animal是一个抽象类，它有一个抽象方法makeSound()。由于该方法没有具体的实现代码，因此它只能被声明而不能被实现。任何继承自Animal的类都必须提供makeSound()方法的具体实现。


注意:
abstract不能用来修饰属性,方法
abstract不能用来修饰: 私有方法,静态方法,final修饰的方法,final修饰的类

//抽象类的匿名子类
Worker worker = new Worker();
//非匿名类的非匿名对象
method1(worker);
//非匿名类的匿名对象
method1(new Worker());
//匿名类的匿名对象
//person类是抽象类
Person p = new Person(){

    @Override
    public void eat();

    @Override
    public void breath();

}

Java接口：
接口（英文：Interface），在JAVA编程语言中是一个抽象类型，是抽象方法的集合，接口通常以interface来声明。一个类通过继承接口的方式，从而来继承接口的抽象方法。
接口并不是类，编写接口的方式和类很相似，但是它们属于不同的概念。类描述对象的属性和方法。接口则包含类要实现的方法。除非实现接口的类是抽象类，否则该类要定义接口中的所有方法。
接口无法被实例化，但是可以被实现。一个实现接口的类，必须实现接口内所描述的所有方法，否则就必须声明为抽象类。另外，在 Java 中，接口类型可用来声明一个变量，他们可以成为一个空指针，或是被绑定在一个以此接口实现的对象。

public interface MyInterface {  
    void method1();  
  
    int method2(String str);  
}

抽象类和接口的对比：
1、抽象类和接口都不能直接实例化。如果要实例化，抽象类变量必须指向实现所有抽象方法的子类对象，接口变量必须指向实现所有接口方法的类对象。
2、抽象类要被子类继承，接口要被类实现。
3、接口只能做方法申明，抽象类中可以做方法申明，也可以做方法实现。
4、接口里定义的变量只能是公共的静态的常量，抽象类中的变量是普通变量。
5、抽象类里的抽象方法必须全部被子类所实现，如果子类不能全部实现父类抽象方法，那么该子类只能是抽象类。同样，实现接口的时候，如不能全部实现接口方法，那么该类也只能为抽象类。
6、抽象方法只能申明，不能实现。
7、抽象类里可以没有抽象方法
8、如果—个类里有抽象方法，那么这个类只能是抽象类
9、抽象方法要被实现，所以不能是静态的，也不能是私有的。
10、接口可以继承接口，并且可多继承接口，但类只能单—继承。
11.接口可以通过匿名内部类实例化。接口是对动作的抽象，抽象类是对根源的抽象。抽象类表示的是，这个对象是什么。而接口表示的是，这个对象能做什么。

接口和抽象类的区别？
1.抽象类是对事物的抽象，即对类抽象；接口是对行为抽象，即局部抽象。抽象类对整体形为进行抽象，包括形为和属性。接口只对行为进行抽象。
2.抽象类是多个子类的像类，是一种模板式设计；接口是一种形为规范，是一种辐射式设计。


为什么Java不是纯面向对象语言？
Java 是一种广泛使用的编程语言，它确实具有许多面向对象的特性，如类、对象、继承、封装和多态等。
然而，Java 并不是纯面向对象的语言，原因主要有以下几点：

1.基本数据类型：Java 中有八种基本数据类型（如 int、float、char 等），这些基本数据类型不是对象。
虽然 Java 为这些基本数据类型提供了对应的包装类（如 Integer、Float、Character 等），但在某些情况下，为了性能考虑，程序员仍然会直接使用基本数据类型。

2.静态成员和方法：Java 中的静态成员（包括静态变量和静态方法）并不属于任何对象实例，而是属于类本身。静态成员和方法可以通过类名直接访问，而不需要创建类的实例。这种特性与面向对象的封装和实例化的概念不完全一致。

3.单例模式：单例模式是一种设计模式，它保证一个类仅有一个实例，并提供一个全局访问点。在某些情况下，为了实现单例模式，需要使用静态成员和方法。这同样与面向对象的多实例、多态等特性不完全吻合。
4.原始类型数组：Java 中的数组实际上是原始类型，而不是对象。虽然数组可以包含对象，但数组本身并不是对象。这意味着数组没有继承自任何类，也没有实现任何接口。
5.主类和主方法：Java 程序从主类（包含 main 方法的类）开始执行。这个主方法是静态的，它不属于任何对象实例。这也是 Java 与纯面向对象语言的一个不同之处。

虽然 Java 不是纯面向对象的语言，但它的面向对象特性仍然非常强大和灵活。Java 通过类和对象的概念，以及封装、继承和多态等机制，为程序员提供了一种高效、可维护的方式来构建复杂的应用程序。
同时，Java 也提供了一些非面向对象的特性，以满足特定场景下的需求。这种折衷使得 Java 在实际应用中既具有面向对象的优势，又具有一定的灵活性。



面向对象和面向过程的区别？
面向过程：是分析解决问题的步骤，然后用函数把这些步骤一步一步地实现，然后在使用的时候一一调用则可。性能较高，所以单片机、嵌入式开发等一般采用面向过程开发

面向对象：是把构成问题的事务分解成各个对象，而建立对象的目的也不是为了完成一个个步骤，而是为了描述某个事物在解决整个问题的过程中所发生的行为。面向对象有封装、继承、多态的特性，所以易维护、易复用、易扩展。
可以设计出低耦合的系统。 但是性能上来说，比面向过程要低。



它们在编程思路、特点、优势以及应用场景等方面存在显著差异
编程思路：
面向对象编程：以对象为核心，将现实世界中的事物抽象为对象，通过对象之间的交互和协作来解决问题。它强调的是数据和功能的结合，以及对象之间的消息传递。
面对过程编程：以过程为中心，将程序分解为一系列的步骤或函数，每个步骤或函数完成一个具体的任务。它强调的是程序功能的顺序执行和数据在程序中的流动。

特点：
面向对象编程：
封装：将对象的属性和方法封装在一起，隐藏内部实现细节，只提供有限的接口供外部访问。
继承：子类可以继承父类的属性和方法，实现代码的复用和扩展。
多态：同一接口可以有多种不同的实现方式，提高了代码的灵活性和通用性。

面对过程编程：
功能分解：将复杂的问题分解成更小、更易于管理的子问题，通过定义一系列的函数或过程来实现。
逐步抽象：将具体操作封装在函数内部，简化程序的复杂度。

优势：
面向对象编程：
代码复用性高：通过封装和继承机制，可以实现代码的模块化和复用。
扩展性好：支持通过继承和派生来实现代码的扩展。
易于维护和调试：代码结构清晰，降低了开发和维护过程中的错误率。
抽象和封装特性：使得代码更加灵活和易于理解。

面对过程编程：
结构清晰：程序流程一目了然，便于跟踪程序执行的过程和调试。
适合简单问题：在处理简单直观的问题时，易于理解和实施。




Java高级应用：
异步、多线程、集合、泛型、序列化、IO、网络编程、反射、动态代理、jdk8.0以及以后新特性

Java多线程：Java多线程实现的原理是基于Java虚拟机（JVM）的线程调度。JVM通过线程调度器负责分配CPU时间，实现多线程的并发执行。

Java的多线程不是直接调用操作系统的线程操作，而是通过Java虚拟机（JVM）提供的抽象层来管理和实现。Java的这种设计使得Java程序具有更好的跨平台性和线程安全性。
下面详细阐述Java多线程的实现机制及其与操作系统的关系：
1. Java线程的抽象
Java中的线程（Thread）是Java编程语言中用于表示线程的对象。每个Thread对象都代表一个线程，并可以启动一个新的线程来执行特定的任务。Java线程模型是建立在JVM之上的，而JVM又是在操作系统之上运行的。
因此，Java线程实际上是JVM层面的线程，这些线程最终会被映射到操作系统的线程上执行。

2. Java线程与操作系统线程的映射
当Java程序创建一个新的线程时，JVM会请求操作系统创建一个新的线程（或可能从线程池中分配一个）。这个操作系统级别的线程将负责执行Java线程的代码。Java线程的创建、调度、同步、销毁等操作在底层都是由JVM和操作系统共同完成的。

操作系统中的多线程：
操作系统的多线程指令并不是一个单一的、具体的指令，而是涉及到多个层面的操作和机制。多线程的实现和管理主要依赖于操作系统内核提供的支持，以及编程语言提供的库和API。以下是从不同角度对操作系统多线程指令或相关机制的解释：

1. 操作系统内核支持
线程创建：操作系统内核提供了创建线程的接口或系统调用（如POSIX线程的pthread_create）。这些接口允许用户态的程序请求内核创建一个新的线程，并为其分配必要的资源（如栈空间、线程ID等）。
线程调度：操作系统内核的线程调度器负责管理线程的执行顺序。调度器会根据一定的调度策略（如时间片轮转、优先级调度等）来决定哪个线程在某一时刻运行。这个过程是由内核自动完成的，用户态程序通常不需要直接干预。
线程同步与通信：为了协调多个线程之间的执行顺序和数据访问，操作系统内核提供了同步机制（如互斥锁、信号量、条件变量等）和通信机制（如共享内存、消息传递等）。这些机制通常通过系统调用或库函数来访问。

2. 编程语言支持
现代编程语言大多提供了对多线程编程的支持，这些支持通常是通过库或API来实现的。

Java：在Java中，可以通过继承Thread类或实现Runnable接口来创建线程。创建线程后，通过调用线程的start方法来启动线程。Java还提供了synchronized关键字和wait/notify/notifyAll机制来实现线程间的同步和通信。
C++：C++11及以后的版本通过引入std::thread类来支持多线程编程。此外，C++还提供了互斥锁（std::mutex）、条件变量（std::condition_variable）等同步机制。
Python：Python通过threading模块来支持多线程编程。该模块提供了Thread类来创建线程，以及锁（Lock）、条件变量（Condition）等同步机制。

3. 指令与实现机制
从指令的角度来看，多线程的实现并不依赖于特定的单条指令，而是依赖于一系列的操作和机制。这些操作和机制包括：

线程创建与销毁：通过系统调用或库函数来创建和销毁线程。
线程调度：由操作系统内核的线程调度器自动完成，无需用户态程序直接干预。
线程同步与通信：通过系统调用或库函数来访问同步机制和通信机制，以确保线程间的正确交互和数据一致性。


关于操作系统中用户级线程和内核级线程的区别：
1.内核支持线程是OS内核可感知的，而用户级线程是OS内核不可感知的。
2.用户级线程的创建、撤消和调度不需要OS内核的支持，是在语言（如Java）这一级处理的；而内核支持线程的创建、撤消和调度都需OS内核提供支持，而且与进程的创建、撤消和调度大体是相同的。
3.用户级线程执行系统调用指令时将导致其所属进程被中断，而内核支持线程执行系统调用指令时，只导致该线程被中断。
4.在只有用户级线程的系统内，CPU调度还是以进程为单位，处于运行状态的进程中的多个线程，由用户程序控制线程的轮换运行；在有内核支持线程的系统内，CPU调度则以线程为单位，由OS的线程调度程序负责线程的调度。
5.用户级线程的程序实体是运行在用户态下的程序，而内核支持线程的程序实体则是可以运行在任何状态下的程序。

内核线程的优点：
1.当有多个处理机时，一个进程的多个线程可以同时执行。

缺点：
1.由内核进行调度。

用户进程的优点：
1.线程的调度不需要内核直接参与，控制简单。
2.可以在不支持线程的操作系统中实现。
3.创建和销毁线程、线程切换代价等线程管理的代价比内核线程少得多。
4.允许每个进程定制自己的调度算法，线程管理比较灵活。这就是必须自己写管理程序，与内核线程的区别
5.线程能够利用的表空间和堆栈空间比内核级线程多。
6.同一进程中只能同时有一个线程在运行，如果有一个线程使用了系统调用而阻塞，那么整个进程都会被挂起。另外，页面失效也会产生同样的问题。

缺点：
1.资源调度按照进程进行，多个处理机下，同一个进程中的线程只能在同一个处理机下分时复用


在Java中，线程可以处于多种状态，包括但不限于：
新建（NEW）：线程被创建但尚未启动。
可运行（RUNNABLE）：线程正在Java虚拟机中运行，但在等待操作系统的调度。
阻塞（BLOCKED）：线程正在等待监视器锁以便进入一个同步块/方法，或者正在等待其他线程释放一些必要的资源。
等待（WAITING）：线程无限期地等待另一个线程执行特定操作，如Object.wait()、Thread.join()和LockSupport.park()。
超时等待（TIMED_WAITING）：与WAITING状态类似，但线程等待的时间是有限的。
终止（TERMINATED）：线程已执行完毕。


Java多线程编程需要注意以下几点：
1.同步问题，如果不同步可能导致线程安全问题；
2.线程间通信，如果不能有效的沟通，线程的任务可能无法完成；
3.线程上下文切换，如果频繁的切换，可能导致性能问题；
4.防止死锁，如果两个线程互相等待对方释放资源，可能造成死锁。
5.线程生命周期的管理，线程不能永久存在，需要正确的关闭和管理。

Java多线程生命周期：
就绪状态：就绪状态的线程又叫做可运行状态，表示当前线程具有抢夺CPU时间片的权力（CPU时间片就是执行权）。当一个线程抢夺到CPU时间片之后，就开始执行run方法，run方法的开始执行标志着线程进入运行状态。
运行状态：run方法的开始执行标志着这个线程进入运行状态，当之前占有的CPU时间片用完之后，会重新回到就绪状态继续抢夺CPU时间片，当再次抢到CPU时间之后，会重新进入run方法接着上一次的代码继续往下执行。
阻塞状态：当一个线程遇到阻塞事件，例如接收用户键盘输入，或者sleep方法等，此时线程会进入阻塞状态，阻塞状态的线程会放弃之前占有的CPU时间片。之前的时间片没了需要再次回到就绪状态抢夺CPU时间片。
锁池：在这里找共享对象的对象锁线程进入锁池找共享对象的对象锁的时候，会释放之前占有CPU时间片，有可能找到了，有可能没找到，没找到则在锁池中等待，如果找到了会进入就绪状态继续抢夺CPU时间片。（这个进入锁池，可以理解为一种阻塞状态）

多线程的实现方式（一）
继承Thread类
1、自定义一个类MyThread类，用来继承与Thread类
2、在MyThread类中重写run（）方法
3、在测试类中创建MyThread类的对象
4、启动线程
PS:调用start()方法启动线程后，线程会从run()方法开始执行。

public class Demo01 {
    public static void main(String[] args) {
        //创建线程
        MyThread t01 = new MyThread();
        MyThread t02 = new MyThread();
        MyThread t03 = new MyThread("线程03");
 
        //开启线程
//        t01.run();
//        t02.run();
//        t03.run();
        // 不会启动线程，不会分配新的分支栈。（这种方式就是单线程。）
        // start()方法的作用是：启动一个分支线程，在JVM中开辟一个新的栈空间，这段代码任务完成之后，瞬间就结束了。
        // 这段代码的任务只是为了开启一个新的栈空间，只要新的栈空间开出来，start()方法就结束了。线程就启动成功了。
        // 启动成功的线程会自动调用run方法，并且run方法在分支栈的栈底部（压栈）。
        // run方法在分支栈的栈底部，main方法在主栈的栈底部。run和main是平级的。
        t01.start();
        t02.start();
        t03.start();
        //设置线程名（补救的设置线程名的方式）
        t01.setName("线程01");
        t02.setName("线程02");
        //设置主线程名称
        Thread.currentThread().setName("主线程");
        for (int i = 0; i < 50; i++) {
            //Thread.currentThread() 获取当前正在执行线程的对象
            System.out.println(Thread.currentThread().getName() + ":" + i);
        }
    }
}
class MyThread extends Thread{
    public MyThread() {
    }
 
    public MyThread(String name) {
        super(name);
    }
 
    //run方法是每个线程运行过程中都必须执行的方法
    @Override
    public void run() {
        for (int i = 0; i < 50; i++) {
            System.out.println(this.getName() + ":" + i);
        }
    }
}

此处最重要的为start()方法。单纯调用run()方法不会启动线程，不会分配新的分支栈。
start()方法的作用是：启动一个分支线程，在JVM中开辟一个新的栈空间，这段代码任务完成之后，瞬间就结束了。线程就启动成功了。
启动成功的线程会自动调用run方法（由JVM线程调度机制来运作的），并且run方法在分支栈的栈底部（压栈）。
run方法在分支栈的栈底部，main方法在主栈的栈底部。run和main是平级的。
单纯使用run()方法是不能多线程并发的。

多线程的实现方式（二）
实现Runnable接口
1、自定义一个MyRunnable类来实现Runnable接口
2、在MyRunnable类中重写run（）方法
3、创建Thread对象，并把MyRunnable对象作为Tread类构造方法的参数传递进去
4、启动线程
开发中使用这种方式创建多线程，而不是继承Thread类
PS:调用start()方法启动线程后，线程会从run()方法开始执行。

public class Demo02 {
    public static void main(String[] args) {
        MyRunnable myRun = new MyRunnable();//将一个任务提取出来，让多个线程共同去执行
        //封装线程对象
        Thread t01 = new Thread(myRun, "线程01");
        Thread t02 = new Thread(myRun, "线程02");
        Thread t03 = new Thread(myRun, "线程03");
        //开启线程
        t01.start();
        t02.start();
        t03.start();
        //通过匿名内部类的方式创建线程
        new Thread(new Runnable() {
            @Override
            public void run() {
                for (int i = 0; i < 20; i++) {
                    System.out.println(Thread.currentThread().getName() + " - " + i);
                }
            }
        },"线程04").start();
    }
}
//自定义线程类，实现Runnable接口
//这并不是一个线程类，是一个可运行的类，它还不是一个线程。
class MyRunnable implements Runnable{
    @Override
    public void run() {
        for (int i = 0; i < 50; i++) {
            System.out.println(Thread.currentThread().getName() + " - " + i);
        }
    }
}

实际工作过程中，Java创建多线程的方式大多数使用实现Runnable接口而不是继承Thread类的方式。
原因：
1. Java中的单继承特性
Java不支持多重继承，即一个类只能继承一个父类。如果一个类已经继承了另一个类，那么它不能再继承Thread类。因此，通过实现Runnable接口来创建线程允许你的类可以继承其他类，这提高了类的灵活性和复用性。

2. 更好的设计模式
实现Runnable接口是将任务与线程分离的一种方式。这符合面向对象设计中的单一职责原则，即任务的逻辑（业务逻辑）和线程管理（线程的创建和启动）是分离的。
通过实现Runnable接口，可以将业务逻辑封装在一个独立的类中，而线程管理由Thread类处理，这种方式更符合面向对象的设计思想。

3. 共享资源
使用Runnable接口可以更方便地共享资源和数据。多个线程可以共享一个实现了Runnable接口的实例，从而能够更方便地实现线程间的资源共享和数据共享。这在需要多个线程协同工作的场景下非常有用。

4. 更灵活的线程池管理
Java中的线程池（如ExecutorService）通常接受实现了Runnable接口的任务。使用线程池可以更有效地管理线程资源，避免频繁地创建和销毁线程带来的性能开销。
而线程池中的线程复用机制可以显著提升应用的性能和稳定性。

5. 简化代码
实现Runnable接口的代码通常更简洁明了。通过实现Runnable接口，不需要考虑线程的生命周期管理，Thread类会自动处理线程的启动和结束，开发者只需专注于业务逻辑的实现。


多线程的实现方式（三）
实现Callable接口（ java.util.concurrent.FutureTask; /JUC包下的，属于java的并发包，老JDK中没有这个包。新特性。）
1、自定义一个MyCallable类来实现Callable接口
2、在MyCallable类中重写call()方法
3、创建FutureTask，Thread对象，并把MyCallable对象作为FutureTask类构造方法的参数传递进去，把FutureTask对象传递给Thread对象。
4、启动线程

        这种方式的优点：可以获取到线程的执行结果；方法可以抛出异常；返回结果支持泛型的返回值

        这种方式的缺点：效率比较低，在获取t线程执行结果的时候，当前线程受阻塞，效率较低。

		
package com.lmw.thread3;

import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.FutureTask;

// 1.创建一个实现callable接口的实现类
class Number implements Callable {
    // 2.实现call方法，将此线程需要执行的操作写在call方法中
    @Override
    public Object call() throws Exception {
        int sum = 0;
        for (int i = 1; i <= 100; i++) {
            if(i % 2 == 0) {
                sum  = sum + i;
            }
        }
        return sum;
    }
}

public class ThreadCallableTest {
    public static void main(String[] args) {
        //3.创建callable接口实现类的对象
        Number number = new Number();
        //4.将callable实现接口实现类的对象作为参数传递到FutureTask构造器中，创建FutureTask的对象
        FutureTask futureTask = new FutureTask(number);
        //5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start()
        new Thread(futureTask).start();

        try {
            // get方法的返回值即为futureTask构造器的参数Callable实现类重写的call方法的返回值
            //6.获取callable中call方法的返回值
            Object sum = futureTask.get();
            System.out.println("总和为:" + sum);
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (ExecutionException e) {
            e.printStackTrace();
        }


    }
}



多线程的实现方式（四）
创建线程池		
优点：1.提高响应速度 2.降低资源消耗 3.便于线程管理

import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
​
class MyCallable implements Callable<Object> {
    private int taskNum;
​
    MyCallable(int taskNum) {
        this.taskNum = taskNum;
    }
​	@Override
    public Object call() throws Exception {
        System.out.println(">>> " + taskNum + " 线程任务启动");
        Date startTime = new Date();
        Thread.sleep(1000);
        Date endTime = new Date();
        long time = endTime.getTime() - startTime.getTime();
        System.out.println(">>> " + taskNum + " 线程任务终止");
        return taskNum + "线程任务返回运行结果, 当前任务耗时【" + time + "毫秒】";
    }
}


public class ThreadDemo {
    
    @SuppressWarnings({ "rawtypes", "unchecked" })
    public static void main(String[] args) throws ExecutionException, InterruptedException {
        System.out.println("---- 主程序开始运行 ----");
        Date startTime = new Date();
        
        int taskSize = 5;
        // 创建一个线程池,Executors提供了创建各种类型线程池的方法，具体详情请自行查阅
        ExecutorService executorService = Executors.newFixedThreadPool(taskSize);
        
        // 创建多个有返回值的任务
        List<Future> futureList = new ArrayList<Future>();
        for (int i = 0; i < taskSize; i++) {
            Callable callable = new MyCallable(i);
            // 执行任务并获取Future对象
            Future future = executorService.submit(callable);
            futureList.add(future);
        }
        
        // 关闭线程池
        executorService.shutdown();
​
        // 获取所有并发任务的运行结果
        for (Future future : futureList) {
            // 从Future对象上获取任务的返回值，并输出到控制台
            System.out.println(">>> " + future.get().toString());
        }
​
        Date endTime = new Date();
        System.out.println("---- 主程序结束运行 ----，程序运行耗时【" + (endTime.getTime() - startTime.getTime()) + "毫秒】");
    }
}


Java synchronized 关键字：同步代码块，解决线程安全问题，解决多线程上下文冲突问题，保证线程原子性
synchronized(同步监视器){
//需要被同步的代码
}
注：1.同步监视器，俗称：锁。使用当前类的对象当成锁，锁住需要同步的代码。
    2.可以使用this关键字，也可以使用Object对象充当锁。
	3.非静态的同步方法，同步监视器是this；静态的同步方法，同步监视器是当前类本身

synchronized关键字工作原理：
锁机制：
当一个线程进入一个同步方法或同步代码块时，它必须先获得相应对象的锁。其他线程如果试图进入同一个对象的同步方法或代码块，则必须等待，直到持有锁的线程释放锁。锁是互斥的，即同一时间只能有一个线程持有锁。

可见性：
synchronized不仅确保了互斥性，还确保了可见性。当一个线程释放锁时，之前对共享变量的修改对接下来获取锁的线程是可见的。

解决的问题：
数据一致性：
通过锁定共享资源，synchronized确保了线程对共享变量的修改是原子操作，防止出现数据不一致的问题。

synchronized修饰方法：
在Java中，synchronized 关键字用于修饰方法时，它可以确保在同一时间内只有一个线程可以执行该方法。这种机制是Java并发控制的一种重要方式，用于防止多个线程同时访问共享资源时可能引起的数据不一致或线程安全问题。

修饰实例方法：
当synchronized 修饰一个实例方法时，它实际上是对调用该方法的对象实例加锁。这意味着，如果一个线程正在执行某个对象的某个synchronized 实例方法，那么其他线程不能同时执行该对象的其他任何synchronized 实例方法。
但是，这些线程可以访问该对象的非synchronized 方法或该类的其他对象的synchronized 方法（如果它们锁定的是不同的对象实例）。

public class Counter {  
    private int count = 0;  
  
    // 同步实例方法  
    public synchronized void increment() {  
        count++;  
    }  
  
    public synchronized int getCount() {  
        return count;  
    }  
}
在上面的例子中，increment 和 getCount 方法都是同步的，它们都锁定了调用它们的Counter对象实例。如果两个线程尝试同时访问同一个Counter实例的这两个方法之一，那么它们将被迫串行执行。

修饰静态方法：
当synchronized 修饰一个静态方法时，它锁定的是调用该方法的类的Class对象，而不是类的某个特定实例。因此，如果一个线程正在执行某个类的某个synchronized 静态方法，那么其他线程不能同时执行该类的任何其他synchronized 静态方法。
但是，这些线程仍然可以访问该类的非synchronized 方法或该类的实例的synchronized 实例方法（因为实例方法的锁是实例级别的，与静态方法的锁不同）。

public class StaticCounter {  
    private static int count = 0;  
  
    // 同步静态方法  
    public static synchronized void increment() {  
        count++;  
    }  
  
    public static synchronized int getCount() {  
        return count;  
    }  
}
在上面的例子中，increment 和 getCount 方法都是静态同步的，它们都锁定了StaticCounter类的Class对象。如果两个线程尝试同时调用这些静态方法中的任何一个，那么它们将被迫串行执行。

总结
当synchronized 修饰实例方法时，它锁定的是调用该方法的对象实例。
当synchronized 修饰静态方法时，它锁定的是调用该方法的类的Class对象。
无论是实例方法还是静态方法，synchronized 都确保了同一时间只有一个线程可以执行该方法，从而避免了多线程环境下的数据不一致和线程安全问题。
需要注意的是，过度使用synchronized 可能会导致性能下降，因为它会引入线程间的竞争和等待，因此应该谨慎使用，并在必要时考虑使用其他并发控制机制，如java.util.concurrent 包中的工具类。


死锁问题：
需要注意的是，不恰当的使用synchronized可能导致死锁问题。例如，如果两个线程互相等待对方释放锁，就会产生死锁。因此，在设计并发程序时，需要小心避免死锁。

通过同步实例方法解决线程安全问题：
public class SynchronizedExample {
    private int count = 0;

    public synchronized void increment() {
        count++;
    }

    public synchronized int getCount() {
        return count;
    }

    public static void main(String[] args) {
        SynchronizedExample example = new SynchronizedExample();

        Runnable task = () -> {
            for (int i = 0; i < 1000; i++) {
                example.increment();
            }
        };

        Thread thread1 = new Thread(task);
        Thread thread2 = new Thread(task);

        thread1.start();
        thread2.start();

        try {
            thread1.join();
            thread2.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        System.out.println("Final count: " + example.getCount());
    }
}

lock方法解决线程安全问题：
在Java中，除了synchronized关键字外，java.util.concurrent.locks包提供了更加灵活和强大的锁机制，比如ReentrantLock。
ReentrantLock提供了与synchronized相同的互斥锁功能，并且提供了更多的高级特性，例如可重入、定时锁定和中断锁定等。

定义共享资源类，并使用ReentrantLock进行同步
public class Counter {
    private int count = 0;
    private final Lock lock = new ReentrantLock();

    public void increment() {
        lock.lock(); // 获取锁
        try {
            count++;
        } finally {
            lock.unlock(); // 确保在执行完后释放锁
        }
    }

    public int getCount() {
        lock.lock(); // 获取锁
        try {
            return count;
        } finally {
            lock.unlock(); // 确保在执行完后释放锁
        }
    }
}

创建线程
public class LockExample {
    public static void main(String[] args) {
        Counter counter = new Counter();

        Runnable task = () -> {
            for (int i = 0; i < 1000; i++) {
                counter.increment();
            }
        };

        Thread thread1 = new Thread(task);
        Thread thread2 = new Thread(task);

        thread1.start();
        thread2.start();

        try {
            thread1.join();
            thread2.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        System.out.println("Final count: " + counter.getCount());
    }
}

ReentrantLock的创建：ReentrantLock对象通过new ReentrantLock()来创建，并作为Counter类的一个私有成员变量。
锁的获取与释放：在需要保护的代码块前，使用lock.lock()获取锁，在代码块后使用lock.unlock()释放锁。为了确保无论代码块是否抛出异常，锁都能被正确释放，unlock()方法通常放在finally块中。
try-finally结构：确保了即使在方法执行过程中发生异常，锁也会被正确释放，从而避免死锁情况。


lock方法与synchronized关键字解决线程安全问题的区别：
synchronized：适用于简单的同步场景，且由于其简洁和JVM的优化，性能在简单同步下表现较好。
Lock：适用于复杂的同步场景，提供了更高的灵活性和更多的控制能力，适合需要高级锁特性（如公平锁、轮询锁定、定时锁定和中断锁定）的情况。

1. 基本功能
synchronized：
内置语言特性，通过关键字实现同步。
自动管理锁的获取和释放。
Lock：
类库特性，通过java.util.concurrent.locks.Lock接口实现同步。
需要显式地获取和释放锁。

2. 灵活性
synchronized：
只能锁住方法或代码块。
锁的范围和粒度是固定的。
Lock：
可以在方法或代码块中自由地控制锁的获取和释放。
提供了更灵活的锁机制，例如：尝试锁定、定时锁定和可中断锁定。

3. 锁的特性
synchronized：
递归锁（Reentrant）：同一线程可以多次进入一个同步方法或代码块。
公平性：无内置的公平性控制，锁是非公平的。
Lock：
递归锁（Reentrant）：ReentrantLock是递归锁，同一线程可以多次获取锁。
公平性：可以通过构造函数参数设置公平锁或非公平锁（默认非公平）。
ReentrantLock lock = new ReentrantLock(true); // 公平锁

4. 性能
synchronized：
简单同步下性能较好，JVM对其进行了许多优化（如偏向锁、轻量级锁、锁粗化等）。
Lock：
在复杂同步场景下性能较好。
提供了更好的线程调度和资源管理能力。

5. 错误处理
synchronized：
编译器自动管理锁的获取和释放，较少发生因忘记释放锁而导致的问题。
Lock：
必须显式地在finally块中释放锁，容易因编程错误导致死锁。

6. 其他特性
synchronized：
不支持锁的轮询、定时获取锁和中断获取锁。
Lock：
提供了尝试锁定（tryLock()）、定时锁定（tryLock(long time, TimeUnit unit)）和中断锁定（lockInterruptibly()）。


Java多线程实现线程通讯的方法：sleep()  wait()  
sleep和wait的区别
1、sleep是线程中的方法，但是wait是Object中的方法。
2、sleep方法不会释放资源锁，但是wait会释放资源锁，而且会加入到等待队列中。
3、sleep方法不依赖于同步器synchronized，但是wait需要依赖synchronized关键字。
4、sleep不需要被唤醒（休眠之后推出阻塞），但是wait需要（不指定时间需要被别人中断）。

Java多线程中的notify()方法：
Java多线程中的notify()方法是用于唤醒正在等待该对象监视器（monitor）的单个线程的方法。

基本原理
当一个线程进入某个对象的同步代码块或同步方法时，它会自动获取该对象的监视器锁。如果在执行过程中，线程调用了该对象的wait()方法，那么它会释放该监视器锁并进入等待状态，直到其他线程调用了该对象的notify()或notifyAll()方法。

示例代码
public class SharedResource {  
    private int value = 0;  
    private boolean available = false;  
  
    public synchronized void produce(int newValue) {  
        while (available) { // 等待资源可用  
            try {  
                wait();  
            } catch (InterruptedException e) {  
                Thread.currentThread().interrupt();  
                return;  
            }  
        }  
        value = newValue;  
        available = true;  
        notify(); // 唤醒一个等待的消费者  
    }  
  
    public synchronized int consume() {  
        while (!available) { // 等待资源生成  
            try {  
                wait();  
            } catch (InterruptedException e) {  
                Thread.currentThread().interrupt();  
                return Integer.MIN_VALUE; // 或抛出异常  
            }  
        }  
        available = false;  
        notify(); // 可选：唤醒一个可能的生产者（虽然在这个例子中可能不需要）  
        return value;  
    }  
}


notify()方法：唤醒在该对象监视器上等待的单个线程（如果有的话）。如果有多个线程在等待，选择哪一个线程被唤醒是由JVM的线程调度器决定的，无法预测。
notifyAll()方法：唤醒在该对象监视器上等待的所有线程。


线程重新获取锁的过程
等待线程被唤醒：当一个线程调用wait()方法时，它会释放所持有的监视器锁并进入等待状态。随后，当另一个线程调用同一个对象的notify()或notifyAll()方法时，至少有一个（或所有）等待的线程会被唤醒。
重新竞争锁：被唤醒的线程会重新进入可运行状态（RUNNABLE），但此时它并不立即获得监视器锁。相反，它必须与其他线程竞争，以重新获取对该对象的监视器锁。
获取锁并继续执行：一旦被唤醒的线程成功获取了监视器锁，它就会从wait()方法调用之后的下一条语句继续执行。

public class WaitNotifyExample {  
    private final Object lock = new Object();  
  
    public void waitForSignal() {  
        synchronized (lock) {  
            try {  
                System.out.println("Waiting for signal...");  
                lock.wait(); // 释放lock锁并进入等待状态  
                System.out.println("Received signal and continuing execution.");  
            } catch (InterruptedException e) {  
                Thread.currentThread().interrupt();  
            }  
        }  
    }  
  
    public void signal() {  
        synchronized (lock) {  
            System.out.println("Signaling...");  
            lock.notify(); // 唤醒一个等待的线程，但不释放lock锁  
            // 当synchronized块结束时，lock锁被释放  
        }  
    }    
}

Java多线程的中断方法：
1. 调用Thread.interrupt()方法
Thread.interrupt()方法用于中断目标线程。当调用一个线程的interrupt()方法时，Java虚拟机（JVM）会将该线程的中断状态设置为true。然而，这并不会立即停止线程的执行。
线程必须检查其中断状态（通过Thread.currentThread().isInterrupted()或Thread.interrupted()），并据此做出响应，比如通过抛出InterruptedException或简单地结束执行。

2. 响应中断
要使线程能够响应中断，它需要在执行过程中周期性地检查自己的中断状态，或者在执行可能阻塞的操作（如Thread.sleep(), Object.wait(), Thread.join()等）时正确响应这些操作抛出的InterruptedException。
检查中断状态：线程可以通过Thread.currentThread().isInterrupted()检查自己是否被中断，并据此决定是否需要结束执行。
捕获InterruptedException：对于可能抛出InterruptedException的阻塞方法，线程需要捕获这个异常，并决定是继续执行还是抛出InterruptedException，或者重新设置中断状态（Thread.currentThread().interrupt()）并结束执行。

3. 使用Future和ExecutorService
在使用ExecutorService管理线程池时，可以通过Future接口来中断正在执行的任务。调用Future.cancel(mayInterruptIfRunning)方法尝试取消任务的执行。
如果mayInterruptIfRunning为true，并且任务正在执行，那么将尝试中断任务。需要注意的是，Future.cancel()并不能保证任务一定会被取消，特别是如果任务已经完成了，或者由于某些原因不能被中断。

4. 使用volatile变量或Atomic类
虽然不是直接中断线程，但可以使用volatile变量或Atomic类来在多个线程之间共享状态，并通过检查这个状态来决定线程是否应该停止执行。这要求线程在执行过程中定期检查这个状态。

5. 使用Cooperative Cancellation（协作式取消）
这是一种设计模式，通过定义一组协议（比如，设置中断状态，检查中断状态，以及抛出InterruptedException等），使得线程之间可以协作地处理中断请求。这种方式要求线程在设计时就考虑到中断的响应机制。


Java多线程的挂起方法：
使用Thread.sleep(long millis)方法：
这是最直接的方式来让当前线程暂停执行指定的毫秒数。需要注意的是，sleep方法会让当前线程进入TIMED_WAITING状态，但它不会释放锁（如果当前线程持有锁的话）。

try {  
    Thread.sleep(1000); // 暂停当前线程1秒  
} catch (InterruptedException e) {  
    e.printStackTrace();  
}

使用Object.wait()方法：
当一个线程调用某个对象的wait()方法时，它会释放对象的锁并进入WAITING状态，直到其他线程调用了同一个对象的notify()或notifyAll()方法。需要注意的是，wait()方法必须在同步块或同步方法中调用，否则会抛出IllegalMonitorStateException。

synchronized (someObject) {  
    someObject.wait(); // 等待其他线程唤醒  
}

Java多线程的join方法：
Java中的Thread类提供了一个join()方法，它允许一个线程等待另一个线程完成其执行。当在某个线程中调用另一个线程的join()方法时，当前线程会暂停执行，直到被join()方法调用的那个线程执行完毕。这是一种线程同步的方式，用于确保线程之间的执行顺序。
public final void join() throws InterruptedException

示例代码：
public class JoinExample {  
    public static void main(String[] args) {  
        Thread thread1 = new Thread(() -> {  
            System.out.println("线程1开始执行");  
            try {  
                // 模拟耗时操作  
                Thread.sleep(2000);  
            } catch (InterruptedException e) {  
                e.printStackTrace();  
            }  
            System.out.println("线程1执行完毕");  
        });  
  
        Thread thread2 = new Thread(() -> {  
            try {  
                // 等待线程1执行完毕  
                thread1.join();  
            } catch (InterruptedException e) {  
                e.printStackTrace();  
            }  
            System.out.println("线程2在线程1之后执行");  
        });  
  
        thread1.start();  
        thread2.start();  
    }  
}

在这个示例中，thread1首先开始执行，并打印出“线程1开始执行”。然后，它进入了一个sleep()调用，模拟了一个耗时的操作。与此同时，thread2也开始了执行，但它在尝试执行任何操作之前，首先调用了thread1.join()。
这导致thread2暂停执行，并等待thread1完成其sleep()调用并打印出“线程1执行完毕”。一旦thread1执行完毕，thread2就会继续执行，并打印出“线程2在线程1之后执行”。

注意事项
1.调用join()方法的线程会阻塞，直到目标线程执行完毕。因此，如果目标线程是一个无限循环的线程，那么调用join()的线程将会永远等待下去。
2.join()方法会抛出InterruptedException，如果当前线程在等待过程中被中断，那么它会清除中断状态并抛出此异常。因此，在调用join()时，通常应该将其放在try-catch块中，或者至少检查中断状态并适当地处理它。
3.join()方法是线程同步的一种手段，但它并不提供像锁（如ReentrantLock）那样的灵活性和功能。在需要更复杂的同步机制时，应该考虑使用其他同步工具。

Java多线程的yield方法：
Java中的Thread类提供了一个yield()方法，这是一个静态的线程调度提示，用于建议当前正在执行的线程将CPU资源让给同一个处理器上的其他线程（如果有的话）。
然而，需要注意的是，yield()方法是一个提示（hint），而不是一个强制性的命令。它仅仅是告诉JVM，当前线程愿意放弃当前的CPU时间片，但JVM可以忽略这个提示。

示例代码：
public class YieldExample {  
    public static void main(String[] args) {  
        Thread thread1 = new Thread(() -> {  
            for (int i = 0; i < 10; i++) {  
                System.out.println("线程1: " + i);  
                // 尝试让出CPU时间片  
                Thread.yield();  
            }  
        });  
  
        Thread thread2 = new Thread(() -> {  
            for (int i = 0; i < 10; i++) {  
                System.out.println("线程2: " + i);  
                // 尝试让出CPU时间片  
                Thread.yield();  
            }  
        });  
  
        thread1.start();  
        thread2.start();  
    }  
}

在这个示例中，thread1和thread2都会打印出0到9的数字，并在每次迭代中调用Thread.yield()。然而，由于yield()的非强制性，这两个线程的执行顺序和输出可能会因JVM的调度策略和当前系统的负载而有所不同。因此，你不应该依赖yield()方法来控制线程的执行顺序或同步。

Java多线程的volatile方法：
在Java中，volatile 关键字是一个轻量级的同步机制，它用于确保变量的可见性和有序性，但并不保证原子性。volatile 关键字主要用于修饰变量，而不是方法。然而，理解volatile如何工作以及它如何影响多线程编程是很重要的，尽管它不能直接应用于方法。

可见性
在多线程环境中，每个线程可能在自己的工作内存（如CPU缓存）中持有变量的副本。这可能导致一个线程修改了某个变量的值，但这个修改对其他线程是不可见的，因为它们可能还在使用旧的变量值。
使用volatile修饰变量可以确保，当一个线程修改了volatile变量的值后，这个新值会立即对其他线程可见。这是通过禁止指令重排序和确保每次读取volatile变量时都从主内存中读取最新值来实现的。

有序性
Java虚拟机（JVM）和处理器可能会对指令进行重排序以提高性能。然而，这种重排序可能会导致多线程程序出现意外的行为。volatile变量可以禁止指令重排序，确保程序按照预期的顺序执行。
这主要涉及到volatile变量前后的读写操作，而不是整个方法的执行顺序。

原子性
尽管volatile提供了可见性和有序性的保证，但它并不保证原子性。原子性意味着操作是不可分割的，即操作要么完全执行，要么完全不执行。例如，对于volatile int count = 0;，count++操作（count = count + 1;）就不是原子的，因为它包含读取、计算和写入三个步骤。如果多个线程同时执行这个操作，可能会导致结果不正确。

使用场景
状态标记：volatile变量常用于作为状态标记，如控制循环的布尔变量。
单例模式的双重检查锁定（Double-Checked Locking）：在Java 5及以后的版本中，可以使用volatile来确保单例对象的初始化是线程安全的。
内存屏障：在某些情况下，volatile变量可以用作内存屏障，防止指令重排序。

Java多线程的线程组：
在Java中，线程组（ThreadGroup）是一个可以包含线程和其他线程组的容器。它是Java早期版本中用于组织和管理线程的一种方式，但随着时间的推移，Java并发API的进化（如java.util.concurrent包中的类和接口），线程组的使用已经变得不那么常见了。
然而，了解线程组的基本概念和用法对于理解Java并发编程的历史和某些遗留代码仍然是有帮助的。

线程组的基本特性
包含关系：线程组可以包含线程和其他线程组，形成一个树状结构。每个线程都属于某个线程组，且这个关系在线程的生命周期内是不变的。
资源管理：线程组可以用于资源的统一管理，比如监控组中所有线程的状态，或者当组中的某个线程出现问题时，能够方便地中断或停止组中所有的线程。
未捕获异常的处理：如果线程组中某个线程发生了未捕获的异常，且该线程没有设置自己的UncaughtExceptionHandler，那么这个异常会沿着线程组的层次结构向上传播，直到找到一个合适的处理器或者到达最顶层的线程组（即根线程组）。

线程组的常用方法
int activeCount(): 返回线程组中当前活跃的线程数。
void interrupt(): 中断线程组中的所有线程。需要注意的是，这个操作并不能保证能够中断所有线程，因为线程可能无法响应中断。
Thread[] enumerate(): 返回一个包含线程组中所有线程的数组。这个方法存在不同的重载版本，允许调用者指定是否包含子线程组中的线程。
void addUncaughtExceptionHandler(UncaughtExceptionHandler eh): 为线程组设置未捕获异常的处理器。这个处理器将用于处理组中那些没有设置自己异常处理器的线程的未捕获异常。

线程组的局限性
尽管线程组提供了线程组织和管理的一种机制，但它也存在一些局限性：
不可变性：一旦线程被创建并加入到某个线程组，它就不能改变其所属的线程组。
资源管理的不便：线程组并没有提供细粒度的资源管理方法，比如限制线程组的总资源使用量或线程数。
现代并发工具的替代：随着Java并发包（java.util.concurrent）的引入，现代Java应用程序更倾向于使用ExecutorService、Future、CountDownLatch、CyclicBarrier等并发工具来管理线程和任务，这些工具提供了更强大、更灵活的并发控制能力。

示例代码：
public class ThreadGroupExample {  
    public static void main(String[] args) {  
        // 创建一个名为"MyGroup"的线程组  
        ThreadGroup myGroup = new ThreadGroup("MyGroup");  
  
        // 创建并启动两个线程，将它们添加到"MyGroup"线程组中  
        Thread thread1 = new Thread(myGroup, new Runnable() {  
            @Override  
            public void run() {  
                System.out.println(Thread.currentThread().getName() + " 属于 " + Thread.currentThread().getThreadGroup().getName());  
            }  
        }, "Thread-1");  
        thread1.start();  
  
        Thread thread2 = new Thread(myGroup, new Runnable() {  
            @Override  
            public void run() {  
                System.out.println(Thread.currentThread().getName() + " 属于 " + Thread.currentThread().getThreadGroup().getName());  
            }  
        }, "Thread-2");  
        thread2.start();  
  
        // 输出线程组中活跃线程的数量  
        System.out.println("线程组 'MyGroup' 中活跃线程的数量: " + myGroup.activeCount());  
    }  
}


Java多线程的守护线程：
Java多线程中的守护线程（Daemon Thread）是一种特殊的线程类型，它在程序运行过程中在后台执行，主要任务是为其他线程（主要是用户线程）提供服务和支持。

一、定义与特点
定义：守护线程是在后台运行的线程，它的主要目的是为其他线程（特别是用户线程）提供服务。
特点：
随主线程结束而结束：当所有的非守护线程（即用户线程）都执行完毕后，守护线程会随之被终止，不会阻止Java虚拟机（JVM）的退出。
优先级较低：守护线程的优先级通常较低，以避免抢占过多CPU资源。
不能执行finally块：如果守护线程被终止，其中的finally块不会被执行。
不能持有关键资源：守护线程不能持有程序运行的关键资源，因为它们可能在所有用户线程结束时被强制退出，这可能导致数据不一致或资源泄漏。
不能用于执行必须完成的任务：由于守护线程可能被随时中断，因此不适合执行必须完成的任务，如文件写入等。

二、创建与设置
创建：守护线程的创建与普通线程的创建方式相同，可以使用Thread类或其子类，但需要通过特定的方法将其设置为守护线程。
设置：通过调用线程的setDaemon(true)方法将线程设置为守护线程。注意，这个设置必须在线程启动之前进行，否则将抛出IllegalThreadStateException异常。

三、使用场景
守护线程通常用于执行一些后台任务，这些任务对于应用程序的运行不是必须的，但可以提供额外的功能和服务。
常见的使用场景包括：
日志记录：守护线程可以定期或实时地将程序运行日志输出到文件或控制台，帮助开发者监控程序运行状态。
系统监控：守护线程可以监控系统资源的使用情况，如CPU、内存等，以便在资源不足时及时采取措施。
内存清理：虽然Java有垃圾回收机制，但守护线程可以执行一些额外的内存清理任务，以优化内存使用。
服务监听：在服务器应用中，守护线程可以用于监听某个端口，等待客户端的连接请求。
定时任务：一些定时任务可以作为守护线程执行，如定时备份数据库、定时清理临时文件等。

四、注意事项
使用守护线程时需要谨慎，确保其行为不会影响到程序的正确性和稳定性。
守护线程不能捕获和处理异常，一旦出现异常，守护线程会自动结束。
守护线程无法使用join()方法等待其他线程的结束，因为守护线程会在所有非守护线程结束后自动结束。
综上所述，Java多线程中的守护线程是一种特殊的线程类型，它在后台运行并为其他线程提供服务。了解守护线程的定义、特点、创建与设置方法以及使用场景对于编写高效、稳定的Java多线程程序具有重要意义。

public class DaemonThreadExample {  
  
    public static void main(String[] args) {  
        // 创建一个线程  
        Thread daemonThread = new Thread(() -> {  
            while (true) {  
                try {  
                    // 模拟守护线程正在执行的任务  
                    System.out.println("守护线程正在运行: " + System.currentTimeMillis());  
                    // 为了演示效果，每次打印后休眠1秒  
                    Thread.sleep(1000);  
                } catch (InterruptedException e) {  
                    e.printStackTrace();  
                }  
            }  
        });  
  
        // 将线程设置为守护线程  
        daemonThread.setDaemon(true);  
  
        // 启动线程  
        daemonThread.start();  
  
        try {  
            // 主线程休眠5秒  
            System.out.println("主线程休眠5秒后结束...");  
            Thread.sleep(5000);  
            System.out.println("主线程结束，由于守护线程的存在，JVM将在守护线程结束时退出。但由于守护线程的特性，这不会发生，因为主线程已经退出。");  
        } catch (InterruptedException e) {  
            e.printStackTrace();  
        }  
  
        // 注意：这里不需要显式地停止守护线程  
        // JVM会在只剩下守护线程时自动退出  
    }  
}

Java多线程的几种锁：
有四种锁类型：偏向锁、轻量级锁、自旋锁、重量级锁。
加锁顺序：偏向锁→轻量级锁→自旋锁→重量级锁。

在Java中，偏向锁、轻量级锁、自旋锁（尽管Java的官方文档中并不直接称其为“自旋锁”，但自旋的概念在轻量级锁的实现中有所体现）和重量级锁都是JVM（Java虚拟机）内部用于实现同步机制的锁优化技术。
这些锁的优化策略旨在提高多线程程序的性能，减少线程同步的开销。

偏向锁（Biased Locking）
特点：
当一个线程访问同步代码块并获取锁时，如果发现锁对象处于无锁状态（即没有被其他线程持有），JVM会把这个锁对象的锁记录（Mark Word）中的线程ID设置为当前线程的ID，表示这个锁偏向当前线程。
以后该线程再次进入这个同步代码块时，会检查锁对象的锁记录中是否存储着自己的线程ID，如果是，则无需进行任何同步操作，直接可以进入同步代码块，从而提高程序性能。
如果后续有其他线程尝试获取这个锁，偏向锁会被撤销，并升级为轻量级锁或重量级锁。
适用场景：
适用于只有一个线程访问同步代码块的场景，或者多个线程交替访问同步代码块的场景，可以减少锁的开销。

轻量级锁（Lightweight Locking）
特点：
如果在偏向锁撤销后，多个线程交替访问同步代码块，那么锁会升级为轻量级锁。
轻量级锁通过CAS（Compare-And-Swap）操作来实现，线程在尝试获取锁时，会检查锁对象的锁记录中的线程ID是否指向当前线程，如果是，则获取锁成功；如果不是，则通过自旋的方式不断尝试获取锁，直到成功为止。
轻量级锁的开销比偏向锁大，但比重量级锁小，它避免了线程挂起和恢复的开销。
适用场景：
适用于线程交替执行同步代码块的场景，以及锁竞争不激烈的场景。

自旋锁（Spin Lock）
特点：
自旋锁是一种基于忙等待的锁实现方式，当线程尝试获取锁时，如果锁已被其他线程持有，则当前线程会不断循环检查锁的状态，直到锁被释放为止。
自旋锁不会使线程进入阻塞状态，因此可以减少线程切换的开销，但在CPU资源紧张的情况下，可能会导致CPU利用率过高。
适用场景：
适用于锁持有时间非常短的场景，以及CPU密集型任务，可以减少线程切换的开销。

重量级锁（Heavyweight Locking）
特点：
重量级锁是一种互斥锁，它利用操作系统的同步机制来实现，因此开销较大。
当多个线程同时竞争同一个锁，且锁竞争时间较长时，轻量级锁会升级为重量级锁。
在重量级锁下，线程在获取锁失败后会进入阻塞状态，等待锁被释放后再重新尝试获取锁。
适用场景：
适用于锁竞争激烈的场景，以及需要确保数据一致性和完整性的场景。



为什么重量级锁消耗资源？
1. 线程状态切换（最主要的原因）
重量级锁在锁被占用时，会导致线程在用户态与内核态之间频繁切换。当一个线程尝试获取已被其他线程持有的重量级锁时，它会被操作系统阻塞，并从运行状态转换为阻塞状态。
这种状态的切换需要消耗CPU资源，并且可能导致线程的挂起和唤醒，进一步增加系统资源的消耗。

2. 上下文切换
上下文切换是指CPU从一个线程切换到另一个线程时，需要保存当前线程的执行状态（包括CPU寄存器、程序计数器等），并加载新线程的执行状态。由于重量级锁可能导致线程频繁地被阻塞和唤醒，因此会增加上下文切换的次数，进而消耗更多的CPU资源。

3. 操作系统介入
重量级锁的实现通常依赖于操作系统的互斥量（Mutex Lock）或其他同步机制。当一个线程需要获取重量级锁时，它必须向操作系统发送请求，由操作系统来管理锁的分配和释放。这种依赖操作系统的介入增加了系统的复杂性和资源消耗。

4. 锁竞争
在高并发场景下，多个线程可能同时尝试获取同一个重量级锁，导致锁竞争。锁竞争不仅会增加线程被阻塞和唤醒的次数，还会增加上下文切换的次数，从而进一步消耗系统资源。

5. 系统调用开销
每次线程尝试获取或释放重量级锁时，都可能涉及到系统调用。系统调用是用户态程序与内核态程序之间的接口，它需要消耗一定的时间和资源来完成。因此，重量级锁的使用会增加系统调用的次数和开销。


i++为什么不是原子性操作？
i++不是原子性操作，这主要是因为i++的操作过程包含了多个步骤，这些步骤在执行过程中可能被其他线程或操作中断，从而导致数据不一致性。

具体来说，i++的操作过程大致可以分为三个阶段：
读取阶段：首先，线程需要将变量i的当前值从主内存读取到自己的工作内存中。
修改阶段：然后，线程在工作内存中将读取到的i值加1。
写回阶段：最后，线程将修改后的i值写回主内存，以便其他线程可以访问到最新的值。

由于这三个阶段是分开的，因此在多线程环境下，就可能发生以下情况：
线程切换：在第一个线程完成读取和修改阶段，但尚未写回主内存时，如果发生线程切换，另一个线程可能会读取到尚未更新的i值，并进行自己的修改和写回操作。这样，第一个线程对i的修改就可能被覆盖，导致最终结果不是预期的。
缓存不一致：在多核处理器上，每个核心可能都有自己的缓存。如果多个核心上的线程同时操作同一个变量，就可能发生缓存不一致的问题。即使一个线程已经将修改后的值写回主内存，其他线程也可能因为缓存的延迟而未能立即看到最新的值。
为了解决这些问题，确保操作的原子性，Java提供了多种同步机制，如synchronized关键字、volatile关键字以及原子类（如AtomicInteger）等。这些机制可以在不同程度上保证操作的原子性和可见性，从而避免多线程环境下的数据不一致问题。

综上所述，i++不是原子性操作，主要是因为其操作过程包含了多个可能被中断的步骤。在需要保证操作原子性的场景中，应该使用Java提供的同步机制来确保数据的一致性和正确性。


为什么多线程能解决高并发问题？
多线程之所以能解决高并发问题，主要基于以下几个原因：

1.资源利用优化：
多线程允许程序同时执行多个任务，每个线程可以独立运行在不同的处理器核心上，从而充分利用多核CPU的计算资源。这样可以显著提高程序的执行效率，尤其是在处理大量独立任务时。

2.提高响应速度：
在多线程环境中，I/O密集型任务（如文件读写、网络请求等）可以通过异步方式处理，这意味着主线程可以继续执行其他任务而不必等待I/O操作完成。这样，程序可以同时处理多个请求，显著提高响应速度和处理能力。

3.并行处理能力：
多线程允许将任务分解为多个子任务，每个子任务由一个独立的线程处理。这样可以并行处理多个任务，特别是在处理复杂计算或大量数据时，可以显著减少总体处理时间。

4.提高系统吞吐量：
多线程能够同时处理多个用户请求，提高了系统的吞吐量。在高并发场景下，多线程可以有效地管理和分配资源，确保系统能够稳定地处理大量请求。

5.灵活的任务调度：
多线程允许操作系统和运行时环境根据当前的系统负载和线程优先级灵活调度任务。这确保了系统资源的高效利用，并可以根据需要动态调整线程的执行。

然而，多线程也带来了一些挑战，如线程同步、死锁、资源竞争等问题。因此，在设计多线程程序时，需要仔细考虑线程之间的交互和数据共享，以确保程序的正确性和性能。


Java异步线程：
Java 的异步线程是指在 Java 应用程序中，多个线程同时执行不同的任务，并且他们是独立的，互不影响，任意一个线程的完成不会影响其他线程的运行。
在Java 异步编程中，不同的线程可以在不需要阻塞等待的情况下并行执行，这样可以提高程序的执行效率。

线程异步需要考虑原子性：
在Java中异步编程中，线程原子性是需要考虑的因素。线程原子性指的是线程执行过程中不会被其他线程打断的特性。
如果不考虑线程原子性，那么会有可能出现线程在执行过程中对数据的不一致性问题。
因此，在异步编程中，为了保证数据的一致性，需要考虑线程的原子性。


原子性可以通过以下几种方式来实现：
使用synchronized关键字：synchronized关键字可以保证同一时刻只有一个线程可以执行被synchronized修饰的方法或者代码块，因此可以保证原子性。
使用Lock：Lock是Java提供的一种更加灵活的线程同步机制，它可以在方法或者代码块前后分别加锁和解锁，从而保证原子性。
使用Atomic类：Java提供了许多Atomic类，如AtomicInteger、AtomicLong等，这些类可以保证对基本数据类型的操作是原子的。
使用CountDownLatch 和 CyclicBarrier：这两个工具类可以用来控制线程的执行顺序，从而保证多线程下的原子性。
使用Semaphore：Semaphore是一种用来控制访问资源的数量的工具类，可以在保证原子性的同时，控制并发线程的数量。

Java异步线程的实现方式：
继承Thread类
class MyThread extends Thread {
    @Override
    public void run() {
        System.out.println("Thread is running");
    }
}

public class Main {
    public static void main(String[] args) {
        MyThread thread = new MyThread();
        thread.start(); // 启动线程
    }
}

实现Runnable接口
class MyRunnable implements Runnable {
    @Override
    public void run() {
        System.out.println("Runnable is running");
    }
}

public class Main {
    public static void main(String[] args) {
        Thread thread = new Thread(new MyRunnable());
        thread.start(); // 启动线程
    }
}

实现Callable接口并使用FutureTask
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.FutureTask;

class MyCallable implements Callable<String> {
    @Override
    public String call() throws Exception {
        return "Callable result";
    }
}

public class Main {
    public static void main(String[] args) {
        MyCallable callable = new MyCallable();
        FutureTask<String> futureTask = new FutureTask<>(callable);
        Thread thread = new Thread(futureTask);
        thread.start(); // 启动线程

        try {
            String result = futureTask.get(); // 获取结果
            System.out.println(result);
        } catch (InterruptedException | ExecutionException e) {
            e.printStackTrace();
        }
    }
}

使用ExecutorService：
ExecutorService提供了更高级的线程管理功能，可以管理线程池和异步任务。

import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

public class Main {
    public static void main(String[] args) {
        ExecutorService executor = Executors.newFixedThreadPool(2);
        executor.submit(() -> System.out.println("Task 1"));
        executor.submit(() -> System.out.println("Task 2"));
        executor.shutdown(); // 关闭线程池
    }
}


使用CompletableFuture：
CompletableFuture<Long> completableFuture = CompletableFuture.supplyAsync(() -> factorial(number));
while (!completableFuture.isDone()) {
    System.out.println("CompletableFuture is not finished yet...");
}
long result = completableFuture.get();

SpringBoot @Async异步：
@SpringBootApplication
@EnableAsync
public class StartApplication {

    public static void main(String[] args) {
        SpringApplication.run(StartApplication.class, args);
    }
}

使用ScheduledExecutorService：
ScheduledExecutorService可以用于调度任务，在未来某个时间点执行任务或周期性地执行任务。

import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class Main {
    public static void main(String[] args) {
        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        scheduler.schedule(() -> {
            System.out.println("Scheduled task");
        }, 5, TimeUnit.SECONDS);

        scheduler.scheduleAtFixedRate(() -> {
            System.out.println("Fixed rate task");
        }, 0, 10, TimeUnit.SECONDS);

        scheduler.scheduleWithFixedDelay(() -> {
            System.out.println("Fixed delay task");
        }, 0, 10, TimeUnit.SECONDS);
    }
}


使用自定义线程池：
在Java中，ExecutorService 接口是执行提交给它的 Runnable 任务的对象的方法，这些任务可以异步地执行。ThreadPoolExecutor 类是 ExecutorService 接口的一个具体实现，它允许你通过调整多个参数来灵活地配置线程池。
ThreadPoolExecutor 的构造方法有多种重载形式，但最常用和灵活的构造方法包含以下七个参数：

corePoolSize（核心线程数）：线程池中的核心线程数，即使这些线程处于空闲状态，它们也不会被销毁，除非设置了 allowCoreThreadTimeOut 为 true。
maximumPoolSize（最大线程数）：线程池中允许的最大线程数。当工作队列满了之后，如果线程数小于最大线程数，则会继续创建线程。
keepAliveTime（存活时间）：当线程数大于核心线程数时，这是多余空闲线程在终止前等待新任务的最长时间。
unit（时间单位）：keepAliveTime 参数的时间单位，例如 TimeUnit.SECONDS。
workQueue（工作队列）：用于在执行任务之前保存任务的队列。这个队列仅保存由 execute 方法提交的 Runnable 任务。
threadFactory（线程工厂）：用于创建新线程的工厂。通过自定义线程工厂，你可以设置线程名、优先级、守护状态等。
handler（拒绝策略）：当工作队列和线程池都达到饱和状态时（即工作队列已满且线程数达到最大线程数），线程池会拒绝新的任务。RejectedExecutionHandler 提供了几种处理策略，如抛出异常、直接忽略新任务、使用调用者线程处理任务或将任务放入一个等待队列中。

一个使用这些参数的 ThreadPoolExecutor 构造方法示例如下：
import java.util.concurrent.*;  
  
public class ThreadPoolExample {  
    public static void main(String[] args) {  
        int corePoolSize = 5;  
        int maximumPoolSize = 10;  
        long keepAliveTime = 1L;  
        TimeUnit unit = TimeUnit.SECONDS;  
        BlockingQueue<Runnable> workQueue = new ArrayBlockingQueue<>(100);  
        ThreadFactory threadFactory = Executors.defaultThreadFactory();  
        RejectedExecutionHandler handler = new ThreadPoolExecutor.AbortPolicy();  
  
        ThreadPoolExecutor executor = new ThreadPoolExecutor(  
                corePoolSize,   
                maximumPoolSize,   
                keepAliveTime,   
                unit,   
                workQueue,   
                threadFactory,   
                handler);  
  
        // 使用executor提交任务  
        executor.execute(() -> {  
            // 你的任务逻辑  
        });  
  
        // 优雅关闭线程池  
        executor.shutdown();  
    }  
}
这个示例展示了如何配置一个 ThreadPoolExecutor 实例，包括核心线程数、最大线程数、存活时间、工作队列、线程工厂和拒绝策略。

使用Guava：
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.ListeningExecutorService;
import com.google.common.util.concurrent.MoreExecutors;

import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;

public class Main {
    public static void main(String[] args) {
        ListeningExecutorService executor = MoreExecutors.newDirectExecutorService();

        Callable<String> task = () -> {
            Thread.sleep(2000); // 模拟耗时任务
            return "Task result";
        };

        ListenableFuture<String> future = executor.submit(task);

        Futures.addCallback(future, new FutureCallback<String>() {
            @Override
            public void onSuccess(String result) {
                System.out.println("Task result: " + result);
            }

            @Override
            public void onFailure(Throwable t) {
                t.printStackTrace();
            }
        }, executor);
    }
}
在这个示例中，使用了Futures.addCallback()方法注册了一个回调函数，当任务成功完成时调用onSuccess()方法，当任务失败时调用onFailure()方法。
使用Guava的ListenableFuture和ListeningExecutorService接口可以更加方便地处理异步任务的结果，并且提供了更丰富的回调机制来处理任务的执行完成或失败情况。

如何创建动态线程池？
创建动态线程池的步骤
在Java中，创建动态线程池主要依赖于java.util.concurrent包中的Executors类，该类提供了多种静态方法来创建不同类型的线程池。动态线程池通常指的是那些可以根据工作负载自动调整其线程数量的线程池，比如ThreadPoolExecutor。

创建动态线程池的核心参数：
确定线程池的核心线程数（corePoolSize）：这是线程池中始终保持存活的线程数，即使它们处于空闲状态。
确定最大线程数（maximumPoolSize）：线程池中允许的最大线程数。当工作队列已满时，线程池会创建新的线程，直到达到这个限制。
确定工作队列（workQueue）：用于保存等待执行的任务的阻塞队列。常用的有LinkedBlockingQueue、ArrayBlockingQueue等。
设置线程存活时间（keepAliveTime）：当线程数大于核心线程数时，这是多余空闲线程在终止前等待新任务的最长时间。
设置线程工厂（ThreadFactory）：用于创建新线程的工厂，可以自定义线程的创建过程。
设置拒绝策略（RejectedExecutionHandler）：当线程池和任务队列都满了时，对新任务的处理策略。

示例代码
以下是一个使用ThreadPoolExecutor创建动态线程池的示例：

import java.util.concurrent.ArrayBlockingQueue;  
import java.util.concurrent.ThreadPoolExecutor;  
import java.util.concurrent.TimeUnit;  
import java.util.concurrent.Executors;  
  
public class DynamicThreadPoolExample {  
  
    public static void main(String[] args) {  
        // 创建一个线程池  
        ThreadPoolExecutor executor = new ThreadPoolExecutor(  
            2, // 核心线程数  
            5, // 最大线程数  
            10, // 线程存活时间  
            TimeUnit.SECONDS, // 存活时间单位  
            new ArrayBlockingQueue<>(10), // 工作队列  
            Executors.defaultThreadFactory(), // 线程工厂  
            new ThreadPoolExecutor.CallerRunsPolicy() // 拒绝策略  
        );  
  
        // 提交任务  
        for (int i = 0; i < 15; i++) {  
            int taskId = i;  
            executor.execute(() -> {  
                System.out.println(Thread.currentThread().getName() + " is processing " + taskId);  
                try {  
                    Thread.sleep(1000); // 模拟耗时任务  
                } catch (InterruptedException e) {  
                    Thread.currentThread().interrupt();  
                }  
            });  
        }  
  
        // 关闭线程池（不再接受新任务，但已提交的任务会继续执行）  
        executor.shutdown();  
  
        // 等待所有任务完成  
        while (!executor.isTerminated()) {  
            // 等待一段时间  
        }  
  
        System.out.println("All tasks completed.");  
    }  
}
线程池的常用方法和适用场景
常用方法：
execute(Runnable command): 提交一个任务用于执行，该任务将等待一个空闲的线程。
submit(Callable<T> task): 提交一个返回值的任务用于执行，并返回一个表示该任务的Future。
shutdown(): 启动线程池的关闭序列，不再接受新任务，但已提交的任务会继续执行。
shutdownNow(): 尝试停止所有正在执行的活动任务，停止处理正在等待的任务，并返回等待执行的任务列表。

适用场景：
当你需要并行处理多个任务时。
当任务执行时间可能较长，且任务数量动态变化时。
当你希望控制线程池中的线程数量以优化资源使用时。

避免常见错误
避免使用无界队列：无界队列可能会导致内存溢出，因为线程池会不断尝试添加新任务到队列中。
合理设置线程数：过多或过少的线程数都可能影响性能。
及时关闭线程池：确保在不再需要时关闭线程池，以释放资源。
正确处理异常：确保任务执行过程中抛出的异常被妥善处理，避免影响线程池的正常工作。


Java多线程中的CAS（无锁）：
Java多线程中的CAS（Compare-And-Swap）是一种用于实现无锁编程的技术。CAS操作包含三个操作数：内存位置（V）、预期原值（A）和新值（B）。
如果内存位置的值与预期原值相匹配，那么处理器会自动将该位置值更新为新值，整个操作是原子的。这意味着，CAS操作在执行期间不会被线程调度机制中断，从而保证了操作的原子性。

CAS的应用场景
CAS操作主要用于实现线程安全的操作，尤其是在多线程环境下对共享变量的修改。它常用于实现自旋锁、原子变量（如AtomicInteger、AtomicLong等）以及并发集合等。

CAS的优缺点
优点
非阻塞算法：CAS操作是一种非阻塞算法，它不会造成线程挂起或阻塞，因此不会引起线程上下文切换，适用于读多写少的场景。
细粒度锁定：CAS能够实现对共享资源的细粒度锁定，提高了程序的并发性能。
缺点
ABA问题：如果两个线程同时修改一个变量，一个线程将变量从A改为B，然后又改回A，此时另一个线程执行CAS操作发现变量仍然是A，就认为没有变化，实际上变量已经被修改过了。
循环时间长开销大：如果CAS操作一直不成功，会一直处于自旋状态，这会消耗CPU资源。
只能保证一个共享变量的原子操作：CAS操作通常只涉及到一个共享变量的修改，对于复杂场景（如多个共享变量）需要其他机制来辅助实现。

Java中的CAS实现
Java中提供了java.util.concurrent.atomic包，里面包含了多个使用CAS操作的原子类，如AtomicInteger、AtomicLong、AtomicReference等。这些类提供了丰富的原子操作，如getAndIncrement()、compareAndSet(expectedValue, updateValue)等。

示例代码
以下是一个使用AtomicInteger的示例，展示了如何利用CAS实现线程安全的自增操作：

import java.util.concurrent.atomic.AtomicInteger;  
  
public class CASExample {  
    private static AtomicInteger count = new AtomicInteger(0);  
  
    public static void main(String[] args) {  
        for (int i = 0; i < 1000; i++) {  
            new Thread(() -> {  
                count.incrementAndGet(); // 底层使用CAS实现  
            }).start();  
        }  
  
        // 等待所有线程执行完毕  
        while (Thread.activeCount() > 2) {  
            Thread.yield();  
        }  
  
        System.out.println("Final count: " + count.get()); // 输出应该是1000  
    }  
}
在这个例子中，incrementAndGet()方法内部使用了CAS操作来确保对count变量的自增操作是线程安全的。


Java多线程的ThreadLocal：
ThreadLocal 是 Java 中一个非常有用的类，它提供了一种线程局部变量的能力。这意味着每个使用该变量的线程都会有一个该变量值的独立副本，这些副本之间互不影响，从而避免了多线程环境下的数据共享问题。ThreadLocal 非常适合用于那些需要保持线程隔离性的场景，比如每个线程需要有自己的数据库连接、用户会话信息等。

基本原理
ThreadLocal 内部维护了一个以 Thread 对象为键，以线程局部变量为值的 HashMap（实际上，在 JDK 8 及以后版本中，ThreadLocal 的实现已经不再是简单的 HashMap，而是使用了更高效的 ThreadLocalMap 和 Entry 数组来存储数据，以减少哈希冲突和提高性能）。
当线程访问某个 ThreadLocal 变量时，ThreadLocal 会通过当前线程作为键来查找对应的值。如果找到了，就返回该值；如果没有找到，就根据 ThreadLocal 的初始值设置规则来返回初始值（如果设置了初始值的话）。

主要方法
void set(T value): 设置当前线程的线程局部变量的值。
T get(): 返回当前线程所对应的线程局部变量的值。
void remove(): 移除当前线程所对应的线程局部变量的值。
protected T initialValue(): 返回该线程局部变量的初始值，该方法是一个受保护的方法，可以被子类覆盖以返回特定的初始值。默认情况下，返回 null。

使用场景
数据库连接管理：每个线程都可以有自己的数据库连接，避免了数据库连接在多线程环境下的共享和同步问题。
用户会话管理：在 Web 应用中，每个用户请求都会由不同的线程处理，使用 ThreadLocal 可以为每个线程保存用户的会话信息，从而避免会话信息的混乱。
事务管理：在需要事务支持的应用中，可以使用 ThreadLocal 来保存每个线程的事务上下文信息。

注意事项
内存泄漏：由于 ThreadLocal 的生命周期与线程的生命周期相同，如果线程长时间运行，并且 ThreadLocal 变量被设置为非 null 后一直没有被清理（即没有调用 remove() 方法），那么这些 ThreadLocal 变量就会一直存在，从而导致内存泄漏。因此，在使用完 ThreadLocal 变量后，应该显式地调用 remove() 方法来清理。
性能问题：虽然 ThreadLocal 提供了线程隔离的能力，但是过多的 ThreadLocal 变量会增加内存的使用量，并且由于 ThreadLocalMap 的实现方式（基于数组和线性探测法解决哈希冲突），在极端情况下可能会导致性能下降。

示例代码
public class ThreadLocalExample {  
  
    private static final ThreadLocal<Integer> threadLocal = new ThreadLocal<Integer>() {  
        @Override  
        protected Integer initialValue() {  
            return 0;  
        }  
    };  
  
    public static void main(String[] args) throws InterruptedException {  
        Thread thread1 = new Thread(() -> {  
            System.out.println(Thread.currentThread().getName() + " 初始值: " + threadLocal.get());  
            threadLocal.set(1);  
            System.out.println(Thread.currentThread().getName() + " 修改后的值: " + threadLocal.get());  
        });  
  
        Thread thread2 = new Thread(() -> {  
            System.out.println(Thread.currentThread().getName() + " 初始值: " + threadLocal.get());  
            threadLocal.set(2);  
            System.out.println(Thread.currentThread().getName() + " 修改后的值: " + threadLocal.get());  
        });  
  
        thread1.start();  
        thread2.start();  
  
        thread1.join();  
        thread2.join();  
  
        // 注意：这里不会打印出任何值，因为threadLocal是线程隔离的  
        // System.out.println(threadLocal.get()); // 可能会是null，取决于JVM的实现  
    }  
}
在这个示例中，thread1 和 thread2 分别设置了它们自己的 ThreadLocal 变量的值，并且这些值之间互不影响。

JUC：
Java并发包（Java Util Concurrent，简称JUC）是Java提供的一组用于并发编程的类和接口，旨在简化多线程编程并提高并发程序的性能和可靠性。
JUC包位于java.util.concurrent和java.util.concurrent.atomic包中，提供了丰富的工具来处理线程同步、线程池、并发集合等。

以下是对JUC的详细阐述：
1. 线程池
线程池是用于管理和复用线程资源的机制。通过线程池，可以避免频繁创建和销毁线程带来的开销，提高系统性能。

Executor接口：定义了执行任务的基本方法。
ExecutorService接口：扩展了Executor，提供了更多的方法来管理任务的提交和执行。
Executors工厂类：提供了一些静态方法来创建不同类型的线程池，如固定线程池、缓存线程池、单线程池和定时任务线程池。


ExecutorService executor = Executors.newFixedThreadPool(5);
executor.submit(() -> {
    // 任务代码
});
executor.shutdown();

2. 并发集合
JUC包提供了一些线程安全的集合类，用于在多线程环境下安全地操作集合。

ConcurrentHashMap：一个高效的线程安全的哈希表。
CopyOnWriteArrayList：适用于读多写少的场景，每次写操作都会复制整个底层数组。
CopyOnWriteArraySet：基于CopyOnWriteArrayList实现的集合。
ConcurrentLinkedQueue：一个基于链表的无界线程安全队列。
BlockingQueue：支持阻塞操作的队列接口及其实现，如ArrayBlockingQueue、LinkedBlockingQueue等。

ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();
map.put("key", 1);

3. 同步工具类
JUC提供了一些高级的同步工具类，用于控制线程之间的协作。

CountDownLatch：允许一个或多个线程等待其他线程完成操作。
CyclicBarrier：使一组线程相互等待，直到到达某个公共屏障点。
Semaphore：控制访问共享资源的线程数量。
Exchanger：两个线程可以在这个同步点交换数据。

CountDownLatch latch = new CountDownLatch(3);
for (int i = 0; i < 3; i++) {
    new Thread(() -> {
        // 执行任务
        latch.countDown();
    }).start();
}
latch.await();

4. 锁和条件
JUC包提供了更灵活和强大的锁机制，包括显式锁和条件对象。

ReentrantLock：一个可重入的互斥锁，与synchronized相比，它提供了更多的功能，如尝试锁定、定时锁定和中断锁定。
ReentrantReadWriteLock：读写锁，允许多个读线程同时访问，但写线程访问时独占。
StampedLock：提供了更高效的读写锁机制。
Condition接口：与Lock结合使用，允许线程在满足特定条件时被唤醒。

ReentrantLock lock = new ReentrantLock();
lock.lock();
try {
    // 临界区代码
} finally {
    lock.unlock();
}

5. 原子变量
JUC包提供了一些类来支持原子操作，避免了使用锁。

AtomicInteger、AtomicLong：提供了对整数和长整型的原子操作。
AtomicReference：提供了对对象引用的原子操作。
AtomicBoolean：提供了对布尔值的原子操作。

AtomicInteger atomicInteger = new AtomicInteger(0);
atomicInteger.incrementAndGet();

6. Fork/Join框架
用于并行执行任务的框架，特别适合分治算法。

ForkJoinPool：执行ForkJoinTask的线程池。
RecursiveTask：需要返回结果的任务。
RecursiveAction：不需要返回结果的任务。

ForkJoinPool pool = new ForkJoinPool();
pool.invoke(new RecursiveTaskExample());



Java中哪些集合类型是线程安全的，哪些集合类型是线程不安全的？如何区分？
在Java中，集合类型可以分为线程安全和线程不安全的集合。以下是主要的线程安全和线程不安全的集合类型以及如何区分它们的方法：

线程安全的集合类型
1.Vector
    - Vector 是一个同步的集合类，因此是线程安全的。
    - 它的所有方法都是通过synchronized关键字实现的。

2.Stack
    - Stack是Vector的一个子类，因此也是线程安全的。

3.Hashtable
    - Hashtable是一个同步的键值对集合类，因此是线程安全的。
    - 它的所有方法都通过synchronized关键字实现。

4.Collections.synchronizedXXX方法返回的集合
    - Collections类提供了一些静态方法，可以将非线程安全的集合包装成线程安全的集合。
    - 例如：Collections.synchronizedList(new ArrayList<>())，Collections.synchronizedSet(new HashSet<>())，Collections.synchronizedMap(new HashMap<>())等。

5.Concurrent包下的集合类
    - java.util.concurrent包提供了一些线程安全的集合类，如：
        - ConcurrentHashMap
        - CopyOnWriteArrayList
        - CopyOnWriteArraySet
        - ConcurrentLinkedQueue
        - ConcurrentSkipListMap
        - ConcurrentSkipListSet

线程不安全的集合类型
1. ArrayList
    - ArrayList 是非同步的集合类，因此是线程不安全的。

2. LinkedList
    - LinkedList 是非同步的集合类，因此是线程不安全的。

3. HashSet
    - HashSet 是非同步的集合类，因此是线程不安全的。

4. TreeSet
    - TreeSet 是非同步的集合类，因此是线程不安全的。

5. HashMap
    - HashMap 是非同步的集合类，因此是线程不安全的。

6. TreeMap
    - TreeMap 是非同步的集合类，因此是线程不安全的。

如何区分线程安全和线程不安全的集合
1. 查看API文档
    - Java官方API文档会明确说明集合类是否是线程安全的。

2. 查看源码
    - 查看集合类的源码，线程安全的集合类通常使用synchronized关键字来同步方法或代码块。

3. 集合类的包
    - java.util.concurrent包下的集合类大多是线程安全的。

4. 集合类的名称
    - 一些集合类如Vector、Hashtable等，它们历史上就被设计为线程安全的。

5. 使用工具类Collections
    - Collections工具类提供了多种方法来创建线程安全的集合，如Collections.synchronizedList、Collections.synchronizedSet等。

在实际开发中，如果需要使用线程安全的集合类型，建议优先考虑java.util.concurrent包下的集合类，因为它们在保证线程安全的同时，通常具有更好的性能和扩展性。


操作系统并发区别于Java并发吗？Java并发是否用了操作系统并发？
操作系统并发和Java并发是两个不同但相关的概念，它们在多线程编程中发挥不同的作用，但也有一些联系。

1. 操作系统并发：
   - 操作系统并发是指操作系统的内核和调度器如何管理和调度计算机上的进程和线程。
   - 操作系统负责分配CPU时间片、管理进程和线程的状态、实现上下文切换等，以便多个进程和线程可以在同一台计算机上并发执行。
   - 操作系统提供了基本的多线程支持，但它通常是底层的，与具体编程语言无关。

2. Java并发：
   - Java并发是一种编程范式，用于在Java编程语言中实现多线程和多任务处理。
   - Java提供了丰富的多线程编程工具和库，包括 java.lang.Thread 类、同步机制（如 synchronized 关键字）、线程池、原子操作类、并发集合等。
   - Java并发是建立在操作系统并发之上的，它通过Java虚拟机（JVM）实现多线程调度和管理。

虽然Java并发建立在操作系统并发之上，但它具有一些独立的特点和抽象，使得多线程编程更容易和可移植。
例如，Java的线程模型抽象了底层的操作系统线程，使开发人员可以更轻松地创建和管理线程，而无需关心操作系统的细节。
Java并发通常使用了操作系统提供的底层线程支持，但它也在其之上构建了更高级别的抽象，以便于开发人员处理并发编程。 
Java的并发库和工具旨在提供更高级别的抽象，帮助开发人员解决常见的并发问题，如同步、锁、线程安全性等。
这些工具和库在跨平台开发中尤其有用，因为它们隐藏了操作系统差异，使得Java程序在不同操作系统上的行为更加一致。

总的来说，操作系统并发和Java并发是相关的，但它们在概念上和实现上存在差异。 Java并发建立在操作系统并发之上，提供了更高级别的抽象和工具，以便于开发人员编写多线程应用程序。
操作系统并发管理底层的线程和进程，而Java并发是编写多线程Java应用程序的一种编程范式。

Java集合：
集合 → 数组 ：toArray()
数组 → 集合 ：调用Array类的静态方法asList()

遍历集合的方法：
1.迭代器
Collection coll = new ArrayList();
Iterator iterator = coll.iterator();
while(iterator.hasNext()){
	System.out.println(iterator.next());
};
2.增强型for循环
for (集合元素的类型 局部变量 ：集合对象)


Java集合可分为Collection和Set两个体系：
Collection接口：
Collection 接口是 Java 集合框架（Java Collections Framework）中的一个根接口，它位于 java.util 包中。Collection 接口为各种具体的集合提供了统一的访问和操作方式，比如添加（add）、删除（remove）、遍历（通过迭代器 Iterator）等。
Collection 接口是其他集合接口父类，Collection接口有三种子类型，分别是List、Set 和 Queue。
List：表示一个有序的元素集合，可以包含重复的元素。Set：表示一个不包含重复元素的集合。Queue：表示一组先进先出（FIFO）的元素。

List：元素有序、可重复的集合 
>ArrayList:作为list接口的主要实现类；访问速度快，因为可以直接通过下标访问元素；
插入和删除操作需要移动元素，因此相对较慢；线程不安全的，效率高；底层使用Object[]elementData存储

Array和ArrayList的区别？
Array（数组）和ArrayList在Java中都是用来存储元素的数据结构，但它们在多个方面存在显著的区别。
以下是它们之间主要区别的详细解释：

1. 长度与大小
Array（数组）：数组的长度在定义后是不可变的。如果需要在数组中添加额外的元素，必须创建一个新的数组，其长度比原数组至少多出一个位置，然后将原数组的内容复制到新数组中，并在新数组中添加新的元素。
ArrayList：ArrayList是一个动态数组，其大小可以随着元素的添加和删除而自动调整。它内部维护了一个动态的Object数组，当需要添加新元素而数组空间不足时，ArrayList会自动扩容（通常是扩容为原来的1.5倍），并复制原数组的内容到新数组中，然后再添加新元素。

2. 存储内容
Array：数组可以存储基本数据类型（如int、double等）和对象类型。
ArrayList：ArrayList只能存储对象类型。对于基本数据类型的存储，ArrayList会将其自动装箱为对应的包装类对象（如Integer、Double等）。

3. 初始化与声明
Array：数组的声明需要指定类型和长度，例如int[] arr = new int[5];。初始化可以在声明时完成，也可以通过循环或直接赋值进行。
ArrayList：ArrayList的声明需要指定泛型类型（即存储对象的类型），但不需要指定长度，例如ArrayList<Integer> list = new ArrayList<>();。初始化通常是通过调用add()方法逐个添加元素。

4. 遍历与访问
Array：数组可以通过下标直接访问元素，也可以通过循环（如for循环、foreach循环）遍历数组中的所有元素。
ArrayList：ArrayList同样支持通过下标访问元素（使用get(int index)方法），并且也支持通过循环遍历元素。此外，ArrayList还提供了迭代器（Iterator）来遍历元素。

5. 性能与适用场景
Array：由于数组的长度固定，因此在已知元素数量且不需要动态改变大小的情况下，数组的性能通常优于ArrayList。数组也更适合进行大量的随机访问操作。
ArrayList：ArrayList适用于需要动态改变集合大小的情况。虽然它在动态扩容时会有一定的性能开销，但提供了更灵活的操作方式，如添加、删除和查找元素等。


ArrayList的源码分析：
构造器：
1.空参构造ArrayList()
public class ArrayList<E> {
    /**
     * 默认初始容量
     */
    private static final int DEFAULT_CAPACITY = 10;
    /**
     * 空数组
     */
    private static final Object[] EMPTY_ELEMENTDATA = {};
    /**
     * 默认容量的空数组
     */
    private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};
    /**
     * 集合真正存储数组元素的数组
     */
    transient Object[] elementData;
    /**
     * 集合的大小
     */
    private int size;
    public ArrayList() {
        this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;
    }
}
通过空参构造方法创建集合对象并未构造一个初始容量为十的空列表，仅仅 DEFAULTCAPACITY_EMPTY_ELEMENTDATA 的地址赋值给 elementData

2.指定容量构造器ArrayList(int initialCapacity)：根据 ArrayList 构造方法参数创建指定长度的数组
3.创建集合的方式创建构造器ArrayList(Collection<? extends E> c)：将集合构造中的集合对象转成数组,且将数组的地址赋值给elementData

插入方法add():
1.添加单个元素public boolean add(E e) ：
public boolean add(E e) {
    // 1. 检测是否需要扩容
    ensureCapacityInternal(size + 1);
    // 2. 将新元素插入序列尾部
    elementData[size++] = e;
    return true;
}

private void ensureCapacityInternal(int minCapacity) {
    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));
}

//通过max方法比较默认容量和传入的容量minCapacity，返回一个最大的数字作为容量
private static int calculateCapacity(Object[] elementData, int minCapacity) {
    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
        return Math.max(DEFAULT_CAPACITY, minCapacity);
    }
    return minCapacity;
}

//如果最小容量大于elementData的长度，进行扩容
private void ensureExplicitCapacity(int minCapacity) {
    modCount++;
    // overflow-conscious code
    if (minCapacity - elementData.length > 0)
        grow(minCapacity);
}

//
private void grow(int minCapacity) {
    // overflow-conscious code
    int oldCapacity = elementData.length;
    int newCapacity = oldCapacity + (oldCapacity >> 1);
    if (newCapacity - minCapacity < 0)
        newCapacity = minCapacity;
    if (newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);
    // minCapacity is usually close to size, so this is a win:
    elementData = Arrays.copyOf(elementData, newCapacity);
}

2.在指定索引处添加元素 public void add(int index, E element) ：
与无参的add()方法相比，这个方法的区别主要在于：它不是简单地在数组末尾顺序添加元素，而是允许在指定位置插入元素。这一过程额外涉及了一个数组拷贝的操作。
以数组[1,2,3,4,5]为例，若需在索引1的位置插入数字10，则首先会从索引1开始复制数组元素，将后续的数字（即2,3,4,5）都向后移动一位，得到[1,2,2,3,4,5]（这里仅为说明过程，实际操作中不会真的重复元素）。
随后，将索引1位置的数字（原为2）替换为10，最终数组变为[1,10,2,3,4,5]。这一操作的时间复杂度为O(n)，因为最坏的情况下（如在索引0处插入），整个数组的所有元素都需要被移动一位。


>LinkedList：对于频繁的插入、删除操作，使用效率比ArrayList高；插入和删除操作非常快，因为不需要移动元素；
访问元素相对较慢，因为需要从头部或尾部开始遍历；
底层使用双向链表存储

LinkedList的源码分析：
构造器：
1.空参构造LinkedList
public class LinkedList<E>{
    transient int size = 0;

    /**
     * 创建一个指针，指向头节点
     * 不变量: (first == null && last == null) ||
     *            (first.prev == null && first.item != null)
     */
    transient Node<E> first;

    /**
     * 创建一个指针，指向尾节点
     * 不变量: (first == null && last == null) ||
     *            (last.next == null && last.item != null)
     */
    transient Node<E> last;

    public LinkedList() {
    }

    private static class Node<E> {
    E item;
    Node<E> next;
    Node<E> prev;

    Node(Node<E> prev, E element, Node<E> next) {
        this.item = element;
        this.next = next;
        this.prev = prev;
    }
    }


}
通过空参构造方法创建一个双向链表，使其长度初始化为0。Node是一个静态内部类，用于表示链表中的节点，item为存储内容，next存下一个节点地址，prev存上一个节点地址。
小结：jdk7中的ArrayList对象创建类似于单列模式的饿汉式；而jdk8中的ArrayList对象创建类似于单列模式的懒汉式，延迟了数组的创建，节省内存

2.使用集合创造LinkedList
/**
 * 这个方法或构造函数创建了一个新的列表（例如ArrayList、LinkedList等），并将指定的集合（Collection）中的所有元素按照该集合迭代器（Iterator）返回的顺序添加到这个新列表中。
 *
 * @param  c 集合里的元素会被替换到此列表里
 * @throws NullPointerException 如果集合为空
 */
public LinkedList(Collection<? extends E> c) {
    this();
    addAll(c);
}
通过调用空参构造器的方法创建linkedlist，并通过addAll方法将集合中的全部元素添加到列表中

插入方法add：
1.添加单个元素public boolean add(E e) ：在链表的尾部插入元素
public boolean add(E e) {
    linkLast(e);
    return true;
}

void linkLast(E e) {
    final Node<E> l = last;
    final Node<E> newNode = new Node<>(l, e, null);
    last = newNode;
    if (l == null)
        first = newNode;
    else
        l.next = newNode;
    size++;
    modCount++;
}

2.添加多个元素
public boolean addAll(int index, Collection<? extends E> c) {
    checkPositionIndex(index);

    Object[] a = c.toArray();
    int numNew = a.length;
    if (numNew == 0)
        return false;

    Node<E> pred, succ;
    if (index == size) {
        succ = null;
        pred = last;
    } else {
        succ = node(index);
        pred = succ.prev;
    }

    for (Object o : a) {
        @SuppressWarnings("unchecked") E e = (E) o;
        Node<E> newNode = new Node<>(pred, e, null);
        if (pred == null)
            first = newNode;
        else
            pred.next = newNode;
        pred = newNode;
    }

    if (succ == null) {
        last = pred;
    } else {
        pred.next = succ;
        succ.prev = pred;
    }

    size += numNew;
    modCount++;
    return true;
}

private void checkPositionIndex(int index) {
    if (!isPositionIndex(index))
        throw new IndexOutOfBoundsException(outOfBoundsMsg(index));
}

private boolean isPositionIndex(int index) {
    return index >= 0 && index <= size;
}

①检查索引是否在范围内，不在则抛出异常
②然后将加入的内容转化成数组；创建两个node的对象pred，succ；前者代表加入内容的前一个节点，后者表示加入位置的地址，如果index == size也就是是从链表尾部加入，那么succ就是null；否则succ就是第index个节点而pred就是第index前的那个节点
③遍历数组创建双链表

>Vector：作为list接口的古老实现类；线程安全的，效率低；由于同步操作，Vector的性能比ArrayList差；底层使用Object[]elementData存储

>Stack:Stack是Vector的一个子类，用于实现后进先出（LIFO）的堆栈；
提供了额外的后进先出功能，如push(E item)、pop()、peek()等方法；
Stack继承自Vector，因此它也继承了Vector的线程安全性，但同样存在性能问题；
在实际应用中，推荐使用Deque接口的实现类（如ArrayDeque）来代替Stack，因为Deque提供了更丰富的操作，并且性能更好。

>CopyOnWriteArrayList:一个线程安全的变体，适用于读多写少的并发场景；
在写操作时（如add、set等），通过复制底层数组的方式来实现，因此写操作代价昂贵，但读操作非常快，且无需同步。


Set：元素无序、不可重复的集合
>HashSet:作为Set接口的主要实现类；线程不安全的；
可以存储null值（底层：数组+链表）；
HashSet不保留插入顺序，因为它是基于哈希表实现的，哈希表的元素是无序的。

HashSet的源码解析：
构造器：
1.空参构造HashSet
public class HashSet<E>{
    static final long serialVersionUID = -5024744406713321676L;

    private transient HashMap<E,Object> map;

    // 在底层映射中，与对象关联的哑值（或占位值）
    private static final Object PRESENT = new Object();

    /**
     * 构造一个新的空的集合时，背后使用的HashMap实例拥有默认的初始容量（16）和加载因子（0.75）
     */
    public HashSet() {
        map = new HashMap<>();
    }
}    
HashSet底层创建了一个HashMap，默认的初始容量（16）、加载因子（0.75）。HashSet的增删查改都是在HashMap上进行的。

>LinkedHashSet：作为HashSet的一个子类；
内部通过LinkedHashMap实现，它在保留HashSet特性的基础上，通过维护一个双向链表来记录元素的插入顺序；
遍历其内部数据时，可以按照添加的顺序遍历；
LinkedHashSet 是线程不安全的，如果在多线程环境中使用，需要考虑线程同步的问题，或者考虑使用线程安全的集合类如 ConcurrentLinkedHashSet。

LinkedHashSet的源码解析：
构造器：
1.空参构造LinkedHashSet
public class LinkedHashSet<E> extends HashSet<E> implements Set<E>, Cloneable, java.io.Serializable {
    public LinkedHashSet() {
        super(16, .75f, true);
    }
}
LinkedHashSet底层创建了一个HashSet，默认初始容量为16，加载因子为0.75。

LinkedHashSet添加元素的方法:
public boolean add(E e) {
    return map.put(e, PRESENT)==null;
}

//put方法。value与map中存在的key相关联，如果map中已经有值，旧值将会被覆盖。
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

//计算哈希值
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

//put方法具体实现。
首先调用元素a所在类的hashcode()方法，计算元素a的哈希值，此哈希值接着通过某种算法计算出在HashSet底层数组中的存放位置(即为: 索引位置)，判断数组此位置上是否已经有元素:
如果此位置上没有其他元素，则元素a添加成功。--->情况1
如果此位置上有其他元素b(或以链表形式存在的多个元素)，则比较元素a与元素b的hash值:如果hash值不相同，则元素a添加成功。--->情况2
如果hash值相同，进而需要调用元素a所在类的equlas()方法:equaLs()返回true，元素a添加失败equaLs()返回false，则元素a添加成功。--->情况3

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
注意：
对于添加成功的情况2和情况3而言: 元素 与已经存在指定索引位置上数据以链表的方式存储
jdk 7 : 元素a放到数组中，指向原来的元素。jdk 8 : 原来的元素在数组中，指向元素a	
对于Set中相同元素的比较：先比较两个元素的哈希值，然后再比较两个元素是否相同


>TreeSet：只能添加同一类型的对象；
基于红黑树（一种自平衡二叉查找树）实现，能够确保集合元素处于排序状态；
不允许使用null元素

TreeSet的源码解析：
构造器：
1.空参构造TreeSet
public class TreeSet<E>{
    private transient NavigableMap<E,Object> m;

    // 在底层映射中，与对象关联的哑值（或占位值）
    private static final Object PRESENT = new Object();

    public TreeSet() {
        this(new TreeMap<E,Object>());
    }
}
TreeSet底层创建了一个TreeMap


Map接口：双列数据，存储的是键值对。Map接口不继承Collection接口。在Java集合框架中，Map接口和Collection接口是两个并行的根接口，它们之间没有继承关系。
>HashMap:最常用的Map实现类，基于哈希表实现，不保证映射的顺序。允许使用null值和null键（但最多只能有一个null键）；
具有较快的访问速度，时间复杂度接近O(1)；非线程安全，如果多个线程同时访问且至少有一个线程修改了映射，则需要外部同步。

HashMap的源码解析:
1.空参构造HashMap()
/**
 * 构造一个空的hashmap拥有16的默认容量和0.75的负载因子
 */
public HashMap() {
    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
}

put方法：
1.put方法put(K key, V value)
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}
与HashSet一致。HashSet底层运用了HashMap。
首先，调用key1所在类的hashCode()计算key1哈希值，此哈希值经过某种算法计算以后，得到在Entry数组中的存放位置。
	如果此位置上的数据为空，此时的key1-value1添加成功。 ---- 情况1
	如果此位置上的数据不为空，(意味着此位置上存在一个或多个数据(以链表形式存在)，
	比较key1和已经存在的一个或多个数的哈希值:
		如果key1的哈希值与已经存在的数据的哈希值都不相同，此时key1-value1添加成功。----情况2
		如key1的哈希值和已经存在的某一个数(key2-value2)的哈希值相同，继续比较: 调用key1所在类的equals(key2)
			如果equals()返回false: 此时key1-value1添加成功。---- 情况3
			如果equals()返回true: 使用vaLue1 vaLue2.

补充:关于情况2和情况3: 此时key1-value1和原来的数据以链表的方式存储
在不断的添加过程中，会涉及到扩容问题，默认的扩容方式:扩容为原来容量的2倍，并将原有的数据复制过来。

在HashMap中，如果两个键的hashcode相同，如何取值？
当你尝试通过键来从HashMap中取值时，HashMap会首先计算该键的hashCode，然后使用这个hashCode来定位到对应的桶（bucket）位置。
在桶内，如果存在链表或红黑树，那么HashMap会遍历这个数据结构，对每个元素调用equals方法来检查是否找到了正确的键。

具体来说，取值过程大致如下：
1.计算键的hashCode：首先，HashMap会对你提供的键调用hashCode()方法来得到一个整数。
2.定位桶：然后，HashMap会使用这个hashCode值和一个特定的哈希函数（通常是取模运算hashCode % capacity，其中capacity是HashMap的容量）来确定键应该被放在哪个桶里。
3.处理哈希冲突：
    如果桶内没有元素，那么直接认为没有找到对应的键。
    如果桶内有链表或红黑树：
    对于链表：HashMap会遍历链表，对每个元素调用equals()方法来比较键是否相等。
    对于红黑树：HashMap会利用红黑树的特性来高效地查找键。
    如果找到了相等的键，则返回该键对应的值。
    如果没有找到相等的键，则返回null（假设HashMap允许null值）。
4.返回值：如果找到了对应的键，则返回其对应的值；如果没有找到，则返回null（或抛出异常，如果HashMap不允许null值，并且你尝试获取一个不存在的键的值时）。

jdk8相较于jdk7在底层安现方面的不同:
1.new HashMap(): 底层没有创建一个长度为16的数组
2.jdk8底层的数组是: Node[]，而Entry[]首次调用put()方法时，底层创建长度为16的数组
3.jdk7底层结构只有:数组+链表。jdk8中底层结构: 数组+链表+红黑树。
当数组的某一个索引位置上的元素以链表形式存在的数个数 >8 且当前数组的长度>64时此时此索引位置上的所有数据改为使用红黑树存储。


>LinkedHashMap:LinkedHashMap继承了HashMap，是HashMap的一个子类，在HashMap的基础上增加了一个双向链表来维护插入顺序或访问顺序；
遍历速度取决于数据多少，而不是容量大小；

LinkedHashMap的源码解析：
构造器：
1.空参构造LinkedHashMap()
/**
 * 构造一个空的、按照插入顺序排序的LinkedHashMap实例，使用默认的初始容量（16）和加载因子（0.75）。
 */
public LinkedHashMap() {
    super();
    accessOrder = false;
}

LinkedHashMap底层的数据结构：双向链表
static class Entry<K,V> extends HashMap.Node<K,V> {
    Entry<K,V> before, after;
    Entry(int hash, K key, V value, Node<K,V> next) {
        super(hash, key, value, next);
    }
}

>TreeMap:基于红黑树实现，能够按照键的自然顺序或指定的Comparator进行排序；
由于红黑树的特性，其插入、删除和查找操作的时间复杂度均为O(log n)。

>HashTable:是Java早期提供的线程安全的Map实现类，通过在每个方法上添加synchronized关键字来实现线程安全；
由于同步的粒度较大，其性能相对较低。







Java泛型：
所操作的数据类型被指定为一个参数（type parameter）这种参数类型可以用在类、接口和方法的创建中，分别称为泛型类、泛型接口、泛型方法。

在集合中使用泛型:
>集合接口或集合类在jdk5.0时都修改为带泛型的结构
>在实例化集合类时，可以指明具体的泛型类型
>指明完以后，在集合类或接口中凡是定义类或接口时，内部结构使用到类的泛型的位置，都指定为实例化的泛型类型
(比如: add(E e) --->实例化以后: add(Integer e))
注意点: 泛型的类型必须是类，不能是基本数据类型。需要用到基本数据类型的位置，拿包装类替换 
>如果实例化时，没有指明泛型的类型。默认类型为java.lang.object类型。

自定义泛型接口：
语法：Interface 接口名<T, R…>{ }
使用注意事项：
① 接口中，静态成员也不能使用泛型；
② 泛型接口的类型，在继承接口或者实现接口时确定；
③ 没有指定类型，默认为Object。

泛型的通配符：?
① <?>：支持任意泛型
② <? extends A>：支持A类以及A类的子类，规定了泛型的上限
③ <? Super A>：支持A类以及A类的父类，不限于直接父类，规定了泛型的下限。	
④ 不能向List<?> 内添加除null之外的数据
⑤ 允许读取数据，读取数据的类型为Object

泛型的好处：
①把运行时的问题变成了编译时的问题
②强转的问题

泛型的上限和下限：
泛型的上限（Upper Bound）
泛型的上限指的是泛型类型参数所能接受的最大类型范围，即该泛型类型参数必须是某个特定类型或其子类。在Java中，使用? extends Type来表示泛型的上限。这里的Type是泛型类型参数的上限类型，意味着泛型类型参数可以是Type类型或其任何子类。

泛型的下限（Lower Bound）
泛型的下限指的是泛型类型参数所能接受的最小类型范围，即该泛型类型参数必须是某个特定类型或其父类。在Java中，使用? super Type来表示泛型的下限。这里的Type是泛型类型参数的下限类型，意味着泛型类型参数可以是Type类型或其任何父类。

泛型使用注意事项：
泛型是工作在编译阶段的，一旦程序编译成class文件，class文件就不存在泛型了，这就是泛型擦除。通过泛型的桥界方法，Java编译器能够确保在运行时，尽管泛型类型被擦除，但仍然能够正确地处理类型转换，从而保持代码的类型安全性。
泛型不支持基本数据类型，只能支持对象类型（引用数据类型）。
在使用泛型的上下限时，需要特别注意PECS（Producer Extends Consumer Super）原则，即生产者使用? extends，消费者使用? super。这有助于避免在泛型集合中发生类型不匹配的错误。

Java IO（Flie类、IO流、网络编程、序列化与反序列化）:
Java的IO是实现输入和输出的基础，可以方便的实现数据的输入和输出操作。在Java中把对于输入/输出操作是以流的方式进行操作的。
java.io 包下提供了大量的供我们使用的操作【流】的方法和接口，用于进行各类数据的处理和传输。
注意： Windows各个文件之间分隔符为：“ \ ”；Linux各个文件之间分割符为：“ / ”

Java Flie类：Flie类是文件和目录路径名的抽象表示，主要用于文件和目录的创建、查找和删除等操作。
Flie类构造方法：
public File(String pathname) ：通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例。
public File(String parent, String child) ：从父路径名字符串和子路径名字符串创建新的 File实例。
public File(File parent, String child) ：从父抽象路径名和子路径名字符串创建新的 File实例。

Java IO流：通过IO可以完成硬盘文件的读和写   I:Input 往内存中去 O:Output 从内存中出来
在java中只要"类名"以 Stream 结尾的都是字节流。以"Reader/Writer"结尾的都是字符流。
字符流不能处理图片文件，字节流不能处理文本文件

流的体系结构：
抽象基类
InputStream
OutputStream
Reader
Writer
节点流(或文件流)
FileInputStream
FileOutputstream
FileReader
Filewriter
缓冲流(处理流的一种)
BufferedInputStream
BufferedOutputstream
BufferedReader
Bufferedwriter

文件专属字符流FlieReader：
//1.实例化File类的对象，指明要操作的文件
File file = new File( pathname:"hello.txt");//相较于当前Module
//2.提供具体的流
FileReader fr = new FileReader(file);
//3.数据的读入
int data;
while((data = fr.read()) != -1){
System.out.print((char)data);
}
//4.流的关闭操作
try {
fr.close();
} catch (IOException e) {
e.printStackTrace()
};

文件专属字节流FileInputStream：
public class FileInputStreamTest04 {
    public static void main(String[] args) {
        FileInputStream fis = null;
        try {
            fis = new FileInputStream("chapter23/src/tempfile3");
            // 开始读，采用byte数组，一次读取多个字节。最多读取“数组.length”个字节。
            byte[] bytes = new byte[4];// 准备一个4个长度的byte数组，一次最多读取4个字节。
            int readCount = 0;
            // 这个方法的返回值是：读取到的字节数量。（不是字节本身）;1个字节都没有读取到返回-1(文件读到末尾)
            while((readCount = fis.read(bytes)) != -1) {
            	// 不应该全部都转换，应该是读取了多少个字节，转换多少个。
                System.out.print(new String(bytes, 0, readCount));
            }
        } catch (FileNotFoundException e) {
            e.printStackTrace();
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
        	// 在finally语句块当中确保流一定关闭。
            if (fis != null) {// 避免空指针异常！
            	// 关闭流的前提是：流不是空。流是null的时候没必要关闭。
                try {
                    fis.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
    }
}

缓冲字符流BufferReader：
包装流的方法
public class BufferedReaderTest01 {
    public static void main(String[] args) throws Exception{

        FileReader reader = new FileReader("Copy02.java");
        // 当一个流的构造方法中需要一个流的时候，这个被传进来的流叫做：节点流。
        // 外部负责包装的这个流，叫做：包装流，还有一个名字叫做：处理流。
        // 像当前这个程序来说：FileReader就是一个节点流。BufferedReader就是包装流/处理流。
        BufferedReader br = new BufferedReader(reader);

        // br.readLine()方法读取一个文本行，但不带换行符。
        String s = null;
        while((s = br.readLine()) != null){
            System.out.print(s);
        }

        // 关闭流
        // 对于包装流来说，只需要关闭最外层流就行，里面的节点流会自动关闭。（可以看源代码。）
        br.close();
    }
}

缓冲流InputStreamReader转换流FileInputStream：
public class BufferedReaderTest02 {
    public static void main(String[] args) throws Exception{

        /*// 字节流
        FileInputStream in = new FileInputStream("Copy02.java");

        // 通过转换流转换（InputStreamReader将字节流转换成字符流。）
        // in是节点流。reader是包装流。
        InputStreamReader reader = new InputStreamReader(in);

        // 这个构造方法只能传一个字符流。不能传字节流。
        // reader是节点流。br是包装流。
        BufferedReader br = new BufferedReader(reader);*/

        // 合并
        BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream("Copy02.java")));

        String line = null;
        while((line = br.readLine()) != null){
            System.out.println(line);
        }

        // 关闭最外层
        br.close();
    }
}

DataInputStream:数据字节输入流。
DataOutputStream写的文件，只能使用DataInputStream去读。并且读的时候你需要提前知道写入的顺序。
读的顺序需要和写的顺序一致。才可以正常取出数据。


序列化：将对象写入到IO流中，说的简单一点就是将内存模型的对象变成字节数字，可以进行存储和传输。
使用ObjectOutputStream实现序列化：
public class ObjectOutputStreamTest01 {
    public static void main(String[] args) throws Exception{
        // 创建java对象
        Student s = new Student(1111, "zhangsan");
        // 序列化
        ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream("students"));
        // 序列化对象
        oos.writeObject(s);
        // 刷新
        oos.flush();
        // 关闭
        oos.close();
    }
}
注意：序列化的对象需要实现Serializable接口，用static和transient修饰的变量不可序列化

反序列化：从IO流中恢复对象，将存储在磁盘或者从网络接收的数据恢复成对象模型。
使用ObjectInputStream实现反序列化：
public class ObjectInputStreamTest01 {
    public static void main(String[] args) throws Exception{
        ObjectInputStream ois = new ObjectInputStream(new FileInputStream("students"));
        // 开始反序列化，读
        Object obj = ois.readObject();
        // 反序列化回来是一个学生对象，所以会调用学生对象的toString方法。
        System.out.println(obj);
        ois.close();
    }
}

使用场景：所有可在网络上传输的对象都必须是可序列化的，否则会出错；所有需要保存到磁盘的Java对象都必须是可序列化的。


磁盘IO（Input/Output，即输入/输出）是指计算机系统中数据在磁盘存储设备和内存之间的传输过程。磁盘IO性能对系统整体性能有重要影响，尤其在数据密集型应用中。
以下是磁盘IO的几个关键方面：

1. 磁盘类型
   - 机械硬盘（HDD）：基于旋转磁盘和磁头的传统存储设备。读取/写入数据时，磁头需要移动到正确的位置，因而具有较高的寻道时间和旋转延迟。
   - 固态硬盘（SSD）：使用闪存存储数据，没有机械部件，因而具有较低的访问时间和更高的读写速度。

2. IO操作类型
   - 顺序IO：数据按连续的块读写，通常性能较好，因为减少了磁头移动（对HDD而言）或内存地址跳转（对SSD而言）。
   - 随机IO：数据读写是非连续的，需要频繁移动磁头或访问不同的内存地址，通常性能较低。

3. IO性能指标
   - 吞吐量（Throughput）：单位时间内能够完成的IO操作数量，通常以MB/s（兆字节每秒）或IOPS（每秒输入输出操作数）来衡量。
   - 延迟（Latency）：完成单个IO操作所需的时间，通常以毫秒（ms）为单位。

4. 磁盘调度算法
   - 先来先服务（FCFS）：按请求到达的顺序处理。
   - 最短寻道时间优先（SSTF）：优先处理离当前磁头位置最近的请求。
   - 电梯算法（SCAN）：磁头像电梯一样在磁盘上移动，按一个方向处理完请求后再反向移动。

5. 缓存和预读
   - 磁盘缓存：存储设备自带的一小块高速缓存，用于临时存储频繁访问的数据，提高访问速度。
   - 预读：操作系统或存储设备在实际需要数据之前预先读取数据到缓存，以减少等待时间。

6. RAID技术
   - RAID（独立磁盘冗余阵列）：通过将多个物理磁盘组合成一个逻辑磁盘来提高性能和数据可靠性。常见的RAID级别有RAID 0、RAID 1、RAID 5、RAID 6等，每种级别有不同的性能和冗余特性。

7. 文件系统
   - 文件系统在磁盘IO过程中也扮演重要角色，负责管理数据的存储和检索。常见的文件系统包括NTFS、EXT4、XFS等，不同的文件系统在性能和功能上有不同的特点。

总结
磁盘IO是指数据在磁盘存储设备和内存之间的传输，涉及磁盘类型、IO操作类型、性能指标、调度算法、缓存机制、RAID技术和文件系统等多个方面。优化磁盘IO性能对于提升系统整体性能具有重要意义。


RandomAccessFile：
RandomAccessFile是Java输入/输出流体系中功能最丰富的文件内容访问类，既可以读取文件内容，也可以向文件输出数据。

RandomAccessFlie的特点：
1.RandomAccessFlie直接继承于java.lang.Object，实现了DataInput和DataOutput
2.RandomAccessFlie既可以作为输入流，又可以作为输出流
3.如果RandomAccessFlie作为一个输出流时，写出的文件如果不存在，则在执行过程中自动创建；如果文件存在，则会对原有文件内容进行覆盖。（默认情况下，从头覆盖）
4.用seak方法可以实现在第N位后覆盖原文件

示例代码：
import java.io.RandomAccessFile;
import java.io.IOException;

public class RandomAccessFileExample {
    public static void main(String[] args) {
        try {
            // 创建一个RandomAccessFile对象，以读写模式打开文件
            RandomAccessFile raf = new RandomAccessFile("example.txt", "rw");

            // 写入数据
            raf.writeInt(100);
            raf.writeDouble(3.14);
            raf.writeUTF("Hello, World!");

            // 移动文件指针到文件开始
            raf.seek(0);

            // 读取数据
            int intValue = raf.readInt();
            double doubleValue = raf.readDouble();
            String stringValue = raf.readUTF();

            // 打印读取的数据
            System.out.println("Int value: " + intValue);
            System.out.println("Double value: " + doubleValue);
            System.out.println("String value: " + stringValue);

            // 关闭文件
            raf.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

注意事项
同步问题：RandomAccessFile 不支持多线程同步。如果多个线程需要同时访问同一个 RandomAccessFile 对象，需要在外部实现同步机制。
资源管理：使用 RandomAccessFile 时，必须在操作完成后调用 close() 方法关闭文件，以释放资源。
文件指针：读写操作依赖文件指针的位置，进行操作前需要确保文件指针指向正确的位置。


Java NIO：
NIO 中的 N 可以理解为 Non-blocking，不单纯是 New，是解决高并发、I/O高性能的有效方式。
Java NIO是Java1.4之后推出来的一套IO接口，NIO提供了一种完全不同的操作方式， NIO支持面向缓冲区的、基于通道的IO操作。
新增了许多用于处理输入输出的类，这些类都被放在java.nio包及子包下，并且对原java.io包中的很多类进行改写，新增了满足NIO的功能。

同步非阻塞，服务器实现模式为一个线程处理多个请求(连接)，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求就进行处理。
一个线程中就可以调用多路复用接口（java中是select）阻塞同时监听来自多个客户端的IO请求，
一旦有收到IO请求就调用对应函数处理，NIO擅长1个线程管理多条连接，节约系统资源。

运用Java NIO通信框架有Mina、Netty、Grizzly等。

Java NIO详解：https://www.cnblogs.com/mikechenshare/p/16587635.html

NIO 包含3个核心的组件：

Channel(通道)
Buffer(缓冲区)
Selector(选择器)

关系图：
https://static.mikechen.cc/wp-content/uploads/2022/03/java-nio-08.png

关系图的说明：
1.每个 Channel 对应一个 Buffer。
2.Selector 对应一个线程，一个线程对应多个 Channel。
3.该图反应了有三个 Channel 注册到该 Selector。
4.程序切换到那个 Channel 是由事件决定的（Event）。
5.Selector 会根据不同的事件，在各个通道上切换。
6.Buffer 就是一个内存块，底层是有一个数组。
7.数据的读取和写入是通过 Buffer，但是需要flip()切换读写模式，而 BIO 是单向的，要么输入流要么输出流。

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;

public class NIOExample {
    public static void main(String[] args) {
        try (FileChannel channel = FileChannel.open(Paths.get("example.txt"), StandardOpenOption.READ)) {
            ByteBuffer buffer = ByteBuffer.allocate(1024);
            while (channel.read(buffer) > 0) {
                buffer.flip();
                while (buffer.hasRemaining()) {
                    System.out.print((char) buffer.get());
                }
                buffer.clear();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

这个例子中，我们使用FileChannel类打开了一个名为example.txt的文件，并创建了一个ByteBuffer对象作为缓冲区。
然后我们循环读取文件内容到缓冲区中，并使用flip()方法切换缓冲区的读写模式，然后逐个字节读取缓冲区中的内容并打印出来。
最后使用clear()方法清除缓冲区。

创建NIOServer：
public static void main(String[] args) throws  Exception{
        //创建ServerSocketChannel，-->> ServerSocket
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        InetSocketAddress inetSocketAddress = new InetSocketAddress(5555);
        serverSocketChannel.socket().bind(inetSocketAddress);
        serverSocketChannel.configureBlocking(false); //设置成非阻塞
 
        //开启selector,并注册accept事件
        Selector selector = Selector.open();
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);
 
        while(true) {
            selector.select(2000);  //监听所有通道
            //遍历selectionKeys
            Set<SelectionKey> selectionKeys = selector.selectedKeys();
            Iterator<SelectionKey> iterator = selectionKeys.iterator();
            while (iterator.hasNext()) {
                SelectionKey key = iterator.next();
                if(key.isAcceptable()) {  //处理连接事件
                    SocketChannel socketChannel = serverSocketChannel.accept();
                    socketChannel.configureBlocking(false);  //设置为非阻塞
                    System.out.println("client:" + socketChannel.getLocalAddress() + " is connect");
                    socketChannel.register(selector, SelectionKey.OP_READ); //注册客户端读取事件到selector
                } else if (key.isReadable()) {  //处理读取事件
                    ByteBuffer byteBuffer = ByteBuffer.allocate(1024);
                    SocketChannel channel = (SocketChannel) key.channel();
                    channel.read(byteBuffer);
                    System.out.println("client:" + channel.getLocalAddress() + " send " + new String(byteBuffer.array()));
                }
                iterator.remove();  //事件处理完毕，要记得清除
            }
        }
 
 }

使用NIO实现微服务的高并发：
服务器端代码：
import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.SelectionKey;
import java.nio.channels.Selector;
import java.nio.channels.ServerSocketChannel;
import java.nio.channels.SocketChannel;
import java.util.Iterator;
import java.util.Set;

public class NioServer {
    public static void main(String[] args) throws IOException {
        Selector selector = Selector.open();
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        serverSocketChannel.bind(new InetSocketAddress(5000));
        serverSocketChannel.configureBlocking(false);
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);

        while (true) {
            selector.select();
            Set<SelectionKey> selectedKeys = selector.selectedKeys();
            Iterator<SelectionKey> iter = selectedKeys.iterator();

            while (iter.hasNext()) {
                SelectionKey key = iter.next();

                if (key.isAcceptable()) {
                    register(selector, serverSocketChannel);
                }

                if (key.isReadable()) {
                    answerWithEcho(key);
                }

                iter.remove();
            }
        }
    }

    private static void register(Selector selector, ServerSocketChannel serverSocketChannel) throws IOException {
        SocketChannel client = serverSocketChannel.accept();
        client.configureBlocking(false);
        client.register(selector, SelectionKey.OP_READ);
    }

    private static void answerWithEcho(SelectionKey key) throws IOException {
        SocketChannel client = (SocketChannel) key.channel();
        ByteBuffer buffer = ByteBuffer.allocate(256);
        int bytesRead = client.read(buffer);
        if (bytesRead == -1) {
            client.close();
            return;
        }
        buffer.flip();
        client.write(buffer);
        buffer.clear();
    }
}


客户端代码：
import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.SocketChannel;

public class NioClient {
    public static void main(String[] args) throws IOException {
        InetSocketAddress hostAddress = new InetSocketAddress("localhost", 5000);
        SocketChannel client = SocketChannel.open(hostAddress);

        String message = "Hello, Server!";
        ByteBuffer buffer = ByteBuffer.allocate(256);
        buffer.put(message.getBytes());
        buffer.flip();
        client.write(buffer);

        buffer.clear();
        client.read(buffer);
        String response = new String(buffer.array()).trim();
        
        System.out.println("Server response: " + response);

        client.close();
    }
}


网络编程：
Java网络编程涵盖了从基础的TCP/UDP通信到高级的网络框架和Web编程。通过使用Java的标准库和各种第三方框架，开发者可以构建高效、可靠和可扩展的网络应用。
无论是简单的客户端-服务器通信，还是复杂的分布式系统，Java都提供了强大的支持和工具。

1. 基础网络通信
1.1. TCP通信
ServerSocket和Socket：
ServerSocket用于创建服务器端，监听特定端口，接受客户端连接。
Socket用于客户端连接服务器，并通过输入输出流进行数据传输。
适用于需要可靠、有序数据传输的应用，如HTTP、FTP等。

1.2. UDP通信
DatagramSocket和DatagramPacket：
DatagramSocket用于发送和接收数据包（DatagramPacket）。
适用于不需要保证数据传输可靠性和顺序的应用，如实时视频、音频流。

2. 高级网络编程
2.1. Java NIO
Selectors、Channels和Buffers：
Java NIO提供了非阻塞I/O操作的支持，适用于高性能、高并发的服务器端编程。
使用Selector管理多个通道（Channel），通过Buffer进行数据操作。

2.2. 高性能网络框架
Netty：
Netty是一个异步事件驱动的网络应用框架，用于快速开发高性能、高可靠性的网络服务器和客户端。
提供了更高层次的抽象，简化了基于NIO的网络编程。

3. Web编程
3.1. Servlet和JSP
Servlet：用于处理HTTP请求和响应，构建动态Web内容。
JSP（JavaServer Pages）：简化了Servlet的编写，可以嵌入Java代码生成动态内容。
3.2. Spring Boot
用于快速创建基于Spring的独立、生产级应用。支持内置的Tomcat、Jetty等容器。
提供了强大的注解支持和自动配置功能。

4. 其他网络编程技术
4.1. WebSockets
用于在客户端和服务器之间建立双向通信，适用于实时应用，如在线聊天、游戏等。
Java可以使用Jetty、Tomcat等支持WebSocket的容器实现WebSocket通信。
4.2. RESTful Web服务
使用JAX-RS（如Jersey）或Spring Boot创建RESTful Web服务，提供基于HTTP的API接口。
4.3. RPC（远程过程调用）
使用gRPC、Apache Thrift等框架实现跨语言的远程过程调用。

Java Stream流（函数式编程）：
Stream流是一种顺序的元素集合，它支持类似于SQL语句的操作，如过滤、映射、排序等。通过使用Stream流，我们可以以声明式的方式对数据进行处理，而不需要关心具体的实现细节。

Stream流的主要特点包括：
Stream流不存储数据，而是通过管道传输数据。
Stream流可以操作任何数据源，如集合、数组、I/O通道等。
Stream流可以进行串行操作和并行操作。
Java中的Stream流主要包含两种类型：中间操作和终端操作。中间操作用于对流进行一系列的转换和操作，而终端操作用于从流中获取结果。

使用Stream流可以通过以下几个步骤进行：

创建流：可以从集合、数组、I/O通道等数据源中创建Stream流。
中间操作：对流进行一系列的转换和操作，如过滤、映射、排序等。
终端操作：从流中获取结果，如聚合、收集、遍历等。


常用的 Stream 函数包括：
中间操作符：通常对于Stream的中间操作，可以视为是源的查询，并且是懒惰式的设计，对于源数据进行的计算只有在需要时才会被执行，
与数据库中视图的原理相似；Stream流的强大之处便是在于提供了丰富的中间操作，相比集合或数组这类容器，极大的简化源数据的计算复杂度
一个流可以跟随零个或多个中间操作。其目的主要是打开流，做出某种程度的数据映射/过滤，然后返回一个新的流，交给下一个操作使用

1. filter(Predicate<T> predicate)：根据指定的条件过滤出符合条件的元素。
userList.stream().filter(user -> user.getId() > 6).collect(Collectors.toList());
2. map(Function<T, R> mapper)：将每个元素映射为另一个元素。
userList.stream().map(user -> user.getName() + "用户").collect(Collectors.toList());
3. flatMap(Function<T, Stream<R>> mapper)：将每个元素映射为一个 Stream，并将所有 Stream 的元素合并为一个 Stream。
userList.stream().flatMap(user -> Arrays.stream(user.getCity().split(","))).forEach(System.out::println);

map：对流中每一个元素进行处理
flatMap：流扁平化，让你把一个流中的“每个值”都换成另一个流，然后把所有的流连接起来成为一个流 
本质区别：map是对一级元素进行操作，flatmap是对二级元素操作map返回一个值；flatmap返回一个流，多个值

4. distinct()：去除重复的元素。
userList.stream().map(User::getCity).distinct().collect(Collectors.toList());
5. sorted()：对元素进行排序。
userList.stream().sorted(Comparator.comparing(User::getName).reversed()).collect(Collectors.toList()).forEach(System.out::println);
6. limit(long maxSize)：限制 Stream 的大小。
userList.stream().limit(5).collect(Collectors.toList()).forEach(System.out::println);
7. skip(long n)：跳过前 n 个元素。
userList.stream().skip(7).collect(Collectors.toList()).forEach(System.out::println);
8. peek():对元素进行遍历处理
//每个用户ID加1输出
userList.stream().peek(user -> user.setId(user.getId()+1)).forEach(System.out::println);

终端操作符：Stream流执行完终端操作之后，无法再执行其他动作，否则会报状态异常，提示该流已经被执行操作或者被关闭，
想要再次执行操作必须重新创建Stream流。一个流有且只能有一个终端操作，当这个操作执行后，流就被关闭了，无法再被操作，
因此一个流只能被遍历一次，若想在遍历需要通过源数据在生成流。

9. forEach(Consumer<T> action)：对每个元素执行指定的操作。
userList.stream().forEach(user -> System.out.println(user));
userList.stream().filter(user -> "上海".equals(user.getCity())).forEach(System.out::println);
10. collect(Collector<T, A, R> collector)：收集器，将流转换为其他形式。
Set set = userList.stream().collect(Collectors.toSet());
List list = userList.stream().collect(Collectors.toList());
11. findFirst():返回第一个元素
User firstUser = userList.stream().findFirst().get();
User firstUser1 = userList.stream().filter(user -> "上海".equals(user.getCity())).findFirst().get();        
12. findAny():返回当前流中的任意元素
User findUser = userList.stream().findAny().get();
User findUser1 = userList.stream().filter(user -> "上海".equals(user.getCity())).findAny().get();
13. count():返回流中元素总数
long count = userList.stream().filter(user -> user.getAge() > 20).count();		
14. sum():求和
int sum = userList.stream().mapToInt(User::getId).sum();
15. max():最大值
int max = userList.stream().max(Comparator.comparingInt(User::getId)).get().getId();
16. min():最小值
int min = userList.stream().min(Comparator.comparingInt(User::getId)).get().getId();
17. anyMatch():检查是否至少匹配一个元素，返回boolean
boolean matchAny = userList.stream().anyMatch(user -> "北京".equals(user.getCity()));
18. allMatch():检查是否匹配所有元素，返回boolean
boolean matchAll = userList.stream().allMatch(user -> "北京".equals(user.getCity()));
19. noneMatch():检查是否没有匹配所有元素，返回boolean
boolean nonaMatch = userList.stream().allMatch(user -> "云南".equals(user.getCity()));
20.reduce():可以将流中元素反复结合起来，得到一个值
Optional reduce = userList.stream().reduce((user, user2) -> {
    return user;
});


		
通过使用这些函数，可以简化代码、提高效率，并且使代码更加易读和可维护。

Java反射：
反射是作为动态语言的关键，反射机制允许程序在执行期借助于Reflection API取得任何类的内部信息，并能直接操作任意对象的内部属性和方法。

动态语言和静态语言概念：
动态类型语言：动态类型语言是指在运行期间才去做数据类型检查的语言，也就是说，在用动态类型的语言编程时，永远也不用给任何变量指定数据类型（变量使用之前不需要类型声明），该语言会在你第一次赋值给变量时，在内部将数据类型记录下来。
Python 和 Ruby 就是一种典型的动态类型语言，其他的各种脚本语言如 JavaScript 、Shell也属于动态类型语言。
静态类型语言：静态类型语言与动态类型语言刚好相反，它的数据类型是在编译其间检查的，也就是说在写程序时要声明所有变量的数据类型，C/C++ 是静态类型语言的典型代表，其他的静态类型语言还有 C#、JAVA 、golang等。
总结：静态类型和动态类型的本质区别在于：变量的数据类型确定的时机不同，前者在运行时根据变量值确定；后者在编译时根据声明类型确定。

在Java JVM（Java虚拟机）中，类生成对象的步骤涉及多个层面的操作，但大致可以概括为以下几个关键步骤：

1.类加载（Class Loading）：
这是对象创建之前的第一步。JVM中的类加载器（Class Loader）负责查找和加载类的二进制数据（通常是.class文件）。这个过程中，类加载器会解析类的二进制数据，并生成JVM内部的表示——元数据（如方法、字段和类的其他信息等）存储在方法区（Method Area）中。

2.链接（Linking）：
链接阶段分为三个子步骤：验证（Verification）、准备（Preparation）和解析（Resolution）。

3.验证：确保加载的类信息符合Java语言规范及JVM规范，没有安全问题的类文件才能被JVM使用。

4.准备：为类的静态变量分配内存，并设置JVM默认的初始值（注意，这里只是初始化为默认值，不是用户代码里指定的值）。

5.解析：将常量池中的符号引用替换为直接引用的过程。这主要是对类或接口、字段、类方法、接口方法等的引用进行解析。

6.初始化（Initialization）：
在这个阶段，执行类的初始化代码（即静态初始化块或静态字段的初始化代码）。这标志着类已经完成了加载、链接过程，可以开始被创建对象了。

7.分配内存（Memory Allocation）：
JVM在堆区（Heap）为对象分配内存空间。分配的内存大小在类加载完成后就可以确定，因为此时类的所有字段和方法所需的空间都已经被计算出来。

8.设置默认值（Default Initialization）：
为对象的属性（包括实例变量）分配内存，并设置JVM默认的初始值（如0、false、null等）。

9.调用构造方法（Constructor Invocation）：
JVM通过调用类的构造方法来初始化对象。首先，调用父类的构造方法（如果有的话，且是按照从基类到派生类的顺序）；然后，执行对象自身的构造方法体中的代码，设置属性值为用户指定的值。

10.返回引用（Reference Return）：
对象创建完成后，JVM会将这个对象的引用（地址）返回给调用者，之后可以通过这个引用来访问对象的属性和方法。
需要注意的是，这个过程对于JVM来说是高度抽象和内部化的，用户代码主要关注的是类的定义和对象的创建（通过new关键字）及使用。JVM则负责在底层实现类的加载、链接、初始化以及对象的分配和初始化等复杂操作。



在Java中，.class文件是Java源代码文件（.java）经过编译后生成的字节码文件。这个文件是Java程序执行的关键载体，包含了编译后的Java字节码指令、常量池、类的结构等信息。
.class文件包含：
1. 魔数（Magic Number）
作用：用于标识.class文件的类型，确保该文件是一个能被Java虚拟机（JVM）接受的Class文件。
值：一般为0xCAFEBABE，这个值在文件的最开始位置，占用4个字节。

2. 版本信息
次版本号（Minor Version）：紧接着魔数的4个字节之后，占用2个字节，表示.class文件的次版本号。
主版本号（Major Version）：紧接着次版本号的2个字节之后，也占用2个字节，表示.class文件的主版本号。这两个版本号共同决定了.class文件的编译版本。

3. 常量池（Constant Pool）
作用：常量池是Class文件结构中的资源仓库，用于存放编译时期生成的各种字面量和符号引用。
内容：主要包括两大类常量：字面量（如文本字符串、声明为final的常量值等）和符号引用（如类和接口的全限定名、字段的名称和描述符、方法的名称和描述符等）。
容量计数：常量池的大小由constant_pool_count项指定，这个值是一个u2类型的数据，表示常量池中的常量项数量（注意，计数从1开始）。

4. 访问标志（Access Flags）
作用：用于识别一些类或接口层次的访问信息，如这个Class是类还是接口、是否定义为public类型、是否定义为abstract类型等。
位置：常量池之后，占用2个字节。

5. 类索引、父类索引和接口索引集合
类索引（this_class）：表示当前类的全限定名。
父类索引（super_class）：表示当前类的父类的全限定名（除了java.lang.Object外，所有Java类的父类索引都不为0）。
接口索引集合（interfaces）：描述这个类实现了哪些接口，接口将按implements语句（或extends语句，如果类本身是接口）后的接口顺序排列。

6. 字段表（Field Table）
作用：描述接口或类中声明的变量，包括类级变量和实例级变量，但不包括在方法内部声明的局部变量。
结构：字段表由多个field_info结构组成，每个结构包含访问标志、名称索引、描述符索引和属性表集合等。

7. 方法表（Method Table）
作用：描述接口或类中声明的方法。
结构：与方法表类似，方法表由多个method_info结构组成，每个结构包含访问标志、名称索引、描述符索引、属性表集合等。

8. 属性表（Attribute Table）
作用：属性表集合用于存储类或接口、字段、方法的元数据。
内容：属性表包含了许多可选的属性，如LineNumberTable（记录行号）、LocalVariableTable（保存局部变量和参数）、StackMapTable（加快字节码校验）、Exceptions（列举异常）、SourceFile（记录来源）等。



Java反射概述：
>Java反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；
对于任意一个对象，都能够调用它的任意一个方法和属性；
这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。

>要想解剖一个类,必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法。所以先要获取到每一个字节码文件对应的Class类型的对象。

>比如之前在没有使用反射的时候，我们这里有一个类，如果我们想要调用里面的属性、方法等，我们要先创建一个对象，才可以调用里面的属性等。

>类似的，我们如果使用反射，想要使用一个类的成员变量，方法，构造器等，我们就要先获取一个Class对象，这个Class对象里面有类的成员变量，方法等信息，这样我们才可以进行相关的操作。

>正因为反射是在运行时获取类的信息，所以用反射去获取类的注解时只能获取到运行时的注解。

Class 类：反射的核心类，可以获取类的属性，方法等信息。
Field 类：Java.lang.reflec 包中的类，表示类的成员变量，可以用来获取和设置类之中的属性值。
Method 类： Java.lang.reflec 包中的类，表示类的方法，它可以用来获取类中的方法信息或 者执行方法。
Constructor 类： Java.lang.reflec 包中的类，表示类的构造方法。

getFields():获取当前运行时类及父类中声明为public访问权限的属性
getDeclaredFields()：获取当前运行时类中声明的所有属性（不包括父类）
getMethods():获取当前运行时类及其父类中声明为public的方法
	
Java反射示例：
创建一个person类：
public class Person {
    private String name;
    private int age;

    public Person() {}

    public Person(String name, int age) {
        this.name = name;
        this.age = age;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }

    public void displayInfo() {
        System.out.println("Name: " + name + ", Age: " + age);
    }
}

编写一个反射来操作person类的示例程序：
import java.lang.reflect.Constructor;
import java.lang.reflect.Field;
import java.lang.reflect.Method;

public class ReflectionExample {
    public static void main(String[] args) {
        try {
            // 获取 Person 类的 Class 对象
            Class<?> personClass = Class.forName("Person");

            // 获取构造函数并创建实例
            Constructor<?> constructor = personClass.getConstructor(String.class, int.class);
            Object personInstance = constructor.newInstance("John Doe", 30);

            // 获取并调用 getName 方法
            Method getNameMethod = personClass.getMethod("getName");
            String name = (String) getNameMethod.invoke(personInstance);
            System.out.println("Name: " + name);

            // 获取并调用 setName 方法
            Method setNameMethod = personClass.getMethod("setName", String.class);
            setNameMethod.invoke(personInstance, "Jane Doe");

            // 获取并调用 getAge 方法
            Method getAgeMethod = personClass.getMethod("getAge");
            int age = (int) getAgeMethod.invoke(personInstance);
            System.out.println("Age: " + age);

            // 获取并调用 setAge 方法
            Method setAgeMethod = personClass.getMethod("setAge", int.class);
            setAgeMethod.invoke(personInstance, 25);

            // 获取并调用 displayInfo 方法
            Method displayInfoMethod = personClass.getMethod("displayInfo");
            displayInfoMethod.invoke(personInstance);

            // 访问私有字段
            Field nameField = personClass.getDeclaredField("name");
            nameField.setAccessible(true); // 需要设置访问权限
            nameField.set(personInstance, "Reflective John");

            // 调用 displayInfo 方法显示更改后的信息
            displayInfoMethod.invoke(personInstance);

        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

使用反射进行赋值：
import java.lang.reflect.Field;

public class ReflectionExample {
    public static void copyFields(Object source, Object target) {
        Class<?> sourceClass = source.getClass();
        Class<?> targetClass = target.getClass();

        // 获取源对象的所有字段
        Field[] sourceFields = sourceClass.getDeclaredFields();

        for (Field sourceField : sourceFields) {
            try {
                // 允许访问私有字段
                sourceField.setAccessible(true);

                // 获取源字段的值
                Object value = sourceField.get(source);

                // 在目标对象中找到相同名称的字段
                Field targetField;
                try {
                    targetField = targetClass.getDeclaredField(sourceField.getName());
                } catch (NoSuchFieldException e) {
                    // 如果目标类中没有这个字段，继续下一个字段
                    continue;
                }

                // 允许访问私有字段
                targetField.setAccessible(true);

                // 将值赋值到目标对象的字段中
                targetField.set(target, value);

            } catch (IllegalAccessException e) {
                e.printStackTrace();
            }
        }
    }

    public static void main(String[] args) {
        Person person = new Person();
        // 初始化 person 的字段

        PersonName personName = new PersonName();
        copyFields(person, personName);

        // personName 对象现在拥有 person 对象的相同字段值
    }
}

反射的优点和缺点：
反射的优点：
1.提高程序的灵活性：
反射允许程序在运行时动态地访问和操作对象、类及其成员（如方法、属性等），这使得程序可以根据需要改变自身的行为，而无需在编译时确定所有细节。这种灵活性对于需要高度可配置或可扩展的应用程序尤其重要。

2.降低耦合性：
通过反射，程序可以在运行时动态地加载和使用类，而无需在编译时显式地引用这些类。这有助于降低程序组件之间的耦合性，提高代码的复用性和可维护性。例如，在插件系统中，主程序可以通过反射动态地加载和使用插件，而无需在编译时知道插件的具体实现。

3.支持动态代理：
反射是实现动态代理的关键技术之一。通过反射，可以在运行时动态地创建代理对象，从而实现对目标对象的代理。这在需要进行方法拦截、权限控制等场景时非常有用。

4.支持动态配置：
反射允许程序在运行时读取和修改配置参数，从而实现动态配置。这对于需要根据不同环境或用户设置调整程序行为的应用程序来说非常重要。

5.支持自动生成代码：
在某些情况下，反射可以用于在运行时生成程序代码。虽然这不是反射的主要用途，但在某些特定场景下（如自动生成ORM映射代码），反射可以大大简化开发过程。

6.支持自动化测试：
通过反射，可以在运行时动态地创建测试用例，从而实现自动化测试。这有助于提高测试效率和覆盖率，确保程序的稳定性和可靠性。

7.支持泛型操作：
在使用泛型时，反射可以帮助程序在运行时获取和操作泛型类型的信息，从而支持更复杂的泛型操作。


反射的缺点：
1.性能开销：
反射操作通常比直接代码调用要慢得多。因为反射涉及类型信息的动态解析和方法的动态查找，这些操作在编译时无法优化，导致运行时性能下降。特别是在对性能要求较高的应用场景中，反射的使用需要谨慎。

2.安全性问题：
反射允许程序访问和操作原本可能不可访问的类或成员，这可能会破坏封装性，并导致潜在的安全漏洞。例如，恶意代码可能通过反射访问敏感信息或执行未经授权的操作。

3.可读性和可维护性降低：
使用反射的代码往往难以理解和维护，因为它破坏了代码的直接性和明确性。反射调用可能隐藏在多层抽象之下，使得跟踪代码的执行流程和调试变得更加困难。

4.类型安全问题：
反射在运行时动态地处理类型信息，这可能导致类型安全问题。如果代码没有正确处理类型不匹配或类型加载失败的情况，就可能会抛出异常，影响程序的稳定性和可靠性。

5.增加代码的复杂性：
反射的使用需要开发者具备较高的编程技能和深厚的语言理解，因为反射涉及到很多底层的概念和复杂的操作。这可能会增加代码的复杂性和学习曲线，使得其他开发者难以理解和维护代码。

6.难以调试：
由于反射操作的动态性和不确定性，使用反射的代码在调试时可能会更加困难。开发者需要熟悉反射的工作原理，并能够正确地设置断点、跟踪调用栈和检查变量值，以便找到并修复问题。

7.环境问题：
反射的可用性和性能可能受到不同编程语言和运行环境的影响。例如，某些语言或框架可能提供了更高效的反射实现，而另一些则可能相对较慢或不够灵活。此外，不同的运行环境（如JVM、CLR等）也可能对反射的实现和性能产生影响。


反射的适用场景：
1. 动态加载类与创建对象
灵活扩展：通过反射，可以在运行时动态地加载类并创建其实例，这使得程序可以根据配置文件或用户输入来决定要加载的类，从而实现灵活的代码扩展性。例如，在插件系统中，主程序可以通过反射动态地加载和使用插件，而无需在编译时知道插件的具体实现。

2. 动态调用方法
灵活业务逻辑：反射允许程序在运行时动态地调用对象的方法，这可以根据不同的条件来选择调用不同的方法，实现更加灵活的业务逻辑。这在处理多态性、实现回调机制或构建动态框架时非常有用。

3. 操作对象的属性
灵活控制对象状态：通过反射，可以在运行时动态地读取或修改对象的私有字段，实现对对象状态的灵活控制。这在需要访问或修改对象内部状态，但又不想直接暴露这些字段时非常有用。

4. 获取类的信息
运行时类型检查：反射允许程序在运行时动态地获取类的信息，如类的构造方法、字段、方法等，并进行相应的操作。这对于实现类型检查、序列化/反序列化、动态UI生成等场景非常有用。

5. 注解处理器
编译时或运行时处理注解：通过反射，可以编写注解处理器来处理自定义注解。这可以在编译期间或运行时对注解进行解析和处理，实现一些特定的功能，如依赖注入、权限控制、日志记录等。

6. 框架支持
依赖注入与AOP：许多Java框架（如Spring）都广泛使用了反射机制，通过反射来实现依赖注入、AOP（面向切面编程）等功能。这些框架利用反射来动态地创建和管理对象，以及在不修改源代码的情况下增强类的功能。

7. 第三方库与插件开发
插件化架构：在开发支持插件的应用程序时，反射允许程序在运行时加载和使用第三方库或插件。这有助于构建可扩展和可定制的应用程序，同时降低对第三方库的依赖。

8. 安全性与权限控制
动态权限检查：虽然反射本身可能带来安全性问题，但也可以利用反射来实现动态权限检查。通过反射获取类的成员信息，并结合安全策略来动态地控制对类成员的访问权限。

注意事项
性能开销：反射操作相比直接调用方法和访问属性会有一定的性能开销。因此，在性能要求较高的场景下应慎重使用反射。
安全性问题：通过反射可以绕过Java语言的访问控制机制访问私有字段和方法，这可能导致代码的安全性问题。需要谨慎使用并确保不会暴露敏感信息或执行未授权的操作。
综上所述，反射在动态加载类与创建对象、动态调用方法、操作对象的属性、获取类的信息、注解处理器、框架支持以及第三方库与插件开发等多个场景中都有广泛的应用。然而，在使用反射时需要注意其性能开销和安全性问题，并采取相应的措施来降低这些风险。


Java动态代理：
代理类在程序运行时创建的代理方式被成为动态代理。在静态代理中，代理类（RenterProxy）是自己已经定义好了的，在程序运行之前就已经编译完成。
而动态代理是在运行时根据我们在Java代码中的“指示”动态生成的。Java运用了反射实现了动态代理。
动态代理相较于静态代理的优势在于可以很方便的对代理类的所有函数进行统一管理，如果我们想在每个代理方法前都加一个方法，如果代理方法很多，我们需要在每个代理方法都要写一遍，很麻烦。而动态代理则不需要。

代码实现：创建RenterInvocationHandler类，这个类实现了InvocationHandler接口，并持有一个被代理类的对象，InvocationHandler中有一个invoke方法，所有执行代理对象的方法都会被替换成执行invoke方法。
然后通过反射在invoke方法中执行代理类的方法。在代理过程中，在执行代理类的方法前或者后可以执行自己的操作，这就是spring aop的主要原理。

动态代理之Cglib代理：
动态代理需要被代理类实现接口，如果被代理类没有实现接口，那么这么实现动态代理？这时候就需要用到CGLib了。这种代理方式就叫做CGlib代理。
Cglib代理也叫作子类代理，他是通过在内存中构建一个子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，然后加入自己需要的操作。因为使用的是继承的方式，所以不能代理final 类
动态代理与CGLib动态代理都是实现Spring AOP的基础。如果加入容器的目标对象有实现接口,用动态代理，如果目标对象没有实现接口,用Cglib代理。

创建目标类：
public class PersonService {
    public void sayHello() {
        System.out.println("Hello, World!");
    }

    public void sayGoodbye() {
        System.out.println("Goodbye, World!");
    }
}

创建代理类：
import net.sf.cglib.proxy.Enhancer;
import net.sf.cglib.proxy.MethodInterceptor;
import net.sf.cglib.proxy.MethodProxy;

import java.lang.reflect.Method;

public class CglibProxy implements MethodInterceptor {

    private Object target;

    public CglibProxy(Object target) {
        this.target = target;
    }

    public Object getProxy() {
        Enhancer enhancer = new Enhancer();
        enhancer.setSuperclass(target.getClass());
        enhancer.setCallback(this);
        return enhancer.create();
    }

    @Override
    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {
        System.out.println("Before method: " + method.getName());
        Object result = proxy.invokeSuper(obj, args);
        System.out.println("After method: " + method.getName());
        return result;
    }

    public static void main(String[] args) {
        PersonService personService = new PersonService();
        CglibProxy proxy = new CglibProxy(personService);
        PersonService proxyPersonService = (PersonService) proxy.getProxy();
        
        proxyPersonService.sayHello();
        proxyPersonService.sayGoodbye();
    }
}

CglibProxy类：
实现了MethodInterceptor接口，该接口定义了代理类的方法拦截器。
构造函数CglibProxy(Object target)接受一个目标对象。
getProxy()方法创建代理对象。
intercept()方法在目标方法调用前后执行自定义逻辑。

Enhancer类：
Cglib中的Enhancer类用于生成代理类。
setSuperclass()方法设置目标类。
setCallback()方法设置回调，即MethodInterceptor。

intercept()方法：
在目标方法调用前后打印日志。
使用proxy.invokeSuper(obj, args)调用目标类的原始方法。

输出如下：
Before method: sayHello
Hello, World!
After method: sayHello
Before method: sayGoodbye
Goodbye, World!
After method: sayGoodbye


Java编译特点：
Java的编译特点主要体现在其跨平台性、解释执行、即时编译、编译和解释的结合、安全性、可移植性和动态加载等方面。这些特点使得Java成为一种功能强大、安全可靠、易于学习和开发的编程语言，适用于各种应用场景。

Javac：
Javac的编译过程：
Javac编译过程主要包括四个步骤：词法分析、语法分析、语义分析和代码生成。词法分析器逐行读取源代码，
将源代码匹配到Token流，识别每一个单词是什么东西；语法分析器将Token流转换成更加结构化的语法树；
语义分析器对语法树进行精简和处理；代码生成器将语法树生成java字节码。


Java JVM：
Java虚拟机(Jvm)是可运行Java代码的假想计算机。Java虚拟机包括一套字节码指令集、一组寄存器、一个栈、一个垃圾回收堆和一个存储方法域。
JVM让Java成为跨平台语言，Java有JVM从软件层面屏蔽了底层硬件、指令层面的细节让他兼容各种系统。

Java虚拟机主要由以下几个部分组成：
1.虚拟机栈。每个线程都在这个空间有一个私有的空间，每个线程会执行一个或多个方法，一个方法对应一个栈帧，栈帧包括局部变量表、操作数栈、动态链接、方法返回地址、附加信息等。
2.堆。所有线程共享的内存区域，用于存放对象实例和数组。
3.方法区。用于存放已被虚拟机加载的类信息、常量、静态变量等数据。
4.程序计数器。用于保存当前线程所执行的字节码指令的行号。

JVM的原理：				
Java源文件，通过编译器，能够生产相应的.Class文件，也就是字节码文件，而字节码文件又通过Java虚拟机中的解释器，也就是前面所有的Java虚拟机中的字节码指令集编译成特定机器上的机器码
实现步骤：
1.Java源文件—->编译器—->字节码文件
2.字节码文件—->Jvm—->机器码

每一种平台的解释器是不同的，但是实现的虚拟机是相同的。这也就是Java为什么能够跨平台的原因。当一个程序从开始运行一个程序，这时虚拟机就开始实例化了。多个程序启动就会存在多个虚拟机实例。
程序退出或者关闭。则虚拟机实例消亡。多个虚拟机实例之间数据不能共享。				


Java与其他语言在编译上的区别：

Java在运行原理上与C++的对比：
区别于其他后端语言，C++在计算机上的运行原理与Java完全不同。C++程序是通过编译和链接过程转换为可执行程序后运行的。

1.首先，C++源代码被编译器转换成机器码，并链接成一个可执行文件。这个过程包括将源代码翻译成汇编语言，
然后将汇编语言转换为机器语言。链接器则将这个可执行文件与系统库和其他必要的库文件链接在一起，形成一个完整的程序。
2.当程序被执行时，操作系统会将程序加载到内存中，创建一个进程，并分配给该进程一些系统资源，如内存、文件、设备等。
程序中的代码会在CPU上运行，并按照程序员的指令执行各种操作。
3.在程序的运行过程中，操作系统会管理程序的执行，包括任务调度、内存管理、文件系统等方面。
程序运行结束后，操作系统会将程序的状态保存下来，并释放分配给该进程的资源。

总之，从计算机底层的角度来看，C++程序的运行过程包括编译、链接、加载、执行和退出等阶段，需要操作系统和硬件的支持。


C++没有虚拟机和垃圾回收机制，它在编程的时候需要注意哪些问题？
相比于Java，C++没有虚拟机和垃圾回收机制，这使得C++在运行时的确更加复杂。

首先，C++需要手动管理内存，这意味着开发人员需要手动分配和释放内存。如果没有正确地管理内存，可能会导致内存泄漏、野指针等问题，这会增加程序的复杂性和运行风险。
其次，C++没有类似于Java的垃圾回收机制，因此需要开发人员手动管理对象的生命周期，以避免内存泄漏和悬挂指针等问题。这需要更加精细的内存管理技巧和经验，增加了程序的复杂性和开发难度。
此外，C++支持多种编程范式和语言特性，如面向对象编程、泛型编程、函数式编程等，这使得C++程序可能更加灵活和复杂。

总的来说，虽然C++没有虚拟机和垃圾回收机制，但它提供了更低级别的控制和更高的性能，同时也需要更严格的内存管理和更多的编程技巧。因此，在运行原理上，C++的确比Java更加复杂。


python的运行原理与Java也不相同：
从计算机底层的角度来说，Python运行的过程可以大致分为以下几个步骤：

编译：当Python代码被执行时，首先会经过一个编译的过程。Python的编译过程与传统的编译过程有些不同，它并不是将代码直接编译成机器码，而是将代码编译成一种叫做字节码（bytecode）的形式。
这些字节码是平台无关的，可以在任何支持Python的平台上运行。这个编译过程是由Python解释器自动完成的，无需程序员手动干预。

解释执行：接下来，编译后的字节码会被加载到Python解释器中。Python解释器会按照字节码的指令顺序逐条解释执行。
在这个过程中，解释器会根据字节码指令调用相应的Python函数或对象方法，完成代码的逻辑执行。

对象引用：Python是一种动态类型语言，变量可以引用任何类型的对象。
在解释执行过程中，当一个变量被引用时，解释器会根据变量的类型和值，动态地创建相应的对象。
这些对象可以是Python内置类型（如整数、字符串、列表等），也可以是用户自定义的类型。

垃圾回收：为了防止内存泄漏，Python使用了垃圾回收机制。当一个对象不再被引用时，垃圾回收器会自动将其标记为可回收对象，
并在适当的时候释放其内存空间。

交互式模式：Python还支持交互式模式，可以在命令行中直接执行Python代码。当在交互式模式下输入代码时，解释器会立即执行并输出结果。

总之，Python的运行过程可以看作是将源代码编译成字节码，然后由解释器逐条解释执行字节码的过程。
在这个过程中，解释器还会处理变量引用、垃圾回收等操作，以保证程序的正常运行。



Java与python对比，在运行上的区别：
Python是一种解释性语言，它的解释器会在运行时解释执行代码。虽然Python解释器可以通过字节码进行编译，
但是在运行时仍然需要进行解释，这可能会影响Python的运行速度。

相反，Java是一种编译性语言，它的代码在编译时会生成与平台相关的机器码，因此Java的执行速度通常比Python更快。

然而，在实际应用中，Python和Java的运行速度差异可能并不明显。对于一些需要快速开发迭代的项目，Python的灵活性和易用性可能会更受欢迎。而对于一些需要更高运行效率的项目，Java可能会更适合。


Java中类加载的过程：
类加载器：
Java类加载器是Java虚拟机的一部分，它的主要任务是将编译好的Java类到Java虚拟机中。类加载器有三个主要的任务：

找到类的二进制文件（.class文件或其他形式的类描述符）。
将这个文件转化为一个代表该类的数据结构。
通过这个数据结构创建一个代表该类的java.lang.Class对象。

类加载器子系统负责从文件系统或者网络中加载Class文件,class文在文件开头有特定的文件标识。
ClassLoader只负责clalss文件的加载,至于它是否可以运行,则由ExecutionEngine决定。
加载的类信息存放于一块称为方法区的内存空间。除了类的信息外,方法区中还会存放运行时常量池信息,
可能还包括字符串字面量和数字常量(这部分常量信息是Class文件中常量池部分的内存映射)。

注意：Java的反射正是运用了JVM类加载机制生成的反射类


类的加载过程：
验证(Verify):
目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求,保证被加载类的正确性,不会危害虚拟机自身安全。
主要包括四种验证,文件格式验证,元数据验证,字节码验证,符号引用验证。

准备(Prepare):
为类变量分配内存并且设置该类变量的默认初始值,即零值。
这里不包含用final修饰的static,因为final在编译的时候就会分配了,准备阶段会显式初始化。
这里不会为实例变量分配初始化,类变量会久配在方法区中,而实例变量是会随着对象一起分配到Java堆中。

解析(Resolve):
将常量池内的符号引用转换为直接引用的过程。
事实上,解析操作往往会伴随着JVM在执行完初始化之后再执行。
符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《java虚拟机规范》的Class文件格式中。
直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。
解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的
CONSTANT Class info, CONSTANT Fieldref infoCONSTANT Methodref info等

初始化:
初始化阶段就是执行类构造器方法<clinit>()的过程。
此方法不需定义,是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。
构造器方法中指令按语句在源文件中出现的顺序执行。
<clinit>()不同于类的构造器。(关联:构造器是虚拟机视角下的<init>())
若该类具有父类,JVM会保证子类的<clinit>()执行前,父类的<clinit>()已经执行完毕。
虚拟机必须保证一个类的<clinit>()方法在多线程下被同步加锁 

获取ClassLoader的方法：
1.直接获取当前类的ClassLoader
ClassLoader classLoader = Class.forName("java.lang.String").getClassLoader()
2.获取当前线程上下文的ClassLoader
ClassLoader classLoader1 = Thread.currentThread().getContextClassLoader();
3.获取系统的ClassLoader
ClassLoader classLoader2 = ClassLoader.getSystemClassLoader().getParent();
4.获取调用者的ClassLoader
ClassLoader classLoader3 = DriverManager.getCallerClassLoader ();


双亲委派机制：
Java JVM的双亲委派机制（Parent Delegation Mechanism）是一种类加载机制，它规定了当一个类加载器收到了类加载请求时，
它首先不会自己尝试去加载这个类，而是把这个请求委派给父类加载器去完成，直到最终由系统类加载器（Bootstrap ClassLoader）加载。
如果父类加载器也无法完成这个加载请求（比如父类加载器也找不到这个类），那么子加载器才会尝试自己去加载。

这种双亲委派机制的主要目的是为了保护Java的核心API，防止恶意代码通过自定义类加载器的方式去加载和修改这些核心API。
它确保了Java的核心类库只被系统类加载器加载一次，从而避免了类定义的混乱和冲突。

沙箱安全机制：
Java JVM里的沙箱安全机制是一种将Java代码限定在虚拟机特定运行范围中，并严格限制代码对本地系统资源访问的机制，以保证对代码的有效隔离，防止对本地系统造成破坏。
沙箱主要限制了CPU、内存、文件系统、网络等系统资源的访问，不同级别的沙箱对这些资源访问的限制也可以不一样。所有的Java程序运行都可以指定沙箱，可以定制安全策略。


.class文件经过类加载器到达运行时数据区，在运行时数据区进行下一步处理。
JVM运行时数据区：运行时数据区是JVM在执行Java程序时使用的内存区域。


PS：CPU如何调用内存去运行系统的？
在计算机系统中，内存扮演着非常重要的角色。它是CPU和外部存储设备（如硬盘）之间的桥梁，用于临时存储正在被处理的数据和指令。
CPU通过内存来访问和操作这些数据，以执行程序和操作系统的各种任务。
CPU调用内存的方式是通过地址总线（Address Bus）和数据总线（Data Bus）。地址总线用于指定要访问的内存地址，数据总线则用于传输数据。

以下是CPU调用内存的基本步骤：
1.CPU通过地址总线发送要访问的内存地址。这个地址是由程序计数器（Program Counter）生成的，它指向下一条要执行的指令在内存中的位置。
2.内存根据接收到的地址，将对应位置上的数据加载到数据总线上。
3.CPU从数据总线上读取加载到数据总线上的数据，然后对其进行处理。
4.如果CPU需要向内存写入数据，它首先通过地址总线发送要写入数据的内存地址，然后将要写入的数据发送到数据总线上。
5.内存接收到写入数据的请求和数据后，将数据存储到指定地址的位置。

通过这种方式，CPU可以不断地在内存中读取和写入数据，以执行程序和操作系统的各种任务。
内存的访问速度比外部存储设备快得多，因此CPU通过访问内存可以更快地执行程序和操作系统的操作。 


PS：在计算机系统里，什么是进程，什么是线程？
在计算机系统里，进程是指在系统中正在运行的一个应用程序，程序一旦运行就是进程，而线程是进程的一个实体，是进程的一条执行路径。
进程是系统资源分配的独立实体，且每个进程都有独立的地址空间。一个进程可以拥有多个线程，每个线程使用其所属进程的栈空间。
线程是CPU独立运行和独立调度的基本单位，它负责当前进程中程序的执行。
线程之间的通信是线程直接读写进程数据段（如全局变量）来进行通信，需要进程同步和互斥的手段的辅助，以保证数据的一致性。

以微信为例，微信是一个进程，在微信中处理网络通信的是一个线程。


如何实现进程间通信？
进程间通信（Inter-Process Communication, IPC）是操作系统中多个进程之间交换数据或信号的一些技术或方法。实现进程间通信的方式多种多样，具体取决于操作系统和应用程序的需求。以下是一些常见的进程间通信方式：

管道（Pipes）：
匿名管道：通常用于父子进程之间的通信。它是单向的，数据只能从一个进程流向另一个进程。
命名管道（FIFO）：允许无亲缘关系的进程之间进行通信。命名管道在文件系统中有一个名字，任何进程都可以通过打开这个文件来写入或读取数据。

消息队列（Message Queues）：
消息队列是消息的链接列表，存放在内核中并由消息队列标识符标识。它允许一个或多个进程向它写入或从中读取消息。消息队列与管道类似，但消息队列的消息是不连续的，具有独立的格式，并且拥有独立的读/写权限和属性。

共享内存（Shared Memory）：
共享内存允许两个或多个进程共享一个给定的存储区。因为数据不需要在客户机和服务器之间复制，所以这是最快的一种IPC方式。但是，它要求程序员能够处理同步和互斥问题。

信号量（Semaphores）：
信号量主要用于控制多个进程对共享资源的访问。它本质上是一个计数器，用于限制对某个特定资源的访问数量。信号量可以用来实现进程间的同步和互斥。

套接字（Sockets）：
套接字是一种更通用的进程间通信机制，它可用于不同机器上的进程间通信。套接字提供了端到端的通信服务，允许进程通过网络发送和接收数据。套接字不仅支持TCP/IP协议，还支持其他协议。

信号（Signals）：
信号是软件中断，提供了一种处理异步事件的方法。一个进程可以向另一个进程发送信号，接收信号的进程可以执行相应的信号处理函数。但是，信号传递的信息量有限，主要用于通知接收进程某个事件已经发生。

文件映射（Memory-Mapped Files）：
文件映射允许进程将文件或文件的一部分映射到进程的地址空间。这样，进程就可以像访问内存一样访问文件内容，从而实现进程间的数据共享。

总结：在实现进程间通信时，需要根据具体的应用场景和需求选择最合适的通信方式。例如，对于需要高速数据交换的场景，共享内存可能是最好的选择；而对于需要跨网络进行通信的场景，套接字则是不可或缺的。



在JVM中，每个线程是独立：程序计数器、栈、本地栈。线程间共享：堆、堆外内存（永久代或元空间、代码缓存）。

PC寄存器为什么被设定为私有？
为了能够准确地记录各个线程正在执行的当前字节码指令地址，为每一个线程都分配一个PC寄存器


方法区（Method Area）
方法区是所有线程共享的内存区域，它用于存储已被Java虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。
它有个别命叫Non-Heap（非堆）。当方法区无法满足内存分配需求时，抛出OutOfMemoryError异常。

Java堆（Java Heap）
Java堆是Java虚拟机所管理的内存中最大的一块，是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例。
在Java虚拟机规范中的描述是：所有的对象实例以及数组都要在堆上分配。
java堆是垃圾收集器管理的主要区域，因此也被成为“GC堆”。
从内存回收角度来看java堆可分为：新生代和老生代。


从内存分配的角度看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区。
无论怎么划分，都与存放内容无关，无论哪个区域，存储的都是对象实例，进一步的划分都是为了更好的回收内存，或者更快的分配内存。
根据Java虚拟机规范的规定，java堆可以处于物理上不连续的内存空间中。当前主流的虚拟机都是可扩展的（通过 -Xmx 和 -Xms 控制）。
如果堆中没有内存可以完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。


程序计数器、PC寄存器（Program Counter Register）
程序计数器是一块较小的内存空间，它可以看作是：保存当前线程所正在执行的字节码指令的地址(行号)
由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，一个处理器都只会执行一条线程中的指令。

因此，为了线程切换后能恢复到正确的执行位置，每条线程都有一个独立的程序计数器，各个线程之间计数器互不影响，独立存储。称之为“线程私有”的内存。
程序计数器内存区域是虚拟机中唯一没有规定OutOfMemoryError情况的区域。


Java JVM里的PC寄存器是链表结构吗？
在Java虚拟机中，PC寄存器是一个较小的寄存器，用于存储当前正在执行的指令的地址。这个寄存器是线程私有的，每个线程都有自己的程序计数器。
PC寄存器的主要作用是帮助JVM实现指令集中的跳转指令。当JVM执行跳转指令时，程序计数器（PC）的值会被更新为跳转指令的目标地址。
这样，JVM就可以从跳转指令的目标地址开始执行字节码指令。

因此，PC寄存器的结构更类似于一个指向当前执行字节码指令的指针，而不是链表结构。

Java虚拟机栈（Java Virtual Machine Stacks）
java虚拟机是线程私有的，它的生命周期和线程相同。

虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）
用于存储局部变量表、操作数栈、动态链接、方法出口等信息，栈帧和方法是一一对应的关系。

解释：每虚拟机栈中是有单位的，单位就是栈帧，一个方法一个栈帧。一个栈帧中他又要存储、局部变量表，操作数栈，动态链接，出口等。
虚拟机栈的异常：StackOverFlowError（栈溢出）、OutOfMemoryError（内存不足）
设置栈内存大小：设置选项-Xss来设置线程的最大栈空间


JVM虚拟机栈的结构：
一、局部变量表：
局部变量表是用于存储方法中定义的局部变量的数据结构。这些变量包括方法参数、方法内部声明的变量等。
局部变量表中的每个条目都包含变量的名称、类型和值等信息。
在Java中，每个方法调用都会在调用栈中创建一个新的栈帧（Stack Frame），用于存储该方法的局部变量、操作数栈和其他信息。
当方法被调用时，它的局部变量表会被创建并存储在栈帧中。当方法执行完毕并返回时，该栈帧会被弹出，同时其中的局部变量表也会被销毁。
在JVM中，局部变量表是在编译时确定的，每个方法的每个局部变量都会在局部变量表中创建一个条目。
在运行时，可以通过访问局部变量表来获取和修改局部变量的值。

二、操作数栈：
操作数栈是虚拟机栈中的一个重要部分，它用于存储方法执行过程中的中间结果和操作数。
在方法的执行过程中，许多操作都需要在操作数栈中进行，例如算术运算、方法调用、对象创建等。
操作数栈中的每个元素都有一个值和一个类型，这些元素可以是原始类型（例如int、char等）或引用类型（例如对象、数组等）。
在方法执行过程中，一些操作（例如算术运算、方法调用等）会向操作数栈中压入新的元素，而其他操作（例如取值、赋值等）则会从操作数栈中弹出元素。
当一个方法调用另一个方法时，被调用的方法会在自己的栈帧中创建一个新的操作数栈。
被调用方法的执行结果会被压入到调用方法的操作数栈中，以供后续使用。

总之，Java虚拟机中的操作数栈是用于存储中间结果和操作数的数据结构，它在方法的执行过程中起着非常重要的作用。


三、栈顶缓存：
Java虚拟机（JVM）的栈顶缓存（TSA，Top Stack Abstraction）是一种优化技术，用于减少在JVM的堆栈中创建和销毁对象的开销。
这种技术主要在HotSpot JVM中使用。

在JVM中，每次方法调用都会创建一个新的栈帧（stack frame），其中包含该方法的局部变量、操作数栈和方法出口信息。
每当方法调用发生时，JVM需要在堆栈中分配新的栈帧空间，并在方法执行结束后释放这些空间。这是一个相对昂贵的过程，特别是在频繁的方法调用情况下，可能会导致性能下降。

栈顶缓存技术的核心思想是在堆栈上保留一些额外的栈帧空间，以便在下次方法调用时重用这些空间，而无需再次分配和释放内存。
这样可以减少堆栈分配和垃圾回收的开销，提高方法的执行速度。

在HotSpot JVM中，栈顶缓存的实现是通过JVM的内部数据结构完成的。JVM维护了一个栈顶缓存区（stack top cache），
用于存储当前线程的堆栈顶部的位置信息。当线程执行方法调用时，JVM会检查栈顶缓存区是否有足够的空间来容纳新的栈帧。
如果有足够的空间，JVM会将新的栈帧分配到缓存区中的空闲空间中，而不是在堆栈中创建新的栈帧。这样可以避免频繁的堆栈分配操作，提高了方法的执行性能。

需要注意的是，栈顶缓存技术并不是没有限制的。如果线程的堆栈空间不足以容纳新的栈帧，或者缓存区中没有足够的空间来容纳新的栈帧，
那么JVM仍然需要在堆栈中分配新的栈帧空间。此外，如果线程调用的方法数量过多，缓存区可能会被填满，导致无法继续使用栈顶缓存技术。
总之，Java虚拟机中的栈顶缓存技术是一种优化技术，用于减少在堆栈中创建和销毁对象的开销，从而提高方法的执行性能。
它是通过在堆栈上保留一些额外的栈帧空间，以便在下次方法调用时重用这些空间来实现的。


四、动态链接：
Java虚拟机（JVM）的动态链接（Dynamic Linking）是指在运行时解析和加载类、接口、字段和方法的信息。
在Java中，类、接口、字段和方法并不是在程序编译时链接在一起的，而是在程序运行时动态链接到一起的。
动态链接的实现是通过Java虚拟机的类加载器（ClassLoader）来完成的。类加载器负责从文件系统、网络或其他来源加载类的字节码文件，
并将其转换为Java虚拟机中的类对象。在类加载过程中，类加载器会解析类的结构，
确定类之间的关系，例如确定一个类继承自另一个类，实现一个接口等。

在动态链接过程中，Java虚拟机会对类进行验证和解析。验证是确保类的字节码文件符合Java虚拟机的规范，并且没有安全方面的问题。
解析是将类中的符号引用转换为直接引用。符号引用是指在类或接口的声明中使用的一种引用方式，例如一个方法调用另一个方法或访问一个字段。
直接引用是指具体的内存地址或方法指针，例如在调用方法时使用的指令地址。

动态链接的好处是可以在程序运行时动态地加载新的类或更新已有的类，而不需要重新编译程序。
这使得Java程序具有很高的灵活性和可扩展性，可以方便地添加或修改功能。
然而，动态链接也带来了一些性能开销，因为需要在运行时解析和加载类、接口、字段和方法的信息。

Java虚拟机栈的执行原理：
Java虚拟机栈执行原理是遵循先进后出（后进先出）原则，JVM直接对Java栈的操作只有两个，就是对栈帧的压栈和出栈。
在一条活动线程中，一个时间点上，只会有一个活动的栈帧。即只有当前正在执行的方法的栈帧（栈顶栈帧）是有效的。
这个栈帧被称为当前栈帧。与当前栈帧相对应的方法就是当前方法。定义这个方法的类就是当前类。
执行引擎运行的所有字节码指令只针对当前栈帧进行操作。如果在该方法中调用了其他方法，对应的新的栈帧会被创建出来，
放在栈的顶端，成为新的当前帧。不同线程中所包含的栈帧不允许存在相互引用的，即不可能在一个栈帧之中引用另外一个线程的栈帧。


JVM中堆的结构：
JVM（Java Virtual Machine）中的堆（Heap）是Java应用程序运行时内存管理的重要组成部分，用于存储Java对象实例和数组。堆内存的大小是可调节的，它取决于应用程序的需求和JVM的配置。
堆的结构可以划分为以下几个主要部分：
堆的总体结构
堆内存：是Java虚拟机所管理的内存中最大的一块，是线程共享的内存区域，几乎所有的对象实例都在这里分配内存。

详细分区
堆内存可以进一步细分为以下几个区域：

1.新生代（Young Generation）：
新生代是JVM堆的一小部分，通常占整个堆的1/10到1/4，主要用于存储新创建的对象。
新生代又可以分为三个区域：
Eden区：大多数对象都在Eden区中创建。
Survivor区：Survivor区分为两个区（S0和S1），也称为From区和To区。当Eden区满时，会触发Minor GC（也称为Young GC），将还存活的对象移动到Survivor区。这两个区通过复制算法来回收内存，即每次GC后，两个Survivor区会交换角色。

老年代（Old Generation）：
老年代是JVM堆的另一部分，用于存储长时间存活的对象。
当Eden区或Survivor区中的对象经过一定次数的GC后仍然存活，或者大对象直接在老年代中创建时，会触发Major GC（也称为Full GC），对老年代进行垃圾回收。

永久代（PermGen，JDK 7及以前版本） 或 元空间（Metaspace，JDK 8及以后版本）：
永久代或元空间用于存储JVM字节码和类的元数据。在JDK 8之前，这部分被称为永久代，并作为堆内存的一部分；从JDK 8开始，它被元空间替代，元空间使用本地内存，即操作系统的内存，以减轻堆内存的压力。

堆内存的特点
线程共享：堆是线程共享的内存区域，所有的线程都访问同一个堆。
垃圾回收的主要场所：JVM的垃圾回收器定期扫描堆中的对象，找到不再被引用的对象，并释放它们的内存。
内存管理：堆内存的大小和分区比例可以通过JVM参数进行调整，例如-Xms和-Xmx用于设置堆内存的初始大小和最大大小，-XX:NewRatio用于设置新生代和老年代的比例，-XX:SurvivorRatio用于设置Eden区和Survivor区的比例。

堆内存的诊断工具
用于堆内存诊断的工具包括jps、jmap、jconsole和jvisualvm等，这些工具可以帮助开发者监控和分析堆内存的使用情况，以及定位内存泄漏等问题。

综上所述，JVM中的堆结构是一个复杂的内存管理区域，通过合理的配置和使用，可以有效地支持Java应用程序的运行和性能优化。


JVM内存结构：
直接内存（Direct Memory）：
直接内存不是虚拟机运行时数据区的一部分，也不是java虚拟机规范中定义的内存区域。
但是既然是内存，肯定还是受本机总内存（包括RAM以及SWAP区或者分页文件）大小以及处理器寻址空间的限制。

在JDK1.4 中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区（Buffer）的I/O 方式，
它可以使用native 函数库直接分配堆外内存，然后通脱一个存储在Java堆中的DirectByteBuffer 对象作为这块内存的引用进行操作。
这样能在一些场景中显著提高性能，因为避免了在Java堆和Native（本地）堆中来回复制数据。

直接内存与堆内存的区别：
直接内存申请空间耗费很高的性能，堆内存申请空间耗费比较低
直接内存的IO读写的性能要优于堆内存，在多次读写操作的情况相差非常明显


JVM的垃圾回收机制：
垃圾回收机制简称GC

GC主要用于Java堆的管理。Java 中的堆是 JVM 所管理的最大的一块内存空间，主要用于存放各种类的实例对象。

关于什么是垃圾回收机制：
程序在运行过程中，会产生大量的内存垃圾（一些没有引用指向的内存对象都属于内存垃圾，因为这些对象已经无法访问，程序用不了它们了，
对程序而言它们已经死亡），为了确保程序运行时的性能，java虚拟机在程序运行的过程中不断地进行自动的垃圾回收（GC）。

GC是不定时去堆内存中清理不可达对象。不可达的对象并不会马上就会直接回收， 垃圾收集器在一个Java程序中的执行是自动的，
不能强制执行清楚那个对象，即使程序员能明确地判断出有一块内存已经无用了，是应该回收的，程序员也不能强制垃圾收集器回收该内存块。
程序员唯一能做的就是通过调用System.gc 方法来"建议"执行垃圾收集器，但是他是否执行，什么时候执行却都是不可知的。
这也是垃圾收集器的最主要的缺点。当然相对于它给程序员带来的巨大方便性而言，这个缺点是瑕不掩瑜的。

手动执行GC：System.gc(); 

finalize方法作用：
-finalize()方法是在每次执行GC操作之前时会调用的方法，可以用它做必要的清理工作。
-它是在Object类中定义的，因此所有的类都继承了它。子类覆盖finalize()方法以整理系统资源或者执行其他清理工作。
finalize()方法是在垃圾收集器删除对象之前对这个对象调用的。

新生代、老年代、永久代(方法区)的区别：
-Java 中的堆是 JVM 所管理的最大的一块内存空间，主要用于存放各种类的实例对象。
-在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。
-老年代就一个区域。新生代 ( Young ) 又被划分为三个区域：Eden、From Survivor、To Survivor。
-这样划分的目的是为了使 JVM 能够更好的管理堆内存中的对象，包括内存的分配以及回收。

-默认的，新生代 ( Young ) 与老年代 ( Old ) 的比例的值为 1:2 ( 该值可以通过参数 –XX:NewRatio 来指定 )，
即：新生代 ( Young ) = 1/3 的堆空间大小。老年代 ( Old ) = 2/3 的堆空间大小。
-其中，新生代 ( Young ) 被细分为 Eden 和 两个 Survivor 区域，这两个 Survivor 区域分别被命名为 From Survivor 和 ToSurvivor ，以示区分。
-默认的，Edem ：From Survivor ： To Survivor = 8 : 1 : 1 ( 可以通过参数 –XX:SurvivorRatio 来设定 )，
即： Eden = 8/10 的新生代空间大小，From Survivor = To Survivor = 1/10 的新生代空间大小。

-JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块 Survivor 区域是空闲着的。
-因此，新生代实际可用的内存空间为 9/10 ( 即90% )的新生代空间。
-永久代就是JVM的方法区。在这里都是放着一些被虚拟机加载的类信息，静态变量，常量等数据。
这个区中的东西比老年代和新生代更不容易回收。


GC算法：
Minor GC、Major GC、Full GC区别及触发条件：
-Minor GC是新生代GC，指的是发生在新生代的垃圾收集动作。由于java对象大都是朝生夕死的，所以Minor GC非常频繁，一般回收速度也比较快。
-Major GC是老年代GC，指的是发生在老年代的GC，通常执行Major GC会连着Minor GC一起执行。Major GC的速度要比Minor GC慢的多。
-Full GC是清理整个堆空间，包括年轻代和老年代

Minor GC 触发条件一般为：
eden区满时，触发MinorGC。即申请一个对象时，发现eden区不够用，则触发一次MinorGC。
​ 新创建的对象大小 > Eden所剩空间
Major GC和Full GC 触发条件一般为：
Major GC通常是跟full GC是等价的

每次晋升到老年代的对象平均大小>老年代剩余空间
MinorGC后存活的对象超过了老年代剩余空间
永久代空间不足
执行System.gc()
CMS GC异常
堆内存分配很大的对象


JVM垃圾收集器：
垃圾收集器是垃圾回收算法（引用计数法、标记清楚法、标记整理法、复制算法）的具体实现，不同垃圾收集器、不同版本的JVM所提供的垃圾收集器可能会有很在差别。

1.Serial 收集器：新生代。发展历史最悠久的收集器。它是一个单线程收集器，它只会使用一个 CPU 或者线程去完成垃圾收集工作，而且在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。
特点：
新生代收集器，使用复制算法收集新生代垃圾。
单线程的收集器，GC工作时，其它所有线程都将停止工作。
简单高效，适合单 CPU 环境。单线程没有线程交互的开销，因此拥有最高的单线程收集效率。

2.ParNew 收集器：新生代。Serial 的多线程版本，即同时启动多个线程去进行垃圾收集。
特点：
新生代收集器。ParNew垃圾收集器是Serial收集器的多线程版本，采用复制算法。
除了多线程外，其余的行为、特点和Serial收集器一样。
只有它能与 CMS 收集器配合使用。
但在单个CPU环境中，不比Serail收集器好，多线程使用它比较好。

3.Parallel Scavenge 收集器：新生代。和 ParNew 的关注点不一样，该收集器更关注吞吐量，尽快地完成计算任务。
特点：
新生代收集器。
采用复制算法。
多线程收集。
与ParNew 不同的是：高吞吐量为目标，（减少垃圾收集时间，让用户代码获得更长的运行时间）

4.Serial Old 收集器：Serial 的老年代版本，使用标记 - 整理算法。
特点：
老年代收集器， 采用"标记-整理"算法。
单线程收集。

5.Parallnel old 收集器，多线程：Parallel 的老年代版本，使用标记 - 整理算法。
特点
针对老年代。
采用"标记-整理"算法。
多线程收集。
但在单个CPU环境中，不比Serial Old收集器好，多线程使用它比较好。

6.CMS 收集器：老年代。是一种以获取最短回收停顿时间为目标的收集器，适用于互联网站或者 B/S 系统的服务端上。
特点：
针对老年代，采用标记-清楚法清除垃圾；基于"标记-清除"算法(不进行压缩操作，产生内存碎片)；以获取最短回收停顿时间为目标；并发收集、低停顿；

CMS收集器有3个明显的缺点：1.对CPU资源非常敏感、2.无法处理浮动垃圾，可能出现"Concurrent Mode Failure"失败、3.产生大量内存碎片
垃圾收集线程与用户线程（基本上）可以同时工作

7.G1 收集器：分代收集器。当今收集器技术发展最前沿成果之一，是一款面向服务端应用的垃圾收集器。G1可以说是CMS的终极改进版，解决了CMS内存碎片、更多的内存空间登问题。虽然流程与CMS比较相似，但底层的原理已是完全不同。
特点：
能充分利用多CPU、多核环境下的硬件优势；可以并行来缩短(Stop The World)停顿时间；也可以并发让垃圾收集与用户程序同时进行；
分代收集，收集范围包括新生代和老年代。能独立管理整个GC堆（新生代和老年代），而不需要与其他收集器搭配；能够采用不同方式处理不同时期的对象；
应用场景可以面向服务端应用，针对具有大内存、多处理器的机器；采用标记-整理 + 复制算法来回收垃圾

JVM配置参数：
一、堆内存相关配置
1.最大堆内存（Max Heap Size）
参数：-Xmx<size> 或 -XX:MaxHeapSize=<size>
说明：设置JVM可以使用的最大堆内存大小，防止程序占用过多的系统资源。

2.初始堆内存（Initial Heap Size）
参数：-Xms<size> 或 -XX:InitialHeapSize=<size>
说明：设置JVM启动时的初始堆内存大小，减少内存波动。

3.新生代（Young Generation）大小
参数：-Xmn<size> 或 -XX:NewSize=<size>、-XX:MaxNewSize=<size>
说明：设置新生代的大小，影响对象在年轻代和老年代之间的分配和GC行为。

4.Eden区和Survivor区比例
参数：-XX:SurvivorRatio=<ratio>
说明：设置新生代中Eden区与Survivor区的比例，如-XX:SurvivorRatio=8表示Eden区与每个Survivor区的比例为8:1。

5.年轻代与老年代比例
参数：-XX:NewRatio=<ratio>
说明：设置年轻代与老年代的比例，如-XX:NewRatio=4表示年轻代与老年代的比例为1:4。

二、垃圾收集器（Garbage Collector）配置
1.串行垃圾收集器（Serial GC）
开启：-XX:+UseSerialGC
关闭：-XX:-UseSerialGC
说明：适用于单核处理器或小型应用。

2.并行垃圾收集器（Parallel GC）
年轻代：-XX:+UseParallelGC（JDK 8及之前版本）
老年代：-XX:+UseParallelOldGC
线程数：-XX:ParallelGCThreads=<number>
最大暂停时间：-XX:MaxGCPauseMillis=<time>
自适应大小调整：-XX:+UseAdaptiveSizePolicy
并发标记清除（CMS）垃圾收集器（已废弃）
开启：-XX:+UseConcMarkSweepGC
触发阈值：-XX:CMSInitiatingOccupancyFraction=<percentage>
压缩：-XX:+UseCMSCompactAtFullCollection

3.G1垃圾收集器
开启：-XX:+UseG1GC
关闭：-XX:-UseG1GC
G1是面向服务端的垃圾收集器，旨在满足高吞吐量同时保证低延迟。

三、线程栈内存配置
线程栈大小
参数：-Xss<size> 或 -XX:ThreadStackSize=<size>
说明：设置每个线程的堆栈大小，过大可能导致线程创建减少，过小可能导致栈溢出。

四、其他常用参数
异常处理
内存溢出时导出堆信息：-XX:+HeapDumpOnOutOfMemoryError
设置导出路径：-XX:HeapDumpPath=<path>

日志配置
GC日志路径：-Xloggc:<file-path>
详细GC日志：-XX:+PrintGCDetails
GC时间戳：-XX:+PrintGCTimeStamps
暂停时间：-XX:+PrintGCApplicationStoppedTime
性能调优
禁用显式GC：-XX:+DisableExplicitGC
偏向锁：-XX:+UseBiasedLocking

五、示例配置
对于一个高并发的Web应用，可能需要如下配置：
-server  
-Xmx4G  
-Xms4G  
-Xmn1.5G  
-Xss256k  
-XX:SurvivorRatio=8  
-XX:+UseG1GC  
-XX:MaxGCPauseMillis=200  
-XX:+HeapDumpOnOutOfMemoryError  
-XX:HeapDumpPath=/var/log/java_heapdump.hprof  
-Xloggc:/var/log/gc.log  
-XX:+PrintGCDetails  
-XX:+PrintGCTimeStamps  
-XX:+UseBiasedLocking
这个配置设置了最大和初始堆内存为4GB，新生代为1.5GB，每个线程的堆栈大小为256KB，使用G1垃圾收集器，并设置了详细的GC日志和内存溢出时的堆导出路径。



Java监控工具：Java监控工具在Java应用程序的开发、部署和维护过程中起着至关重要的作用。它们提供了一系列的功能和特性，帮助开发人员、系统管理员和运维团队更好地理解和管理Java应用程序的性能、健康状况和资源使用情况。

jconsole：
jconsole 是 Java 平台中自带的一款轻量级的 Java 虚拟机（JVM）监视和管理控制台工具。它提供了一个图形化的用户界面，允许用户连接到正在运行的本地或远程 JVM，并监控和管理这些 JVM 的运行状况。

主要功能：
实时监控：jconsole 可以实时监控 JVM 的内存使用情况、线程活动、类加载、垃圾收集等关键指标。
内存管理：通过 jconsole，用户可以查看堆内存（Heap Memory）的使用情况，包括新生代（Young Generation）、老年代（Old Generation）和永久代（Permanent Generation，在 Java 8 之后是元空间 Metaspace）的详细数据。
线程管理：jconsole 可以显示 JVM 中所有活动线程的列表，包括线程状态（如 RUNNABLE、BLOCKED、WAITING 等）。此外，还可以检测线程死锁和长时间运行的线程。
类加载器：jconsole 可以显示已加载的类的数量，以及加载这些类的类加载器的信息。
垃圾收集：通过 jconsole，用户可以查看 JVM 使用的垃圾收集器的类型，以及垃圾收集活动的统计数据，如垃圾收集的次数、耗时等。
MBean 管理：jconsole 支持 Java Management Extensions（JMX）技术，可以连接到运行 JMX 代理的 JVM，并查看和管理 MBean（Managed Bean）的属性、方法和通知。
插件支持：虽然 jconsole 本身提供了丰富的功能，但它还支持插件机制，允许用户扩展其功能。
远程监控：jconsole 不仅可以监控本地 JVM，还可以通过网络连接到远程 JVM 进行监控。

使用方法：
打开命令行或终端，输入 jconsole 命令启动 jconsole 工具。
在 jconsole 的主界面上，选择“远程进程”或“本地进程”选项卡，连接到要监控的 JVM。
一旦连接成功，jconsole 将显示所选 JVM 的详细信息，并提供各种监控和管理功能。


jvisualvm：
jvisualvm（Java VisualVM）是一个用于Java虚拟机（JVM）的可视化工具，它提供了广泛的监控和分析功能，旨在帮助开发人员诊断和解决Java应用程序的性能问题。

一、主要功能
实时监控：jvisualvm可以实时监控Java应用程序的性能指标，如CPU使用率、内存使用率、线程活动等。
性能分析：
CPU分析器：显示CPU时间轴，帮助识别CPU消耗最高的方法和线程。
内存分析器：检测内存泄漏、分析对象的内存使用情况和引用关系，帮助优化内存使用。
线程分析器：提供线程转储和线程活动的详细信息，用于分析线程的执行情况。
故障排查：支持对运行中的应用程序进行故障排查，包括对应用程序的性能问题、内存溢出、线程问题等进行诊断和分析。
快照和报告：
生成CPU和内存快照，用于记录应用程序的状态和性能。
生成线程转储和堆转储，以便进行后续调试和分析。
导出报告和快照，以便与团队成员共享和进一步分析。
远程监控：支持通过Java Management Extensions（JMX）远程监控Java应用程序，可以连接到远程虚拟机，并监控其性能和分析数据。

二、特点
可视化界面：jvisualvm提供了一个直观的图形用户界面，使得监控和分析Java应用程序变得更加简单和直观。
集成多个工具：jvisualvm将多个监视、故障排除和性能分析实用程序组合到一个工具中，使得开发人员无需在多个工具之间切换。
插件和扩展：jvisualvm支持插件和扩展，可以通过安装额外的插件来增强其功能，满足特定的监控和分析需求。

三、使用方法
启动jvisualvm：通过命令行输入jvisualvm命令或在图形界面中找到并双击jvisualvm.exe文件启动jvisualvm。
连接JVM：在jvisualvm的主界面中，选择“本地”或“远程”选项卡，连接到需要监控的Java虚拟机（JVM）。
监控和分析：一旦连接到JVM，jvisualvm将显示有关该JVM的详细信息，包括CPU、内存、线程和类的使用情况。开发人员可以使用jvisualvm的各种功能和工具来监控和分析Java应用程序的性能和故障。

四、注意事项
安全性：在使用远程监控功能时，需要确保连接的安全性，避免未经授权的访问和数据泄露。
性能影响：长时间运行jvisualvm可能会对JVM的性能产生一定的影响，因此建议在必要时才进行监控和分析。

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

J2EE应用程序开发技术：
J2EE（Java 2 Platform, Enterprise Edition）是一个为企业级应用提供的一套标准规范，用于开发和部署可扩展、可移植、可靠且安全的服务器端Java应用程序。
J2EE是Java平台的一部分，特别针对企业级应用进行了优化和扩展。

Java Web是J2EE的重要应用之一。
Web前端技术：
在线帮助文档 https://www.w3school.com.cn/
HTML：(主要用于网页主体结构的搭建)
HTML 指的是超文本标记语言: HyperText Markup Language
HTML（HyperText Markup Language），全称超文本标记语言，是一种用于创建网页的标准标记语言。
它通过一系列标签将网络上的文档格式统一，使得分散的Internet资源能够连接成一个逻辑整体。
这些标签不仅可以描述文字，还可以描述图形、动画、声音、表格、链接等网页元素。

HTML文本由HTML命令组成，这些命令用于说明网页中的各种元素。一个HTML文档通常包括头部（Head）和主体（Body）两大部分。
头部描述浏览器所需的信息，如文档的标题、字符集、样式表链接等；主体则包含所要说明的具体内容，如段落、列表、图片、链接等。

HTML标签结构：
双标签：例<p></p>
单标签: 例</p>
属性: 例<a href = "http://www.xxx.com"/> 中，href = "http://www.xxx.com"是属性，href是属性名，"http://www.xxx.com"是属性值
文本：例<p>段落</p> 中，双标签中的文字
元素：开始标签+属性+文本 +结束标签，称之为一个元素

在Java Web中，.html文件通常放在服务器上。当用户通过浏览器访问Web应用时，服务器会响应请求并发送相应的.html文件给客户端（即用户的浏览器）。浏览器接收并解析这些.html文件，然后展示给用户。

HTML常用标签：
<!DOCTYPE html> 声明为 HTML5 文档
<html> 元素是 HTML 页面的根元素
<head> 元素包含了文档的元（meta）数据，如 <meta charset="utf-8"> 定义网页编码格式为 utf-8（由于在大部分浏览器中直接输出中文会出现乱码，所以要在头部将字符声明为UTF-8）
<title> 元素描述了文档的标题
<body> 元素包含了可见的页面内容
<h1> 元素定义一个大标题
<p> 元素定义一个段落

问：HTML语言比Java、python等语言语法都不严格，是因为浏览器的编译强大吗？
答：HTML语言（HyperText Markup Language，超文本标记语言）与Java、Python等编程语言在设计和用途上存在本质的区别，这导致了它们在语法严格性上的差异。
因此，不能简单地将HTML的语法不严格归因于浏览器的编译强大。

首先，HTML是一种标记语言，主要用于创建网页和网站结构。它的语法相对简单和直观，旨在允许开发人员通过标记（tags）来定义网页内容的结构和样式。
HTML的主要目的是让内容在浏览器中正确地显示，而不是执行复杂的逻辑或计算任务。

相比之下，Java和Python是编程语言，它们具有更复杂的语法和更强大的功能。这些语言允许开发人员编写复杂的程序，
包括算法、数据处理、系统操作等。因此，它们的语法需要更加严格和精确，以确保程序的正确性和可靠性。

浏览器的编译能力确实强大，能够解析和渲染HTML、CSS和JavaScript等前端技术。
然而，浏览器的编译过程主要是将HTML和CSS等标记语言转换为可视化的网页内容，而不是执行像Java或Python那样的复杂程序逻辑。

此外，HTML的语法不严格也与其设计初衷和用途有关。HTML的目标是使内容易于阅读和编写，
同时允许开发者通过简单的标记来控制内容的呈现方式。这种简单性和直观性使得HTML成为非专业开发人员也能轻松上手的工具。

HTML语言比Java、Python等语言语法不严格的原因在于它们的设计目的和用途不同，而不是因为浏览器的编译强大。
每种语言都有其适用的场景和优势，选择使用哪种语言取决于具体的开发需求和目标。

CSS：（主要用于页面元素美化）Cascading Style Sheet 层叠样式表
是一组样式设置的规则，用于控制页面的外观样式

CSS语法：
<head>
	<style>
		选择器{
			属性名：属性值;
			属性名：属性值;
		}
	</style>
</head>

CSS的引入方式：
方式1 行内式
>通过元素的style属性引入样式
>语法:style="样式名:样式值;样式名:样式值;... ..
>缺点:1 代码复用度低 不利于维护
     2 css样式代码和htm1结构代码交织在一起,影响阅读,影响文件大小,影响性能
方式2内嵌式
>通过在head标签中的style标签定义本页面的公共样式
>通过选择器确定样式的作用元素
方式3外部样式表
>将css代码单独放入一个.css文件中,哪个html需要这些代码就在head中通过link标签引入
<link href="路径名" rel="stylesheet">

CSS选择器：
    1 元素选择器 根据标签的名字确定样式的作用元素
        语法:标签名1
        缺点:某些同名的元素不希望使用某些样式,某些不同名的元素也使用该样式,都无法协调
    2 id选择器   根据标签的id值确定样式的作用元素。
                一般每个元素都有id属性,但是在一个页面中,id的值不应该相同,id具有唯一性
        语法: #id值{}
        缺点: id值有唯一性,样式只能作用到一个元素上
    3 class选择器 根据元素的class属性值确定样式的作用元素
        元素的class属性值可以重复 而且一个元素的class属性可以有多个值
        语法: .class属性值{}


    <!--元素选择器-->
    <style>
            input{
            width: 60px;
            height: 40px;
            background-color:rgb(166,232,66);
            color:white;
            font-size:22px;
            font-family:'隶书';
            border:2px solid green;
            border-radius:5px;
        }
    </style>
    
    <!--id选择器-->
    <style>    
    #btn4{
        width: 60px;
        height: 40px;
        background-color:rgb(166,232,66);
        color:white;
        font-size:22px;
        font-family:'隶书';
        border:2px solid green;
        border-radius:5px;
        }
    </style>

    <!--class选择器-->
    <style>
        .shapeClass{
            width: 80px;
            height: 40px;
            border-radius:5px;
        }


        .colorClass{
            background-color:rgb(179，241，85);
            color:white;
            border: 3px solid green;
        }


        .fontclass{
            font-size: 20px;
            font-family:'隶书';
            line-height: 30px;
        }
    </style>

CSS的定位方式：
position:
    static 默认
    absolute 绝对
    relative 相对 相对元素原本的位置
    fixed 相对 相对浏览器窗口

CSS的盒子模型：
CSS的盒子模型是CSS布局的基础，它规定了元素框处理元素内容、内边距、边框和外边距的方式。每个HTML元素都可以看作是由内容、内边距（padding）、边框（border）和外边距（margin）组成的矩形盒子。

内容（Content）：这是盒子模型的中心，它包含了元素的文本、图片或其他媒体内容。内容的大小可以通过 width 和 height 属性来设置。

内边距（Padding）：内边距是内容与边框之间的空间。它位于内容区域的周围，是透明的，不会占用实际的空间，但会影响元素的总大小。
内边距的大小可以通过 padding 属性来设置，例如 padding-top、padding-right、padding-bottom 和 padding-left。

边框（Border）：边框是围绕在内边距和内容周围的线。它的大小和样式可以通过 border 属性来设置。
例如，border-width 设置边框的宽度，border-style 设置边框的样式（如实线、虚线等），border-color 设置边框的颜色。

外边距（Margin）：外边距是边框与其他元素之间的空间。它位于边框的外部，是透明的，不会占用实际的空间，但会影响元素在布局中的位置。
外边距的大小可以通过 margin 属性来设置，例如 margin-top、margin-right、margin-bottom 和 margin-left。

盒子模型的总宽度和总高度可以通过以下公式计算：
总宽度 = 左外边距 + 左边框 + 左内边距 + 内容宽度 + 右内边距 + 右边框 + 右外边距
总高度 = 上外边距 + 上边框 + 上内边距 + 内容高度 + 下内边距 + 下边框 + 下外边距

其中，padding和margin不限定padding/margin - xx 时有默认值，例如margin 10px 20 px 30 px 40 px这四个值按照顺时针方向（上、右、下、左）指定了元素的外边距。
这种简写方式使得你可以在一个属性中一次性设置四个方向的外边距，提高了编写CSS的效率。

JavaScript：（主要用于页面元素的动态处理）JavaScript诞生于1995年，它的出现主要是用于处理网页中的前端验证。
所谓的前端验证，就是指检查用户输入的内容是否符合一定的规则。比如：用户名的长度，密码的长度，邮箱的格式等。

    <!--方式一-->
    <script>
        function suprise(){
        //弹窗提示
        alert("hello·我是惊喜")
        }
    </script>

    <!--方式二，外部引入-->
    <script src="JS/buttom.js" type="text/javascript"></script>

注意:
·一个htm1中可以有多个script标签
·一对script标签不能在引入外部is文件的同时定义内部脚本
·script标签如果用于引入外部is文件,中间最好不要有任何字符。包括空格和换行。
·script标签写成双标签，不要写成单标签。
·引用JS函数的时候要加上括号

JS的引用类型是弱引用类型，变量在声明时不指定类型，赋值时才确定类型。声明变量使用var。
例：var i = 10

JS中使用var声明变量的特点:
1.弱类型变量,可以统一声明成var
2.var声明的变量可以再次声明
3.变量可以使用不同的数据类型多次赋值
4.Js的语句可以以;结尾,也可以不用;结尾
5.变量标识符严格区分大小写
6.标识符的命名规则参照JAVA
7.如果使用了 一个没有声明的变量,那么运行时会报uncaught ReferenceError:***is not defined
8.如果一个变量只声明,没赋值,那么值是undefined

JS中foreach的语法：
for(var index in var){
    console.log(index)
}

函数声明的语法
1 function 函数名(){}
2 var 函数名=function(){}

和java相比有如下特点
1 没有访问修饰符
2 没有返回值类型也没有void 如果有值要返回,则直接return即可
3 没有异常列表
4 调用方法时,实参和形参可以在数量上不一致,在方法内部可以通过 arguments获得调用时的实参
5 函数也可以作为参数传递给另一个方法

JS创建对象：
person =var"name":“李四”,
age":10,
eat":function(food){console.log(this.age+"岁"+this.name+"正在吃"+food)}
// 访问属性
console.log(person.name
console.log(person.age)
// 调用方法
person.eat("火锅”)

JS事件：
在JavaScript中，事件是用户或浏览器本身的某种行为，通常是由用户对页面的某些动作引起的，如单击某个链接或按钮、在文本框中输入文本、按下键盘上的某个按键、移动鼠标等。
当这些事件发生时，可以使用JavaScript中的事件处理程序（也称为事件监听器）来检测并执行特定的代码。

以下是一些常见的JavaScript事件：

点击事件：
onclick: 当用户点击某个元素时触发。
ondblclick: 当用户双击某个元素时触发。

焦点事件：
onblur: 当元素失去焦点时触发（例如，用户点击了页面上的另一个元素）。
onfocus: 当元素获得焦点时触发（例如，用户点击了输入框）。

加载事件：
onload: 当页面或某个资源（如图像）完成加载时触发。

鼠标事件：
onmousedown: 当鼠标按钮被按下时触发。
onmouseup: 当鼠标按钮被释放时触发。
onmousemove: 当鼠标在元素内部移动时触发。
onmouseover: 当鼠标指针移入某个元素上方时触发。
onmouseout: 当鼠标指针移出某个元素时触发。

键盘事件：
onkeydown: 当某个键盘按键被按下时触发。
onkeyup: 当某个键盘按键被释放时触发。
onkeypress: 当某个键盘按键被按下并释放时触发。

选择和改变事件：
onchange: 当表单元素（如<input>、<textarea>或<select>）的内容发生变化时触发。
onselect: 当文本被用户选中时触发。

表单事件：
onsubmit: 当表单的提交按钮被点击，或用户尝试提交表单时触发。
onreset: 当表单的重置按钮被点击时触发，或者表单通过代码重置时触发。

拖拽事件：
ondragstart: 当用户开始拖拽某个元素时触发。
ondragover: 当被拖拽的元素在目标元素上移动时触发。
ondrop: 当拖拽的元素在目标位置放下时触发。

触摸事件（针对移动设备）：
ontouchstart: 当用户开始触摸某个元素时触发。
ontouchmove: 当用户在某个元素上移动手指时触发。
ontouchend: 当用户停止触摸某个元素时触发。

滚动事件：
onscroll: 当用户滚动页面或某个元素时触发。

窗口大小变化事件：
onresize: 当浏览器窗口的大小变化时触发。

这只是JavaScript中事件的一小部分，实际上还有更多的事件类型，它们可以用于实现各种交互效果和响应用户动作。在编写JavaScript代码时，可以根据需要监听和处理这些事件。

BOM：
BOM编程涉及到浏览器对象模型（Browser Object Model）的概念，它使JavaScript有能力与浏览器进行交互。
BOM提供了一系列独立于内容的对象，允许开发者通过JavaScript操作浏览器窗口和框架等浏览器相关的对象。
BOM的核心对象是window，它是浏览器的顶级对象，具有双重角色：一方面，它是JS访问浏览器窗口的一个接口；
另一方面，它是一个全局对象，定义在全局作用域中的变量、函数都会变成window对象的属性和方法。

BOM（Browser Object Model，浏览器对象模型）的Windows API 提供了一系列函数和方法，用于操作和控制浏览器窗口及其组件。以下是一些常用的BOM Windows API：

window.alert()：显示一个带有指定消息和OK按钮的警告框。
window.confirm()：显示一个带有指定消息以及确定和取消按钮的对话框，并返回用户的选择（true或false）。
window.prompt()：显示一个对话框，提示用户输入一些文本（可带一个默认值），并返回用户输入的文本。
window.open()：打开一个新的浏览器窗口或查找一个已命名的窗口，并返回代表该窗口的Window对象。
window.close()：关闭当前窗口。
window.moveTo() 和 window.resizeTo()：移动和改变当前窗口的位置和大小。
window.scrollTo() 和 window.scrollBy()：滚动当前窗口到指定位置或滚动特定距离。
window.focus() 和 window.blur()：将焦点设置到当前窗口或使当前窗口失去焦点。
window.innerWidth 和 window.innerHeight：获取浏览器窗口的视口（viewport）宽度和高度（不包括工具栏和滚动条）。
window.location：包含有关当前URL的信息，并提供了一些方法和属性，用于获取或设置URL，以及重新加载页面。

DOM概述：
D0M(Document obiect Model)编程就是使用document对象的API完成对网页HTML文档进行动态修改,以实现,网页数据和样式动态变化效果的编程。

通过DOM对象模型，JavaScript 获得创建动态 HTML ：

JavaScript 能改变页面中的所有 HTML 元素
JavaScript 能改变页面中的所有 HTML 属性
JavaScript 能改变页面中的所有 CSS 样式
JavaScript 能删除已有的 HTML 元素和属性
JavaScript 能添加新的 HTML 元素和属性
JavaScript 能对页面中所有已有的 HTML 事件作出反应
JavaScript 能在页面中创建新的 HTML 事件
换言之：HTML DOM 是关于如何获取、更改、添加或删除 HTML 元素的标准。

DOM编程获取元素：
<div id="div01">
<input type="text" class="a" id="username name= aaa"/>
<input type="text" class="b" id="password" name="aaa"/>
<input type="text" class="a" id="email"/>
<input type="text" class="b" id="address"/>
</div>

<input type="text" class="a"/><br>
<input type="button" value="根据id获取指定元素" onclick="fun1()" id="btn01"/>
<input type="button" value="根据标签名获取多个元素"onclick="fun2()" id="btn02"/>
<input type="button" value="根据name属性值获取多个元素"onclick="fun3()"id="btn03"/>
<input type="button" value="根据class属性值获得多个元素" onclick="fun4()" id="btn04"/>
<input type="button" value="根据父元素获取子元素" onclick="fun5()" id="btn05"/>
<input type="button" value="根据子元素获取父元素" onclick="fun6()" id="btn06"/>
<input type="button" value="根据子元素获取兄弟元素" onclick="fun7()" id="btn07"/>

function fun1(){
//1 获得document
//2 通过document获得元素
var el1 =document.getElementById("username")//根据元素的id值获取页面上唯一的一个元素
console.log(el1)
}
function fun2(){
    var els =document.getElementsByTagName("input")//根据元素的标签名获取多个同名元素
    for(var i=0 ;i<els.length;i++){
        console.log(els[i])
    }
}

function fun3(){
    var els =document.getElementsByName("aaa")// 根据元素的name属性值获得多个元素
    console.log(els)
    for(var i =0;i< els.length;i++){
        console.log(els[i])
    }
}

function fun4(){
    var els =document.getElementsByClassName("a")//根据元素的class属性值获得多个元素
    for(var i =0;i< els.length;i++){
        console.log(els[i])
    }
}

function fun5(){
    // 先获取父元素
    var div01= document.getElementById("div01")// 获取所有子元素
    var cs=div01.children // 通过父元素获取全部的子元素
    for(var i =0;i< cs.length;i++){
    console.log(cs[i])
    }
    console.log(div01.firstElementChild)// 通过父元素获取第一个子元素
    console.log(div01.lastElementchild)// 通过父元素获取最后一个子元素
}

function fun6(){
    // 获取子元素
    var pinput =document.getElementById("password")
    console.log(pinput.parentElement)//通过子元素获取父元素
}

function fun7(){
    // 获取子元素
    var pinput =document.getElementById("password")
    console.log(pinput.previousElementsibling)// 获取前面的第一个元素
    console.log(pinput.nextElementsibling)//获取后面的第一个元素
}

Json：
JavaScript Object Notation（JavaScript 对象标记法），它是一种存储和交换数据的语法。
当数据在浏览器与服务器之间进行交换时，这些数据只能是文本，JSON 属于文本并且我们能够把任何 JavaScript 对象转换为 Json，然后将 Json 发送到服务器。
我们也能把从服务器接收到的任何 Json 转换为 JavaScript 对象。以这样的方式，我们能够把数据作为 JavaScript 对象来处理，无需复杂的解析和转译。

Json语法：
在json中，每一个数据项，都是由一个键值对（或者说是名值对）组成的，但是键必须是字符串，
且由双引号包围，而值必须是以下数据类型之一：

字符串（在 Json 中，字符串值必须由双引号编写）
数字
对象（Json 对象）
数组
布尔
null

数组（Array）用方括号(“[]”)表示。对象也可以是一个数组。
例：[1,2,"three","four",true,false,null,[1,2],{"name":"兮动人"}]

对象（0bject）用大括号(“{}”)表示。
例：{"name": "兮动人","age":22}

名称/值对(name/value）组合成数组和对象。
>名称(name）置于双引号中，值（value）有字符串、数值、布尔值、null、对象和数组。
>并列的数据之间用逗号(“,”）分隔

Josn 的值不可以是以下数据类型之一：
函数
日期
undefined

Json字符串转换为JS对象：
JSON.parse()

JS对象转换为JSON字符串：
JSON.stringify()

Json与Java对象转换-Json转换库：
GSON:
GSON是Google提供的用于在Java对象和Json数据之间进行相互转换的库，其包含的Json解析方法主要有：
toJson()：将Java对象转化为Json字符串。
fromJson()：将JSON字符串转化为Java对象。

将Java对象转化为Json字符串：
Gson g = new Gson();
book b = new book("100","金苹果","种植苹果的故事");
String s = g.toJson(b);

将Json字符串转化为Java对象：
Gson g = new Gson();
//2. 转换 {"id":"100","name":"金苹果","info":"种植苹果的故事","page":["远赴人间惊鸿晏","一度人间盛世颜","致hdd"]}
//2.1  返回book类型
book b = g.fromJson("{\"id\":\"100\",\"name\":\"金苹果\",\"info\":\"种植苹果的故事\"}", book.class);
System.out.println(b.getId());

//2.2 返回MAP类型，键值对形式
HashMap hm = g.fromJson("{\"id\":\"100\",\"name\":\"金苹果\",\"info\":\"种植苹果的故事\"}", HashMap.class);
System.out.println(hm.get("id"));

//2.3 MAP类型中值是一个数组形式
HashMap data = g.fromJson("{\"id\":\"100\",\"name\":\"金苹果\",\"info\":\"种植苹果的故事\",\"page\":[\"远赴人间惊鸿晏\",\"一度人间盛世颜\",\"致hdd\"]}", HashMap.class);
List list = (List) data.get("page");
System.out.println(list.get(1));

FastJson：
FastJson是阿里巴巴公司基于Java语言开发的高性能且功能完善的Json操作类库。
Fastjson可以解析Json格式的字符串，支持将Java Bean序列化为Json字符串，也可以从Json字符串反序列化到JavaBean。


将Java对象转化为Json字符串：
book b = new book("1002","水浒传","讲述了一群讲义气的莽夫");
String s = JSON.toJSONString(b);


将Json字符串转化为Java对象：

book b = new book("1002","水浒传","讲述了一群讲义气的莽夫");

//1. 转换 {"id":"1002","info":"讲述了一群讲义气的莽夫","name":"水浒传"}   转换成对象
book books = JSON.parseObject("{\"id\":\"1002\",\"info\":\"讲述了一群讲义气的莽夫\",\"name\":\"水浒传\"}", book.class);
System.out.println(books.getName());

//2. 转换成数组 ["人生本该自由","乘兴而去","尽兴而归"]
List<String> strings = JSON.parseArray("[\"人生本该自由\",\"乘兴而去\",\"尽兴而归\"]", String.class);
for(String s:strings){
    System.out.println(s);
}

Json与JS对象之间的互相转换：
Json字符串转换为JS对象：
<script>
	var str = '{"name": "兮动人","age":22}';
	var obj = JSON.parse(str);
	console.log(obj);
</script>


JS对象转换为Json字符串：
<script>
	var str = '{"name": "兮动人","age":22}';
	var obj = JSON.parse(str);
	console.log(obj);
        
	var jsonstr = JSON.stringify(obj);
	console.log(jsonstr);
</script>


AJAX(Asynchronous JavaScript and XML)：
传统的web交互是用户触发一个http请求服务器，然后服务器收到之后，在做出响应到用户，并且返回一个新的页面，每当服务器处理客户端提交的请求时，客户都只能空闲等待，并且哪怕只是一次很小的交互、只需从服务器端得到很简单的一个数据，都要返回一个完整的HTML页，而用户每次都要浪费时间和带宽去重新读取整个页面。
这个做法浪费了许多带宽，由于每次应用的交互都需要向服务器发送请求，应用的响应时间就依赖于服务器的响应时间，这导致了用户界面的响应比本地应用慢得多。
AJAX 的出现,刚好解决了传统方法的缺陷，AJAX 是一种用于创建快速动态网页的技术，通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新，这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。

AJAX的XMLHttpRequest对象：
AJAX 的核心是 XMLHttpRequest 对象。 所有现代浏览器都支持 XMLHttpRequest 对象。
XMLHttpRequest 对象用于幕后同服务器交换数据，这意味着可以更新网页的部分，而不需要重新加载整个页面。
所有现代浏览器（Chrom、IE7+、Firefox、Safari 以及 Opera）都有内建的 XMLHttpRequest 对象。

创建 XMLHttpRequest 的语法是：
variable = new XMLHttpRequest();

为了应对所有浏览器，包括 IE5 和 IE6，请检查浏览器是否支持 XMLHttpRequest 对象。
如果支持，创建 XMLHttpRequest 对象，如果不支持，则创建 ActiveX 对象：
var xhttp;
if (window.XMLHttpRequest) {
    xhttp = new XMLHttpRequest();
} else {
    // code for IE6, IE5
    xhttp = new ActiveXObject("Microsoft.XMLHTTP");
}

AJAX的请求整合：
JSON：
[
  {"name":"孙悟空","age":18,"gender":"男"},
  {"name":"猪八戒","age":19,"gender":"男"},
  {"name":"唐僧","age":20,"gender":"男"},
  {"name":"沙和尚","age":21,"gender":"男"}
]

index.html:
var Ajax = {
    get: function (url, fn) {
        var xhr = new XMLHttpRequest();
        xhr.open('GET', url, true);
        xhr.onreadystatechange = function () {
            if (xhr.readyState == 4 && xhr.status == 200 || xhr.status == 304) {
                fn.call(this, xhr.responseText);
            }
        };
        xhr.send();
    },
    post: function (url, data, fn) {
        var xhr = new XMLHttpRequest();
        xhr.open("POST", url, true);
        xhr.setRequestHeader("Content-Type", "application/x-www-form-urlencoded");
        xhr.onreadystatechange = function () {
            if (xhr.readyState == 4 && (xhr.status == 200 || xhr.status == 304)) {
                fn.call(this, xhr.responseText);
            }
        };
        xhr.send(data);
    }
};

// 演示GET请求
Ajax.get("users.json", function (response) {
    console.log(response);
});

// 演示POST请求
Ajax.post("users.json", "", function (response) {
    console.log(response);
});

Cookie:
Cookie 是一些数据，存储于你电脑上的文本文件中，当 web 服务器向浏览器发送 web 页面时，在连接关闭后，服务端不会记录用户的信息，Cookie 的作用就是用于解决 “如何记录客户端的用户信息”：
当用户访问 web 页面时，它的名字可以记录在 cookie 中。在用户下一次访问该页面时，可以在 cookie 中读取用户访问记录。

一个的Cookie创建，添加了过期时间和路径：
document.cookie = "username=zhangsan; expires=Thu, 18 Dec 2043 12:00:00 GMT; path=/";

当我们请求某个URL路径时，浏览器会根据这个URL路径将符合条件的Cookie放在Request请求头中传回给服务端，服务端通过request.getCookies()来获取所有Cookie。
Cookie是HTTP头中的一个字段，虽然HTTP本身对这个字段并没有多少限制，但是Cookie最终函数存储在浏览器里，所以不同的浏览器对Cookie的存储都有一些限制。


Session：
Session是在服务器端存储的用于跟踪用户状态的机制。

Session和Cookie的区别如下：

存储位置不同：Cookie保存在客户端浏览器中，而Session保存在服务器上。
安全性不同：由于Cookie存储在客户端，所以安全性相对较低，例如用户可以编辑或伪造Cookie信息。而Session存储在服务器上，相对较安全。
有效期不同：Cookie的有效期一般为会话期间，当关闭浏览器窗口后，Cookie就会消失。而Session一般会在一定时间内保存在服务器上，当访问增多，会占用服务器性能。


使用Session可以解决表单重复提交问题，其基本思想是在服务器端使用Session来记录用户提交表单的状态。
当用户提交表单时，将表单的提交状态存储在Session中，同时将表单的提交按钮禁用，防止用户多次点击提交按钮。
当表单处理完成后，将Session中的表单提交状态清除，并启用提交按钮，以便用户下一次提交表单。

下面是一个使用Session解决表单重复提交问题的示例代码：

<%@ page language="java" contentType="text/html; charset=UTF-8"
    pageEncoding="UTF-8"%>
<%@ page import="java.util.*" %>
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Form Submit</title>
</head>
<body>
<%
  // 初始化Session中的表单提交状态为false
  if(session.getAttribute("submitStatus") == null) {
    session.setAttribute("submitStatus", false);
  }

  // 如果表单已经提交过，则显示相应的提示信息并禁用提交按钮
  if(session.getAttribute("submitStatus") == true) {
    out.println("表单已经提交过了，请等待处理完成！");
    // 将提交按钮禁用
    request.setAttribute("submitButton", "disabled");
  } else {
    // 如果表单未提交过，则处理表单数据并启用提交按钮
    String name = request.getParameter("name");
    String email = request.getParameter("email");
    // 处理表单数据，例如插入到数据库中
    // ...

    // 将表单提交状态存储在Session中并启用提交按钮
    session.setAttribute("submitStatus", true);
    // 将提交按钮启用
    request.setAttribute("submitButton", "");
  }
%>
<form action="" method="post">
  <label for="name">姓名：</label><br>
  <input type="text" name="name"><br>
  <label for="email">邮箱：</label><br>
  <input type="text" name="email"><br>
  <!-- 根据需要添加其他表单项 -->
  <input type="submit" value="提交" name="submitButton"
      style="disabled:;<?php echo $submitButton; ?>;">
</form>
</body>
</html>
在该示例代码中，当用户第一次提交表单时，将表单的提交状态存储在Session中，同时将表单的提交按钮禁用。当表单处理完成后，将Session中的表单提交状态清除，并启用提交按钮。这样就可以防止用户多次点击提交按钮，解决表单重复提交问题。

在会话管理中，域对象是指可以在不同的组件（如Servlet、JSP等）之间进行数据传递与共享的对象。这些对象在Web应用程序中起到了至关重要的作用，使得数据能够在不同的请求、响应以及页面之间流通。根据不同的作用范围，域对象可以分为以下几种：

请求域对象（Request Scope）：
生命周期：始于客户端的请求到达服务器，终于服务器对这次请求做出响应并返回给客户端。
作用范围：只在一个请求的生命周期内有效。
用途：主要用于存储与单个请求相关的数据，如从表单中获取的用户输入。

会话域对象（Session Scope）：
生命周期：始于用户第一次访问应用程序并创建会话，终于用户关闭浏览器或会话超时。
作用范围：跨多个请求有效，只要用户会话还在，数据就保持可用。
用途：常用于存储用户的个人信息、购物车内容等需要在多个页面间共享的数据。

应用程序域对象（Application Scope）：
生命周期：始于Web应用程序启动，终于Web应用程序关闭。
作用范围：在整个Web应用程序中都有效，所有用户共享这些数据。
用途：用于存储整个应用程序级别的信息，如配置参数、全局计数器等。

页面域对象（Page Scope）：
生命周期：始于JSP页面被请求，终于JSP页面执行完毕并将响应发送回客户端。
作用范围：仅在当前JSP页面中有效。
用途：主要用于存储与单个JSP页面相关的数据。


如何从请求域（Request Scope）、会话域（Session Scope）、应用程序域（Application Scope）和页面域（Page Scope）中获取数据的示例：

请求域对象（Request Scope）
在Servlet中设置属性到请求域：

protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {  
    String data = "This is request scope data";  
    request.setAttribute("requestData", data);  
    // 转发请求到JSP页面  
    RequestDispatcher dispatcher = request.getRequestDispatcher("result.jsp");  
    dispatcher.forward(request, response);  
}

在JSP页面中获取请求域中的数据：


<%@ page contentType="text/html;charset=UTF-8" language="java" %>  
<html>  
<body>  
    <p>Request Scope Data: ${requestScope.requestData}</p>  
</body>  
</html>

或者使用脚本表达式：

<p>Request Scope Data: <%= request.getAttribute("requestData") %></p>

会话域对象（Session Scope）
在Servlet中设置属性到会话域：


protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {  
    String data = "This is session scope data";  
    HttpSession session = request.getSession();  
    session.setAttribute("sessionData", data);  
    // 转发请求到JSP页面  
    RequestDispatcher dispatcher = request.getRequestDispatcher("result.jsp");  
    dispatcher.forward(request, response);  
}
在JSP页面中获取会话域中的数据：


<p>Session Scope Data: ${sessionScope.sessionData}</p>

或者使用脚本表达式：

<p>Session Scope Data: <%= session.getAttribute("sessionData") %></p>

应用程序域对象（Application Scope）

在Servlet中设置属性到应用程序域：

protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {  
    String data = "This is application scope data";  
    ServletContext context = request.getServletContext();  
    context.setAttribute("applicationData", data);  
    // 转发请求到JSP页面  
    RequestDispatcher dispatcher = request.getRequestDispatcher("result.jsp");  
    dispatcher.forward(request, response);  
}

在JSP页面中获取应用程序域中的数据：

<p>Application Scope Data: ${applicationScope.applicationData}</p>

或者使用脚本表达式：
<p>Application Scope Data: <%= getServletContext().getAttribute("applicationData") %></p>
页面域对象（Page Scope）

页面域实际上是在JSP页面中定义的局部变量，这些变量在JSP页面的生命周期内有效。它们不是通过Servlet设置的，而是在JSP页面中直接定义的。例如：

<%  
    String pageData = "This is page scope data";  
%>  
<p>Page Scope Data: <%= pageData %></p>
注意，页面域中的变量实际上是在JSP页面被转换成Servlet时定义的局部变量，所以它们并不是真正的“域对象”。在Servlet中，没有直接的API可以设置或获取页面域数据，因为页面域仅在JSP页面执行期间存在。


JWT:
JSON Web Tokens（JWT）是一种用于在网络应用中传递信息的开放标准（RFC 7519），它以紧凑且自包含的方式表示数据，
通常用于在身份验证和授权系统中进行安全传输信息。在前后端分离的Java Web架构中，使用JWT或其他类似的Token机制进行身份验证更为常见和推荐。

JWT的具体作用如下：
1. 身份验证：JWT可用于验证用户的身份。当用户登录成功后，服务器可以生成一个JWT令牌并将其返回给客户端。
客户端在后续请求中可以携带这个令牌，服务器可以使用令牌来验证用户的身份，而无需在每个请求中再次提供用户名和密码。

2. 授权：JWT可以包含有关用户的授权信息，例如用户的角色或权限。
这使得服务器可以在接收到JWT后，轻松地确定用户是否有权限执行特定操作。

3. 信息交换：JWT是一种方便的方式来在不同组件或微服务之间安全地传递信息。
例如，一个用户在一个身份验证服务登录后，可以生成一个JWT令牌，然后将其传递给其他服务，
这些服务可以使用令牌来验证用户身份和访问权限。

4. 状态无关性：JWT是无状态的，令牌中包含了所有必要的信息，因此不需要在服务器上保留会话状态。
这使得 JWT 适用于分布式应用程序和负载均衡环境。

5. 跨域通信：由于 JWT 可以在 HTTP 头或 URL 中传递，它们可以用于在不同域之间进行通信。
这对于单点登录（Single Sign-On，SSO）等应用程序非常有用。

6. 安全性：JWT使用数字签名或加密来验证令牌的完整性和真实性。
这意味着只有拥有正确密钥的人才能生成有效的令牌，从而防止伪造令牌。

JWT通常由三部分组成：

- Header（头部）：包含了令牌类型和所用的签名算法等信息。
- Payload（负载）：包含了要传输的数据，如用户ID、过期时间、授权信息等。
- Signature（签名）：用于验证令牌的真实性。通常由头部和负载中的信息以及密钥生成。

创建一个JWT：
package boot.utils;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.SignatureAlgorithm;
import io.jsonwebtoken.security.Keys;
import java.security.Key;
import java.util.Date;
public class JWTUtils {

    private static final String SECRET_KEY = "secret_key";

    public String JwtTokenGenerator (long userId){

            // 当前时间
            Date now = new Date();

            // 过期时间，设置为5小时后
            Date expiration = new Date(now.getTime() + 5 * 60 * 60 * 1000);

            // 创建JWT
            String jwt = Jwts.builder()
                    .setSubject(Long.toString(userId))  // 设置用户ID作为JWT的主题
                    .setIssuedAt(now)  // 设置JWT的发行时间
                    .setExpiration(expiration)  // 设置JWT的过期时间
                    .signWith(SignatureAlgorithm.HS256,SECRET_KEY)  // 使用密钥签名JWT
                    .compact();

            return jwt;

    }

    public String getUsernameFromToken(String token) {
        return Jwts.parser()
                .setSigningKey(SECRET_KEY)
                .parseClaimsJws(token)
                .getBody()
                .getSubject();
    }

    public boolean validateToken(String token, String userName) {
        String username = getUsernameFromToken(token);
        return username.equals(userName);
    }

}



Node.js:
Node.js是一个基于Chrome V8引擎的JavaScript运行环境，由Ryan Dahl在2009年5月发布。它使用了一个事件驱动、非阻塞式I/O模型，使得JavaScript能够在服务端运行，与PHP、Python、Perl、Ruby等服务端语言平起平坐。

Node.js的主要特性和优势包括：

服务器端开发：Node.js允许开发者在服务器端构建高性能的网络应用程序，如Web服务器、API服务器等。
非阻塞异步I/O：Node.js采用了非阻塞型I/O机制，这意味着在执行I/O操作时，如访问数据库，不会阻塞后续代码的执行，从而提高了程序的执行效率。
事件驱动：Node.js采用事件驱动的架构，当新的请求到来时，请求会被压入一个事件队列中，通过一个循环来检测队列中的事件状态变化，并执行相应的事件处理代码。
实时应用和数据流：由于Node.js的事件驱动特性和高效的I/O处理方式，它非常适合构建实时聊天应用、在线游戏、协作工具等需要实时数据传输和处理的应用。
统一的语言：使用Node.js，开发者可以使用JavaScript在前后端开发中分享代码，这有助于简化团队开发和维护的复杂性，提高开发效率。
强大的包管理器：Node.js附带了npm（Node Package Manager），它是全球最大的开源软件库之一，提供了大量的模块和框架，可以帮助开发者快速搭建各种类型的应用。
轻量级和高可伸缩性：Node.js的设计非常轻巧，消耗的系统资源较少，适合构建高可伸缩的应用程序。它具有良好的水平扩展性，可以通过添加更多的服务器节点来横向扩展应用。

然而，Node.js也存在一些缺点，例如不适合CPU密集型应用、只支持单核CPU、可靠性较低、开源组件库质量参差不齐、debug不方便等。


npm：
npm（Node Package Manager）是Node.js的包管理器，它允许Node.js开发者从npm仓库中安装、共享和发布软件包。npm的出现极大地简化了Node.js项目的依赖管理和分发流程。

依赖管理：
当你开发一个Node.js项目时，可能会使用到许多第三方库或框架。npm允许你通过npm install命令轻松地将这些依赖项添加到你的项目中。
npm还维护了一个package.json文件，该文件列出了项目的所有依赖项及其版本。这样，其他开发者可以通过查看该文件来快速了解项目所需的所有依赖项，并通过运行npm install命令来自动安装它们。

软件包仓库：
npm仓库是世界上最大的开源JavaScript软件包仓库之一，包含了数以万计的软件包。这些软件包涵盖了从Web开发框架、测试工具到命令行实用程序等各种领域。
开发者可以通过npm发布自己的软件包，与其他开发者共享自己的代码。

版本控制：
npm使用语义化版本控制（Semantic Versioning, SemVer）来管理软件包的版本。SemVer规定了一个版本号由主版本号、次版本号和补丁版本号三部分组成（如1.2.3），每个部分都有其特定的含义。
通过npm，你可以指定你的项目依赖于某个特定版本的软件包，或者依赖于某个版本的兼容范围。这样，即使软件包被其他开发者更新了，你的项目也不会受到意外的影响。

脚本和任务自动化：
npm不仅用于安装和管理依赖项，还可以用于运行脚本和任务。你可以在package.json文件的scripts字段中定义各种命令和脚本，并通过npm run命令来执行它们。
例如，你可以使用npm来运行测试、构建项目、启动开发服务器等任务。

社区和生态：
npm拥有庞大的用户社区和丰富的生态系统。这个社区由数百万的开发者组成，他们通过npm共享和分发自己的代码，同时也从npm仓库中获取所需的依赖项。
npm还提供了许多工具和插件，用于增强npm的功能和易用性。这些工具和插件涵盖了从代码质量检查、文档生成到持续集成等各个方面。



VUE：
Vue.js（通常简称为Vue）是一个用于构建用户界面的渐进式JavaScript框架。它旨在通过简洁的API实现响应的数据绑定和组合的视图组件。
Vue被设计为可以自底向上逐层应用，其核心库只关注视图层，不仅易于上手，也便于与第三方库或已有项目整合。

Vue的一些主要特点和概念：

响应式数据绑定：Vue通过其内置的响应式系统，可以自动追踪JavaScript对象的变化，并在数据发生变化时更新DOM。这使得开发者无需手动操作DOM来更新视图，提高了开发效率和代码的可维护性。
组件化开发：Vue允许开发者将界面拆分成独立的、可复用的组件。每个组件都包含了自己的HTML、CSS和JavaScript代码，并且可以与其他组件进行通信和交互。这种组件化的开发方式使得代码更加模块化和可维护。
模板系统：Vue使用基于HTML的模板语法来描述组件的视图结构。模板可以绑定到Vue实例的数据上，当数据发生变化时，视图会自动更新。模板系统还支持条件渲染、列表渲染和自定义指令等高级功能。
指令：Vue提供了一套内置指令（如v-if、v-for、v-bind等），用于在模板中操作DOM元素和绑定数据。这些指令可以简化常见的DOM操作，并使得模板代码更加清晰和易于理解。
插件和生态系统：Vue拥有一个活跃的插件和工具生态系统，包括路由管理、状态管理、构建工具等。这些工具和插件可以帮助开发者构建更复杂、更强大的应用程序。
轻量级和灵活性：Vue.js是一个轻量级的框架，核心库的大小相对较小，可以轻松地与其他库或现有项目集成。同时，Vue也提供了足够的灵活性，允许开发者根据项目的需求选择使用哪些功能或插件。

Vue的特点：
遵循MVVM模式
编码简洁，体积小，运行效率高，适合移动/PC端开发
它本身只关注UI，可以引入其它第三方库开发项目

与其他JS框架的关联：
借鉴 Angular 的模板和数据绑定技术
借鉴 React 的组件化和虚拟DOM技术

Vue周边库：
vue-cli：vue脚手架
vue-resource
axios
vue-router：路由
vuex：状态管理
element-ui：基于vue的UI组件库（PC端）

VUE简单实现（Vue3）：

创建一个Vue3：
<template>  
  <div>  
    <p>Count: {{ count }}</p>  
    <button @click="increment">Increment</button>  
    <button @click="decrement">Decrement</button>  
  </div>  
</template>  
  
<script>  
import { ref } from 'vue';  
  
export default {  
  name: 'Counter',  
  setup() {  
    const count = ref(0);  
  
    const increment = () => {  
      count.value++;  
    };  
  
    const decrement = () => {  
      count.value--;  
    };  
  
    return {  
      count,  
      increment,  
      decrement,  
    };  
  },  
};  
</script>  
  
<style scoped>  
/* 样式可以放在这里 */  
</style>


在 HTML 文件中使用 Vue 3：
<!DOCTYPE html>  
<html lang="en">  
<head>  
  <meta charset="UTF-8">  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">  
  <title>Vue 3 Demo</title>  
  <script src="https://unpkg.com/vue@next"></script>  
</head>  
<body>  
  <div id="app">  
    <counter></counter>  
  </div>  
  
  <script>  
    const { createApp } = Vue;  
    import Counter from './Counter.vue';  
  
    const app = createApp({});  
    app.component('counter', Counter);  
    app.mount('#app');  
  </script>  
</body>  
</html>

注意：

想让Vue工作，就必须创建一个Vue实例，且要传入一个配置对象。root容器里的代码依然符合html规范，只不过混入了一些特殊的Vue语法。
root容器里的代码被称为Vue模板。Vue实例与容器是一一对应的。真实开发中只有一个Vue实例，并且会配合着组件一起使用。
{{xxx}}中的xxx要写js表达式，且xxx可以自动读取到data中的所有属性。一旦data中的数据发生变化，那么模板中用到该数据的地方也会自动更新


对比Vue3，创建一个选项试API Vue2：
export default {  
  name: 'Counter',  
  data() {  
    return {  
      count: 0  
    };  
  },  
  methods: {  
    increment() {  
      this.count++;  
    },  
    decrement() {  
      this.count--;  
    }  
  }  
};


Web网络编程基础：
网络七层架构的作用与协议：
层		功能								协议
应用层	提供应用程序之间的通信。		TFTP，HTTP，SNMP，FTP，SMTP，DNS，Telnet
表示层	处理数据格式，数据加密和压缩等。	没有协议
会话层	建立、管理、终止两主机的会话。		没有协议
传输层	建立主机的端到端连接。			TCP，UDP
网络层	路径选择。						ICMP，RIP，OSPF，BGP，IGMP，IP
数据链路层	负责两个相邻结点之间的数据传输。		SLIP，CSLIP，PPP，ARP，RARP，MTU
物理层	使原始的数据比特流能在物理媒介上传输。	ISO2110，IEEE802，IEEE802.2


网络数据传输的过程可以被划分为应用层、传输层、网络层、链路层和物理层这五个层次，每个层次都有其特定的功能和协议。
网络数据传输的过程，按照这五个层次来描述：

1. 物理层（Physical Layer）：
   - 物理层是网络协议的最底层，负责传输比特流，即0和1的电信号，通过物理媒体（如电缆、光纤或无线信号）进行传输。
   - 在这一层，数据以二进制形式从发送方的物理介质传输到接收方。

2. 链路层（Data Link Layer）：
   - 链路层负责将物理层传输的比特流划分为帧（Frame），并添加地址信息，以便确定帧的源和目标。
   - 这一层还包括错误检测和纠正，以确保数据的完整性和可靠性。
   - 在局域网中，以太网协议是链路层的典型代表。

3. 网络层（Network Layer）：
   - 网络层负责将帧从源主机传输到目标主机，跨越不同的网络。
   - 它使用路由协议来确定数据包的路径，使数据能够通过多个路由器（Router）进行中转。
   - IP（Internet Protocol）是网络层的核心协议，它定义了数据包的格式和寻址机制。

4. 传输层（Transport Layer）：
   - 传输层负责端到端的通信，确保数据在源和目标之间可靠地传输。
   - 常见的传输层协议包括TCP（Transmission Control Protocol）和UDP（User Datagram Protocol）。
   - TCP提供可靠的、面向连接的通信，包括错误检测、重传机制等，而UDP提供无连接的、不可靠的通信，更适合实时应用。

5. 应用层（Application Layer）：
   - 应用层是最高层，它包含了各种应用程序，如Web浏览器、电子邮件客户端、文件传输工具等。
   - 在这一层，数据被组织成消息、请求或响应，并与特定的应用程序协议相关联，如HTTP、SMTP、FTP等。
   - 应用程序通过传输层与目标应用程序通信，完成数据的交换和处理。

总的来说，网络数据传输的过程从物理层开始，逐层传递，最终到达应用层。每一层都具有不同的功能和责任，协同工作以确保数据在源和目标之间安全、可靠地传输。这个分层模型有助于实现网络的模块化、可扩展性和互操作性。


一些网络编程概念：
IP：唯一的标识 Internet上的计算机（通信实体）
IP分类：IPV4和IPV6；万维网和局域网
域名：由于IP地址具有不方便记忆并且不能显示地址组织的名称和性质等缺点，人们设计出了域名，并通过网域名称系统（DNS，Domain Name System）来将域名和IP地址相互映射，
使人更方便地访问互联网，而不用去记住能够被机器直接读取的IP地址数串
本地回路地址：127.0.0.1 对应：localhost
在Java中使用InetAddress类代表IP，实例化InetAddress的两个方法：
getByName(String host) getLocalHost()
两个常用方法：getHostName() / getHostAddress()
端口号：正在计算机上运行的进程
  要求：不同的进程有不同的端口号
  范围：被规定为一个16位的整数0-65535
端口号与IP地址的组合得出一个网络套接字:Socket

TCP和UDP协议是TCP/IP协议的核心。 TCP 传输协议：TCP 协议是一TCP (Transmission Control Protocol)和UDP(User Datagram Protocol)协议属于传输层协议。
其中TCP提供IP环境下的数据可靠传输，它提供的服务包括数据流传送、可靠性、有效流控、全双工操作和多路复用。通过面向连接、端到端和可靠的数据包发送。
通俗说，它是事先为所发送的数据开辟出连接好的通道，然后再进行数据发送；而UDP则不为IP提供可靠性、流控或差错恢复功能。
一般来说，TCP对应的是可靠性要求高的应用，而UDP对应的则是可靠性要求低、传输经济的应用。

网络编程之实现InetAddress实例化：InetAddress 是 Java 中表示 IP 地址的类。你可以使用它来获取本地主机或远程主机的 IP 地址，解析主机名等。
public class InetAddressTest {
    public static void main(String[] args) {
 
        InetAddress inet1 = null;
        try {
            inet1 = InetAddress.getByName("192.168.10.14");
            System.out.println(inet1);
 
            InetAddress inet2 = InetAddress.getByName("www.baidu.com");
            System.out.println(inet2);
 
            InetAddress inet3 = InetAddress.getByName("127.0.0.1");
            System.out.println(inet3);
 
            //获取本地ip
            InetAddress inet4 = InetAddress.getLocalHost();
            System.out.println(inet4);
 
            //getHostName()
            System.out.println(inet2.getHostName());
            //getHostAddress()
            System.out.println(inet2.getHostAddress());
        } catch (UnknownHostException e) {
            e.printStackTrace();
        }
 
    }
}

网络编程之Socket编程：
我们可以发现Socket就在应用程序的传输层和应用层之间，设计了一个Socket抽象层，传输层的底一层的服务提供给Socket抽象层，Socket抽象层再提供给应用层。
套接字使用TCP提供了两台计算机之间的通信机制。 客户端程序创建一个套接字，并尝试连接服务器的套接字。
当连接建立时，服务器会创建一个 Socket 对象。客户端和服务器现在可以通过对 Socket 对象的写入和读取来进行通信。
java.net.Socket 类代表一个套接字，并且 java.net.ServerSocket 类为服务器程序提供了一种来监听客户端，并与他们建立连接的机制。
IP和端口号通过Socket传输。

服务器端：服务器端需要创建一个ServerSocket来监听特定的端口，并接受客户端连接。接受连接后，通过Socket与客户端进行通信。
import java.io.*;
import java.net.*;

public class SocketServer {
    public static void main(String[] args) {
        int port = 5000; // 服务器监听的端口号
        try (ServerSocket serverSocket = new ServerSocket(port)) {
            System.out.println("Server is listening on port " + port);
            while (true) {
                // 监听并接受客户端连接
                Socket socket = serverSocket.accept();
                System.out.println("New client connected");

                // 创建新的线程处理客户端连接
                new ServerThread(socket).start();
            }
        } catch (IOException ex) {
            System.out.println("Server exception: " + ex.getMessage());
            ex.printStackTrace();
        }
    }
}

class ServerThread extends Thread {
    private Socket socket;

    public ServerThread(Socket socket) {
        this.socket = socket;
    }

    public void run() {
        try (InputStream input = socket.getInputStream();
             BufferedReader reader = new BufferedReader(new InputStreamReader(input));
             OutputStream output = socket.getOutputStream();
             PrintWriter writer = new PrintWriter(output, true)) {

            String text;
            // 从客户端接收消息并回应
            while ((text = reader.readLine()) != null) {
                System.out.println("Message from client: " + text);
                writer.println("Server: " + text);
            }
        } catch (IOException ex) {
            System.out.println("Server exception: " + ex.getMessage());
            ex.printStackTrace();
        }
    }
}
ServerSocket 用于监听指定端口上的连接请求。
accept() 方法阻塞，直到接收到客户端连接。
每次接收到客户端连接后，启动一个新的线程 ServerThread 来处理该连接，以便服务器可以继续监听新的连接请求。
在 ServerThread 中，通过输入输出流与客户端进行通信。


客户端代码：客户端需要创建一个Socket并连接到服务器的IP地址和端口。通过Socket进行输入输出流的读写操作与服务器通信。
import java.io.*;
import java.net.*;

public class SocketClient {
    public static void main(String[] args) {
        String hostname = "localhost";
        int port = 5000;

        try (Socket socket = new Socket(hostname, port)) {
            OutputStream output = socket.getOutputStream();
            PrintWriter writer = new PrintWriter(output, true);

            InputStream input = socket.getInputStream();
            BufferedReader reader = new BufferedReader(new InputStreamReader(input));

            // 发送消息到服务器
            writer.println("Hello, Server");

            // 从服务器接收消息
            String response = reader.readLine();
            System.out.println("Message from server: " + response);
        } catch (UnknownHostException ex) {
            System.out.println("Server not found: " + ex.getMessage());
        } catch (IOException ex) {
            System.out.println("I/O error: " + ex.getMessage());
        }
    }
}
Socket 用于连接服务器的指定IP和端口。
通过输出流发送消息到服务器，通过输入流接收服务器的响应。


TCP/UDP:
两台计算机之间使用套接字建立TCP连接时的步骤：

1.服务器实例化一个 ServerSocket 对象，表示通过服务器上的端口通信。
2.服务器调用 ServerSocket 类的 accept() 方法，该方法将一直等待，直到客户端连接到服务器上给定的端口。
3.服务器正在等待时，一个客户端实例化一个 Socket 对象，指定服务器名称和端口号来请求连接。
4.Socket 类的构造函数试图将客户端连接到指定的服务器和端口号。如果通信被建立，则在客户端创建一个 Socket 对象能够与服务器进行通信。
5.在服务器端，accept() 方法返回服务器上一个新的 Socket 引用，该 Socket 连接到客户端的 Socket。

Java实现TCP网络程序设计：
服务器端类：
package dreamfly.net.server;

//读取客户端发来的数据，给客户端写出数据

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.ServerSocket;
import java.net.Socket;

public class Server {

    public static void main(String[] args) throws IOException {

        //1.开启服务器，参数是指开着的端口号
        ServerSocket server = new ServerSocket(8899);
        System.out.println("服务器已成功启动");
        //2.接收客户端发来的连接
        Socket socket = server.accept();
        System.out.println("接收到客户端发来的请求");
        //3.获取读取流
        InputStream in = socket.getInputStream();
        //4.读取数据
//        int data = in.read();      //默认返回的是整数
        for (int i = 0;i < 5;i++){
            char data = (char)in.read();
            System.out.print(data);
        }
        //5.给客户端写出数据
        System.out.println();
        OutputStream out = socket.getOutputStream();
        out.write("world".getBytes());
        System.out.println("服务器端成功发送数据");
        out.close();
    }
}

客户端类：
package dreamfly.net.client;

//读取服务器端发来的数据，给服务器端写出数据

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.Socket;

public class Client {

    public static void main(String[] args) throws IOException {
        //1.连接到指定的服务器(ip+port)
        Socket socket = new Socket("127.0.0.1",8899);
        System.out.println("已连接成功");
        //2.获取写出流
        OutputStream out = socket.getOutputStream();
        //3.写出数据,字节流只能写出整数或字节数组
        //将hello对应整数编程对应的字节数组，getBytes()将String转换为byte[]
        out.write("hello".getBytes());
        System.out.println("客户端成功发送数据");
        InputStream in = socket.getInputStream();
        for (int i = 0;i < 5;i++){
            char data = (char)in.read();
            System.out.print(data);
        }
        System.out.println();
        System.out.println("成功接收服务器端数据");
        out.close();
    }
}



Java实现UDP编程：
服务器端
DatagramSocket serverSocket = new DatagramSocket(1234)；//设置监听端口，可以和TCP端口重复，及一个应用程序用TCP占用端口1234，另一个程序可以用UDP占用端口1234
byte[] buff = new byte[1024];

客户端
DatagramPacket packet = new DatagramPacket(buff, buff.length);//设置接受长度为buff.leng的buff数据

serverSocket.receive(packet)；//收取UDP数据包

String word = new String( packet.getData(), packet.getOffset(), packet.getLength(),StandardCharsets.UTF_8 );//将收到的数据按UTF-8转换为String

System.out.println("已经收到"+word);//在服务器做出提示
                byte[] resultbuff = word.getBytes();
                packet.setData(resultbuff);
                serverSocket.send(packet);//发送数据给客户端做出回应


实现UDP的时候如何确保数据不丢失？
在使用UDP协议进行数据传输时，数据包可能会丢失，因为UDP是一个不可靠的协议。为了确保数据不丢失，可以在应用层实现一些机制来增强可靠性。以下是一些常见的技术：

确认应答机制（ACK）：每次发送数据包后，接收方发送一个确认应答（ACK）回给发送方。如果发送方在一定时间内没有收到ACK，则重新发送该数据包。
超时和重传：发送方在发送数据包后启动一个定时器，如果在定时器超时之前没有收到ACK，发送方会重传该数据包。
序列号：为每个数据包分配一个序列号。接收方可以根据序列号检测丢失的数据包，并要求发送方重传。
滑动窗口协议：允许发送方在等待ACK时继续发送多个数据包，这样可以提高传输效率。接收方通过滑动窗口确认已经接收的数据包。

实现代码：
服务器端代码：
import java.net.DatagramPacket;
import java.net.DatagramSocket;
import java.net.InetAddress;
import java.nio.charset.StandardCharsets;
import java.util.HashSet;
import java.util.Set;

public class ReliableUdpServer {
    private static final int PORT = 5000;

    public static void main(String[] args) {
        try (DatagramSocket socket = new DatagramSocket(PORT)) {
            byte[] buffer = new byte[1024];
            Set<Integer> receivedSequences = new HashSet<>();

            while (true) {
                DatagramPacket packet = new DatagramPacket(buffer, buffer.length);
                socket.receive(packet);

                String message = new String(packet.getData(), 0, packet.getLength(), StandardCharsets.UTF_8);
                String[] parts = message.split(":", 2);
                int sequenceNumber = Integer.parseInt(parts[0]);
                String data = parts[1];

                System.out.println("Received packet: " + sequenceNumber + ", Data: " + data);

                if (!receivedSequences.contains(sequenceNumber)) {
                    receivedSequences.add(sequenceNumber);
                    // Process the data
                }

                // Send ACK
                String ackMessage = "ACK:" + sequenceNumber;
                byte[] ackBytes = ackMessage.getBytes(StandardCharsets.UTF_8);
                DatagramPacket ackPacket = new DatagramPacket(ackBytes, ackBytes.length, packet.getAddress(), packet.getPort());
                socket.send(ackPacket);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

使用DatagramSocket接收数据包。
将接收到的数据包的序列号存储在receivedSequences集合中，避免处理重复数据包。
发送确认应答（ACK）回给客户端。

客户端代码：
import java.net.DatagramPacket;
import java.net.DatagramSocket;
import java.net.InetAddress;
import java.nio.charset.StandardCharsets;

public class ReliableUdpClient {
    private static final String SERVER_ADDRESS = "localhost";
    private static final int SERVER_PORT = 5000;
    private static final int TIMEOUT = 1000; // 1 second

    public static void main(String[] args) {
        try (DatagramSocket socket = new DatagramSocket()) {
            socket.setSoTimeout(TIMEOUT);

            InetAddress address = InetAddress.getByName(SERVER_ADDRESS);
            int sequenceNumber = 0;

            String data = "Hello, Server!";
            String message = sequenceNumber + ":" + data;
            byte[] buffer = message.getBytes(StandardCharsets.UTF_8);
            DatagramPacket packet = new DatagramPacket(buffer, buffer.length, address, SERVER_PORT);

            boolean acknowledged = false;
            while (!acknowledged) {
                socket.send(packet);
                System.out.println("Sent packet: " + sequenceNumber);

                try {
                    byte[] ackBuffer = new byte[1024];
                    DatagramPacket ackPacket = new DatagramPacket(ackBuffer, ackBuffer.length);
                    socket.receive(ackPacket);

                    String ackMessage = new String(ackPacket.getData(), 0, ackPacket.getLength(), StandardCharsets.UTF_8);
                    if (ackMessage.equals("ACK:" + sequenceNumber)) {
                        acknowledged = true;
                        System.out.println("Received ACK for packet: " + sequenceNumber);
                    }
                } catch (Exception e) {
                    System.out.println("ACK not received, resending packet: " + sequenceNumber);
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

使用DatagramSocket发送数据包。
等待服务器的ACK响应，如果超时未收到ACK，则重传数据包。


URL：在WWW上，每一信息资源都有统一的且在网上的地址，该地址就叫URL（Uniform Resource Locator,统一资源定位器），它是WWW的统一资源定位标志，就是指网络地址。
URL格式：协议 主机名 端口号 资源地址

Java实现URL网络编程：
public class UrlTest02 {

    public static void main(String[] args) throws Exception {

        HttpURLConnection urlConnection = null;
        InputStream is = null;
        FileOutputStream fos = null;
        try {
            URL url = new URL("http://localhost:8080/examples/beauty.jpg");

            urlConnection = (HttpURLConnection) url.openConnection();

            urlConnection.connect();

            is = urlConnection.getInputStream();
            fos = new FileOutputStream("day10\\beauty3.jpg");

            byte[] buffer = new byte[1024];
            int len;
            while((len = is.read(buffer)) != -1){
                fos.write(buffer,0,len);
            }

            System.out.println("下载完成");
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            //关闭资源
            if(is != null){
                try {
                    is.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if(fos != null){
                try {
                    fos.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if(urlConnection != null){
                urlConnection.disconnect();
            }
        }
    }
}

以上是TCP/IP协议，多用于C/S架构中客户端与服务器端之间的连接。



HTTP协议：
关于C/S架构和B/S架构：
1.C/S架构即客户端/服务器架构，指的是需要安装客户端应用程序才能进行操作的架构模式；
2.而B/S架构即浏览器/服务器架构，指的是无需安装客户端应用程序，通过浏览器进行操作的模式。

>在B/S架构中，用户通过浏览器访问Web服务器上的网页，数据传输使用HTTP或HTTPS协议进行。
HTTPS协议是一种安全的HTTP协议，通过使用SSL/TLS证书来对数据进行加密，以保证数据传输的安全性和完整性。

>在C/S架构中，客户端和服务器之间通过TCP/IP协议进行通信。TCP/IP协议是一种传输层协议，用于在局域网中进行通信，
它规定了如何在网络中传输数据，并且能够保证数据的可靠性、顺序性和完整性。

总结：B/S架构多使用HTTP或HTTPS协议，而C/S架构则更倾向于使用TCP/IP协议。
但是，这并不是绝对的，也取决于具体的应用和需求。
例如，在某些需要保证数据安全性和完整性的应用场景中，C/S架构也可能会使用HTTPS协议。

PS：在JavaEE开发中，几乎全都是B/S架构的开发。系统标准的三层架构在B/S中广泛运用，这种架构包括表现层、业务层、持久层。
PS: 微信小程序的开发用的是C/S架构。


什么是HTTP协议？
HTTP协议（HyperText Transfer Protocol）是一种网络通信协议，它允许将超文本标记语言（HTML）文档从Web服务器传送到客户端的浏览器。
HTTP协议的默认端口是80，它定义了Web客户端和Web服务器之间的通信规则，确保了数据的安全和正确传输。
HTTP协议是无状态协议，即服务器不会保留之前客户端请求的任何信息。
每个HTTP请求都是独立的，服务器不会记住之前的请求和响应，因此每个请求都需要包含所有必要的信息，包括身份验证、Cookie等。


什么是HTTPS协议？
HTTPS协议是超文本传输安全协议的英文翻译缩写，是以安全为目标的HTTP通道，下加入了SSL层。
HTTPS协议是一个抽象标识符体系，句法类同http：体系，用于安全的HTTP数据传输，
https：URL表明它使用了HTTP，但HTTPS存在不同于HTTP的默认端口及一个加密身份验证层。

HTTPS的加密方式是什么？
一、对称加密
定义：对称加密，也称为私钥加密或共享密钥加密，是密码学中的一类加密算法。在这种加密方式中，加密和解密使用的是同一个密钥。
特点：
加密和解密速度快，适合加密大量数据。
安全性依赖于密钥的保密性，密钥在网络中传输时存在被截获的风险。
常见算法：DES、3DES、AES、Blowfish、IDEA、RC5、RC6等。

二、非对称加密
定义：非对称加密，也称为公开密钥加密，使用一对密钥：公钥和私钥。公钥可以公开，私钥保密。公钥加密的数据只能用私钥解密，反之亦然。
特点：
安全性高，因为即使公钥被截获，也无法推算出私钥。
加密和解密速度慢，不适合加密大量数据。
常见算法：RSA、Elgamal、背包算法、Rabin等。其中，RSA算法应用最为广泛。

HTTP协议的特点和基本工作原理：
1.HTTP协议支持客户/服务器模式。
2.HTTP协议简单快速：客户向服务器请求服务时，只需传送请求方法和路径，请求方法常用的有GET、HEAD、POST，
由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。
3.HTTP协议灵活：HTTP允许传输任意类型的数据对象，传输的类型由Content-Type加以标记。
4.HTTP协议无连接：无连接的含义是限制每次连接只处理一个请求，服务器处理完客户的请求，
并收到客户的应答后，即断开连接，采用这种方式可以节省传输时间。

HTTP请求报文格式：
HTTP请求报文格式是HTTP协议中用于从客户端向服务器发送请求的一种标准化格式。一个完整的HTTP请求报文通常由请求行、请求头部和请求体（可选）三部分组成。

请求行：
请求行包含HTTP请求方法、请求的URL和HTTP协议版本。HTTP请求方法常见的有GET、POST、PUT、DELETE等，用于指定对资源的不同操作。请求的URL指明了请求的具体目标资源。
HTTP协议版本则标识了使用的HTTP协议标准，如HTTP/1.1。

请求头部：
请求头部包含了一系列的字段，每个字段由字段名和字段值组成，字段名和字段值之间用冒号分隔。请求头部提供了关于请求的各种元信息，
如客户端的类型（User-Agent）、客户端能够处理的内容类型（Accept）、请求的源地址（Referer）、请求的目标主机（Host）等。这些元信息有助于服务器理解和处理请求。

请求体：
请求体是可选的，它包含了发送给服务器的数据。对于GET请求，通常不会有请求体，因为GET请求主要用于请求数据。而对于POST或PUT请求，请求体则包含了需要发送给服务器的数据，如表单数据、JSON数据等。
在格式上，请求行和请求头部以回车符（CR）和换行符（LF）结尾，形成CRLF序列。请求头部与请求体之间通常会有一个空行，即只包含CRLF的行，用于分隔请求头部和请求体。

下面是一个简单的HTTP GET请求报文的例子：

GET /example HTTP/1.1  
Host: www.example.com  
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36  
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9  
Accept-Encoding: gzip, deflate  
Accept-Language: en-US,en;q=0.9  
Connection: keep-alive  
  
// 此处为请求体开始的位置，对于GET请求，通常没有请求体
在这个例子中，GET /example HTTP/1.1是请求行，Host、User-Agent、Accept等是请求头部的字段，而请求体部分在这个GET请求中是缺失的。


发起HTTPS请求的过程：

1.客户端向服务器发起HTTPS请求，连接到服务器的443端口。
2.服务器端有一个密钥对，即公钥和私钥，是用来进行非对称加密使用的，服务器端保存着私钥，不能将其泄露，公钥可以发送给任何人。
3.服务器将自己的公钥发送给客户端。
4.客户端收到服务器端的证书之后，会对证书进行检查，验证其合法性，如果发现发现证书有问题，那么HTTPS传输就无法继续。
5.如果公钥合格，那么客户端会生成一个随机值，这个随机值就是用于进行对称加密的密钥，我们将该密钥称之为clientkey，即客户端密钥，这样在概念上和服务器端的密钥容易进行区分。
6.然后用服务器的公钥对客户端密钥进行非对称加密，这样客户端密钥就变成密文了。
7.HTTPS中的第一次HTTP请求结束。

HTTPS请求中的TCP三次握手过程：

第一次握手（SYN）
客户端发送SYN包：客户端向服务器发送一个带有SYN标志的TCP报文段。这个报文段不包含应用层的数据，主要目的是告诉服务器客户端想要建立连接，并包含一个初始序列号（例如，seq=j）。
进入SYN_SEND状态：客户端发送SYN包后，进入SYN_SEND状态，等待服务器的确认。

第二次握手（SYN+ACK）
服务器响应SYN+ACK包：服务器收到客户端的SYN包后，会发送一个带有SYN和ACK标志的TCP报文段作为响应。这个报文段不仅确认收到了客户端的SYN包（ACK=j+1），还包含服务器自己的初始序列号（例如，seq=k），表示服务器也愿意建立连接。
进入SYN_RECV状态：服务器发送SYN+ACK包后，进入SYN_RECV状态，等待客户端的确认。

第三次握手（ACK）
客户端发送ACK包：客户端收到服务器的SYN+ACK包后，会发送一个带有ACK标志的TCP报文段作为最终确认。这个报文段的序列号应该是服务器SYN+ACK包中确认号的值加1（ack=k+1），表示客户端已经准备好接收数据。
进入ESTABLISHED状态：这个ACK包发送完毕后，客户端和服务器都进入ESTABLISHED状态，表示TCP连接已经成功建立，双方可以开始传输数据了。

使用Java发起https请求：
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.util.EntityUtils;

public class HttpClientExample {
    public static void main(String[] args) {
        // 创建HttpClient实例
        CloseableHttpClient httpClient = HttpClients.createDefault();

        // 创建HttpGet请求
        HttpGet httpGet = new HttpGet("http://example.com");

        try {
            // 发送HTTP请求并获取响应
            CloseableHttpResponse response = httpClient.execute(httpGet);

            // 从响应中获取HTTP实体
            HttpEntity entity = response.getEntity();

            if (entity != null) {
                // 将HTTP实体内容转换为字符串并输出
                String result = EntityUtils.toString(entity);
                System.out.println(result);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            // 关闭HttpClient实例
            try {
                httpClient.close();
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
}

HTTPS中反斜杠（/）通常用于表示路径（path）的一部分。路径是指URL中主机名和查询字符串之间的部分，用于指定请求的资源位置。
其中，https中反斜杠指向的路径部分指定的是服务器的资源位置。当客户端向服务器发送请求时，服务器会根据路径部分来查找和提供相应的资源。

HTTP请求和HTTPS请求有什么区别：
HTTP请求和HTTPS请求在多个方面存在显著的区别，主要包括安全性、连接方式、端口号、资源消耗、展示方式以及费用等方面。

1. 安全性
HTTP：HTTP（HyperText Transfer Protocol）是一个简单的请求-响应协议，它本身并不对数据进行加密。因此，HTTP传输的数据是明文的，容易被中间人截获或篡改，安全性较低。
HTTPS：HTTPS（HyperText Transfer Protocol Secure）是HTTP的安全版本，它在HTTP的基础上加入了SSL/TLS（Secure Sockets Layer/Transport Layer Security）协议，对传输的数据进行加密。
这使得HTTPS能够保护数据的机密性和完整性，防止数据在传输过程中被窃听或篡改，安全性更高。

2. 连接方式
HTTP：HTTP使用明文传输，即数据在传输过程中不经过加密处理，直接以明文形式发送。
HTTPS：HTTPS则通过SSL/TLS协议对数据进行加密传输，确保数据在传输过程中的安全性。这种加密方式需要客户端和服务器之间进行SSL握手，以协商加密参数和共享密钥。

3. 端口号
HTTP：HTTP通常使用80端口进行通信。
HTTPS：HTTPS则使用443端口进行通信。这是TCP/IP协议对网络通信进行管理的一种约定。

4. 资源消耗
HTTP：由于HTTP不使用加密和解密过程，因此在数据传输过程中消耗的资源相对较少。
HTTPS：HTTPS需要对数据进行加密和解密处理，这会增加服务器的计算负担和网络带宽的消耗。因此，HTTPS在数据传输过程中会消耗更多的资源。

5. 展示方式
HTTP：由于HTTP不加密数据，一些现代浏览器可能会将HTTP站点标记为“不安全”，以提醒用户注意潜在的安全风险。
HTTPS：HTTPS站点则会被浏览器加上“绿色安全锁”标记，以表明该站点使用了加密传输技术，用户数据得到了保护。如果网站配置了增强级SSL证书，地址栏还可能变为“绿色地址栏”，进一步增加用户的信任度。

6. 费用
HTTP：HTTP是免费的，不需要为使用HTTP协议而支付额外费用。
HTTPS：HTTPS需要为网站购买和配置SSL证书，这会产生一定的费用。不过，随着网络安全意识的提高和HTTPS的普及，越来越多的网站开始使用HTTPS来保护用户数据，因此SSL证书的费用也逐渐降低。

PS：443端口通常会被运营商重点关注，原因有以下几点：
防止未备案网站的非法访问：在一些情况下，运营商会重点监听443端口，并检测握手报文中的域名信息。如果域名未备案，运营商可能会采取封禁措施，以防止非法或不安全的网站通过HTTPS协议提供服务。
遵守政策规定：在特定时期，如重要会议或活动期间，为了维护网络安全和稳定，政府或相关部门可能会要求运营商对某些端口进行临时封禁，443端口也可能包括在内。


HTTP请求中的GET请求和POST请求：
GET请求和POST请求是HTTP协议中两种基本的请求方法，它们在Web开发中扮演着重要角色。

GET请求
定义与用途：
GET请求是一个HTTP方法，通常用于从服务器请求数据。它是幂等的，即多次执行相同的GET请求应该返回相同的结果。
GET请求常用于请求一个网页或资源，如搜索、排序和筛选等操作。

特点：
数据传递方式：GET请求通过URL传递参数，这些参数附加在URL后面，以“?”分隔URL和传输数据，以“&”连接多个参数。
安全性：由于数据附加在URL中，GET请求的安全性较低，不适合传输敏感数据，因为数据会暴露在URL中，且可能被缓存和保留在历史记录中。
数据大小限制：GET请求的URL长度有限制，通常为2048个字符（具体限制可能因浏览器和服务器而异），因此不适合传输大量数据。
缓存性：GET请求可以被缓存，即浏览器可以存储响应的结果，以便在下次请求同一URL时直接使用缓存的内容。
幂等性：GET请求是幂等的，即多次执行相同的GET请求应该返回相同的结果。

POST请求
定义与用途：
POST请求是一个HTTP方法，用于向服务器提交数据或发送请求。它不是幂等的，即多次执行相同的POST请求可能会导致不同的结果，因为它通常用于修改或创建服务器上的资源。
POST请求常用于提交表单数据、发送大量数据和执行敏感操作，如用户登录、数据更新等。

特点：
数据传递方式：POST请求将数据放在请求的消息体中发送，而不是作为URL的一部分。这样，数据对于用户来说不可见，增加了安全性。
安全性：相比GET请求，POST请求的数据不会出现在URL中，因此相对更安全，适用于传输敏感数据。
数据大小限制：POST请求没有URL长度的限制，因此可以传输更多的数据量，适用于需要发送大型数据、文件上传等场景。
缓存性：POST请求通常不会被缓存，因为它们的响应结果可能会根据提交的数据而有所不同。
非幂等性：POST请求不是幂等的，多次执行相同的POST请求可能会导致不同的结果，因为它可能会创建新的资源或更新现有资源。



HTTP请求响应格式：
HTTP请求响应格式是HTTP协议中服务器对客户端请求进行回应的一种标准化格式。一个完整的HTTP响应报文通常由状态行、响应头部和响应体三部分组成。

状态行：
状态行包含了HTTP协议版本号、状态码和状态描述。HTTP协议版本号用于标识所使用的HTTP协议标准，如HTTP/1.1。状态码是一个三位数字，用于表示请求的处理结果，
如200表示请求成功，404表示资源未找到等。状态描述是对状态码的简短文字描述，用于提供关于响应状态的额外信息。

响应头部：
响应头部包含了一系列的字段，每个字段由字段名和字段值组成，字段名和字段值之间用冒号分隔。
响应头部提供了关于响应的各种元信息，如内容类型（Content-Type）、内容长度（Content-Length）、服务器类型（Server）等。这些元信息有助于客户端理解和处理响应内容。

响应体：
响应体是服务器返回给客户端的实际数据内容。对于成功的请求，响应体通常包含了请求的资源；对于错误请求，响应体可能包含错误信息或描述。响应体的格式和内容取决于请求的资源类型和服务器的配置。
在格式上，状态行、响应头部和响应体之间都以回车符（CR）和换行符（LF）结尾，形成CRLF序列。响应头部与响应体之间通常会有一个空行，即只包含CRLF的行，用于分隔响应头部和响应体。

下面是一个简单的HTTP响应报文的例子：

HTTP/1.1 200 OK  
Content-Type: text/html; charset=UTF-8  
Content-Length: 1234  
Server: Apache/2.4.41  
  
<!DOCTYPE html>  
<html>  
<head>  
    <title>Example Page</title>  
</head>  
<body>  
    <h1>Welcome to the Example Page!</h1>  
    <!-- 页面内容 -->  
</body>  
</html>
在这个例子中，HTTP/1.1 200 OK是状态行，Content-Type、Content-Length和Server是响应头部的字段，而响应体则包含了HTML页面的实际内容。


设置在后端程序中统一返回HTTP响应对象：

1.创建Http枚举类
public enum HttpResponseStatus {  
    OK(200, "OK"),  
    CREATED(201, "Created"),  
    ACCEPTED(202, "Accepted"),  
    NO_CONTENT(204, "No Content"),  
    BAD_REQUEST(400, "Bad Request"),  
    UNAUTHORIZED(401, "Unauthorized"),  
    FORBIDDEN(403, "Forbidden"),  
    NOT_FOUND(404, "Not Found"),  
    INTERNAL_SERVER_ERROR(500, "Internal Server Error");  
  
    private final int statusCode;  
    private final String statusMessage;  
  
    HttpResponseStatus(int statusCode, String statusMessage) {  
        this.statusCode = statusCode;  
        this.statusMessage = statusMessage;  
    }  
  
    public int getStatusCode() {  
        return statusCode;  
    }  
  
    public String getStatusMessage() {  
        return statusMessage;  
    }  
  
    // 根据状态码获取枚举项  
    public static HttpResponseStatus getStatusByCode(int statusCode) {  
        for (HttpResponseStatus status : HttpResponseStatus.values()) {  
            if (status.getStatusCode() == statusCode) {  
                return status;  
            }  
        }  
        return null;  
    }  
}

2.创建返回对象
public class ResultData<T> {  
    private int code; // HTTP状态码  
    private String message; // 状态消息或错误信息  
    private T data; // 返回的数据  
    // 可以根据需要添加其他字段，如额外信息、时间戳等  
  
    private ResultData(int code, String message, T data) {  
        this.code = code;  
        this.message = message;  
        this.data = data;  
    }  
  
    // 静态方法，用于创建成功的响应对象  
    public static <T> ResultData<T> success(T data) {  
        return new ResultData<>(HttpResponseStatus.OK.getStatusCode(), HttpResponseStatus.OK.getStatusMessage(), data);  
    }  
  
    // 静态方法，用于创建失败的响应对象  
    public static <T> ResultData<T> fail(HttpResponseStatus status, String errorMessage) {  
        return new ResultData<>(status.getStatusCode(), errorMessage, null);  
    }  
  
    // 静态方法，根据状态码创建失败的响应对象  
    public static <T> ResultData<T> failByCode(int statusCode, String errorMessage) {  
        HttpResponseStatus status = HttpResponseStatus.getStatusByCode(statusCode);  
        if (status != null) {  
            return new ResultData<>(statusCode, status.getStatusMessage() + ": " + errorMessage, null);  
        } else {  
            return new ResultData<>(statusCode, "Unknown Error: " + errorMessage, null);  
        }  
    }  
  
    // Getter方法，用于获取响应的各个字段  
    public int getCode() {  
        return code;  
    }  
  
    public String getMessage() {  
        return message;  
    }  
  
    public T getData() {  
        return data;  
    }  
  
    // toString方法，用于调试或日志记录  
    @Override  
    public String toString() {  
        return "ResultData{" +  
                "code=" + code +  
                ", message='" + message + '\'' +  
                ", data=" + data +  
                '}';  
    }  
}


CDN：
什么是CDN？
CDN工作机制是通过在现有的Internet中增加一层新的网站架构，将网站的内容发布到最接近用户的网络“边缘”，使用户可以就近取得所需内容，提高用户访问网站的响应速度。
CDN以缓存网站中的静态数据为主，如CSS、JS、图片和静态页面等数据。用户从主站服务器请求到动态内容后，再从CDN上下载静态数据，从而加速网页数据内容的下载数据，如淘宝有90%以上的数据都是由CDN来提供。
DNS服务器根据用户IP地址，将域名解析成相应节点的缓存服务器IP地址，实现用户就近访问。使用CDN服务的网站，只需将其域名解析权交给CDN的GSLB设备，将需要分发的内容注入CDN，就可以实现内容加速了。

关于网络负载均衡：
什么是负载均衡？
负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。
负载均衡（Load Balance）其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。

关于不同的负载均衡：
阐述一下分布式系统负载均衡、集群负载均衡、操作系统负载均衡、链路负载均衡的不同点。
分布式系统负载均衡、集群负载均衡、操作系统负载均衡和链路负载均衡是不同领域和层次的负载均衡技术，它们在应用场景、范围和实现方式上有不同点：

1. 分布式系统负载均衡：
   - 应用领域： 分布式系统负载均衡主要应用于分布式计算和分布式存储系统中，以确保各个节点上的任务或数据负载均衡。
   - 范围： 它是在分布式系统内部实现的负载均衡，通常不涉及跨越不同物理服务器的负载均衡。
   - 实现方式： 分布式系统负载均衡可以通过各种算法和策略来实现，如任务分配、数据分片等。

2. 集群负载均衡：
   - 应用领域：集群负载均衡主要用于应用程序的高可用性和性能优化，通过将多个服务器组成集群，将负载均衡在这些服务器之间。
   - 范围：集群负载均衡通常涉及跨越多台物理服务器的负载均衡，以实现应用程序的水平扩展。
   - 实现方式：集群负载均衡可以通过硬件负载均衡器、软件负载均衡器（如Nginx、HAProxy）等来实现。

3. 操作系统负载均衡：
   - 应用领域：操作系统负载均衡主要用于操作系统内核的进程和资源管理，以确保操作系统上运行的任务在CPU、内存、磁盘等资源上均衡分配。
   - 范围：它是在单个操作系统内部实现的负载均衡，用于优化操作系统的性能和资源利用率。
   - 实现方式：操作系统负载均衡通过调度算法、进程优先级、内存管理等机制来实现资源分配和任务调度。

4. 链路负载均衡：
   - 应用领域：链路负载均衡主要应用于网络层，用于分发网络流量和请求到多个网络链接或路径上，以提高网络性能和可靠性。
   - 范围：它涉及到物理网络层，以确保数据包在不同链路之间均衡分布，以避免某一链路过载或失效。
   - 实现方式：链路负载均衡可以通过路由协议、硬件负载均衡器、软件路由器等来实现。

综上所述，这些负载均衡技术在不同领域和层次中发挥作用。分布式系统负载均衡和集群负载均衡主要关注应用程序的分布和性能，操作系统负载均衡关注操作系统资源的管理，而链路负载均衡关注网络流量的分发和可靠性。
它们在实现方式、应用场景和目标方面存在差异，但都旨在提高系统的性能、可用性和资源利用率。

explain the difference between distributed system load balancing（分布式系统负载均衡）,cluster load balancing(集群负载均衡),operating system load balancing（操作系统负载均衡）,link load balancing （链路负载均衡）

Load balancing is a technique used in computer systems to distribute workloads or network traffic across multiple resources (such as servers or network links) to ensure optimal performance, 
minimize resource overload, and enhance system reliability. There are different types of load balancing, and here's an explanation of the differences between distributed system load balancing, 
cluster load balancing, operating system load balancing, and link load balancing:

Distributed System Load Balancing:

Purpose: Distributed system load balancing involves distributing workloads across multiple servers or nodes within a distributed computing environment.The goal is to improve the overall system's performance, scalability, and fault tolerance.
Use Case: It is commonly used in web applications, cloud computing, and large-scale distributed systems to evenly distribute user requests and processing tasks across multiple servers.
Techniques: Distributed load balancers often use algorithms like Round Robin, Least Connections, or Weighted Round Robin to determine how to route incoming requests to available resources.

Cluster Load Balancing:

Purpose: Cluster load balancing is a subset of distributed system load balancing. It specifically applies to clusters or groups of servers or nodes that work together as a single entity to provide services.
Use Case: Cluster load balancing is used in scenarios where high availability and fault tolerance are critical, such as in database clusters or application server clusters.
Techniques: Techniques like virtual IP addresses and load balancing algorithms are used to evenly distribute traffic among the nodes within the cluster.

Operating System Load Balancing:

Purpose: Operating system load balancing focuses on distributing the processing load among the CPU cores or processors within a single machine or server.
Use Case: It is relevant in multi-core or multi-processor systems where applications can benefit from parallelism and efficient resource utilization.
Techniques: Modern operating systems often have built-in load balancing mechanisms that distribute tasks or threads across available CPU cores based on factors like CPU utilization and affinity policies.

Link Load Balancing:

Purpose: Link load balancing (or network load balancing) involves distributing network traffic across multiple network links or paths to optimize network performance, increase bandwidth, and improve network reliability.
Use Case: It is commonly used in environments with redundant network connections, such as data centers, to ensure uninterrupted network connectivity and efficient use of available bandwidth.
Techniques: Network load balancers may use techniques like Equal-Cost Multipath (ECMP) routing or dynamic link aggregation (LACP) to balance traffic across multiple network links.

In summary, load balancing techniques are used at various levels of computing infrastructure to achieve different objectives. 
Distributed system load balancing is about distributing workloads across multiple servers or nodes in a distributed environment, while cluster load balancing specifically applies to clusters of servers. 
Operating system load balancing manages CPU resources within a single machine, and link load balancing optimizes network traffic distribution across multiple network links. 
Each type of load balancing serves a unique purpose and addresses specific performance and scalability challenges.


Tomcat:
Tomcat（Apache Tomcat）是一个流行的开源Java应用服务器，用于托管和运行Java Web应用程序。
在Tomcat中，webapps目录是专门用来存放Web应用程序的。这个目录结构允许Tomcat服务器识别并加载其中的Web应用程序，使得它们能够通过HTTP请求被访问。
在webapps目录下，可以存放各种类型的文件来构成完整的Web应用程序。

以下是可以在webapps目录中存放的一些文件类型：
静态内容：
HTML文件：构成Web页面结构的基本文件。
CSS文件：用于定义Web页面的样式。
JavaScript文件：用于添加页面交互和动态功能的客户端脚本。
图片、音频、视频等媒体文件：作为Web页面的一部分被引用，提供多媒体内容。

动态内容：
JSP文件（Java Server Pages）：允许Java代码嵌入到HTML中，用于动态生成Web页面内容。
Servlet类文件：编译后的Java类文件，用于处理请求并生成响应。Servlet通常放在WEB-INF/classes目录下，或者打包成JAR文件放在WEB-INF/lib目录下。

配置文件：
web.xml：Web应用程序的部署描述符，用于配置Servlet、过滤器、监听器以及初始化参数等。
其他应用程序特定的配置文件：如数据库连接配置文件、日志配置文件等。

库文件：
JAR文件：Web应用程序所依赖的库和框架通常放在WEB-INF/lib目录下。这些JAR文件包含了应用程序运行所需的类库和依赖项。

目录结构：
META-INF：存放与JAR文件相关的元数据，如MANIFEST.MF文件。
WEB-INF：一个特殊的目录，用于存放仅供服务器访问的资源，如classes目录（存放编译后的Java类文件）、lib目录（存放JAR库文件）以及任何应用程序内部使用的配置文件。

在将文件添加到webapps目录时，Tomcat会按照其内部机制来加载和处理这些文件。当Tomcat服务器启动时，它会扫描webapps目录并自动部署其中的Web应用程序。
每个Web应用程序通常对应webapps下的一个子目录，这个子目录的名称就是Web应用程序的上下文路径（Context Path）。


Tomcat的系统架构主要包括以下关键组件和层次：
1. Servlet容器：
   - Tomcat的核心部分是Servlet容器，它是一个Web服务器扩展，用于执行Java Servlet和JavaServer Pages（JSP）等Web组件。
   - Servlet容器负责接收HTTP请求、处理Servlet和JSP、生成HTTP响应，并将响应发送回客户端浏览器。
   - Tomcat的Servlet容器遵循Java Servlet和JSP规范，并提供了Servlet的生命周期管理、多线程支持、会话管理等功能。

2. 连接器（Connectors）：
   - 连接器是Tomcat与客户端之间的通信接口。它们负责接受HTTP请求并将其传递给Servlet容器，然后将Servlet容器的响应发送回客户端。
   - Tomcat支持多种连接器，包括HTTP连接器（用于处理HTTP请求）、AJP连接器（用于与Apache HTTP服务器集成）等。

3. Catalina容器：
   - Catalina是Tomcat的核心组件之一，负责Web应用程序的生命周期管理和请求分派。
   - Catalina包括多个子容器，如Engine、Host和Context，它们用于组织和管理Web应用程序。
   - Engine表示整个Tomcat引擎，Host表示一个虚拟主机，Context表示一个Web应用程序上下文。

4. JSP引擎：
   - Tomcat包括一个JSP引擎，用于编译和执行JavaServer Pages。当JSP页面首次被访问时，JSP引擎将其编译成Servlet，并在后续请求中执行该Servlet。
   - JSP引擎支持JSP标签、自定义标签库（Tag Libraries）等JSP特性。

5. 类加载器：
   - Tomcat使用类加载器来加载Web应用程序的Java类。每个Web应用程序都有自己的类加载器，以隔离不同应用程序之间的类。
   - Tomcat的类加载器体系结构支持共享库（shared library）和全局库（global library），使得可以在多个应用程序之间共享某些类。

6. 管理工具：
   - Tomcat提供了一组Web管理工具，用于监视和管理Tomcat服务器。这些工具包括Tomcat管理应用程序、Tomcat管理控制台、Manager应用程序等。
   - 这些工具使管理员能够查看服务器状态、部署和管理Web应用程序、查看日志、配置数据源等。

总的来说，Tomcat的系统架构是一个多层次的体系结构，其中包括Servlet容器、连接器、Catalina容器、JSP引擎、类加载器和管理工具等组件。
这些组件协同工作，使Tomcat成为一个强大的Java应用服务器，用于托管和运行Web应用程序。Tomcat的开源性质和广泛的社区支持使其成为Java Web应用程序开发和部署的流行选择。


Nginx：
NGINX是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。
NGINX是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点开发的，公开版本1.19.6发布于2020年12月15日。其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、简单的配置文件和低系统资源的消耗而闻名。

Nginx（发音为"engine-x"）是一个高性能、可伸缩、开源的反向代理服务器和Web服务器，其系统架构具有以下关键组件和特点：

1. Master Process（主进程）：
   - Nginx的系统架构以主从进程模型为基础。主进程是第一个启动的Nginx进程，它负责管理其他子进程，处理配置文件加载、平滑升级、日志记录等任务。
   - 主进程不处理实际的客户端请求，而是将请求分配给工作进程（worker process）。

2. Worker Processes（工作进程）：
   - Nginx可以配置多个工作进程，每个工作进程独立处理客户端请求。
   - 工作进程是并发处理的核心，它们可以同时处理多个客户端请求，通过事件驱动方式来提供高性能的请求处理。
   - 每个工作进程都是一个独立的进程，它们不共享内存，提高了稳定性和安全性。

3. 事件驱动模型：
   - Nginx采用了事件驱动的架构，使用高效的I/O多路复用机制，如epoll（在Linux上）或kqueue（在BSD上），以实现非阻塞的事件处理。
   - 这意味着Nginx可以在单个工作进程中同时处理多个并发连接，而无需为每个连接创建一个新的线程或进程，从而降低了资源开销。

4. 反向代理（Reverse Proxy）：
   - Nginx经常用作反向代理服务器，接收来自客户端的请求，然后将请求代理到后端的应用服务器上，如Tomcat、Node.js或Ruby on Rails。
   - 这使得Nginx能够负责负载均衡、SSL终止、请求缓存、静态文件服务等功能。

//配置Nginx
   server {
    listen 80;
    server_name example.com;  # 修改为您的域名或IP地址
    location / {
        proxy_pass http://localhost:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}

5. HTTP和HTTPS支持：
   - Nginx是一个全功能的HTTP服务器，支持HTTP/1.0、HTTP/1.1和HTTP/2协议。它能够处理HTTP请求和响应、URL重写、反向代理、gzip压缩等。
   - Nginx还支持HTTPS，可以进行SSL/TLS终止、证书管理和安全连接。

6. 模块化架构：
   - Nginx采用模块化的设计，它允许通过第三方模块来扩展功能。管理员可以根据需要加载不同的模块，以适应特定的用例和需求。
   - 核心模块处理基本功能，如HTTP请求和响应，而第三方模块可以添加缓存、安全性、负载均衡等功能。

7. 负载均衡：
   - Nginx支持多种负载均衡算法，如轮询、IP哈希、最小连接数等，以将客户端请求分发到多个后端服务器上，实现高可用性和性能优化。

   http {
    upstream backend {
        server backend1.example.com;
        server backend2.example.com;
        server backend3.example.com;
    }

    # ...其他Nginx配置...

    server {
        listen 80;

        location / {
            proxy_pass http://backend;
        }
    }
}

负载均衡后的请求将会均匀的分发到backend中列出的三个服务器中。

总的来说，Nginx的系统架构是一个高性能、事件驱动的主从进程模型，具有反向代理、负载均衡、HTTP和HTTPS支持、模块化设计等特点。
这些特性使得Nginx成为一个非常受欢迎的Web服务器和反向代理服务器，适用于各种用例，从静态文件服务到高流量的Web应用程序。 
Nginx的性能和可伸缩性使其在构建现代Web架构中广泛使用。

nginx做资源的动静分离：
配置静态资源服务器：
确定静态资源路径，如/var/www/static。
在Nginx的server块内添加location规则来匹配静态资源，并指定其本地存储路径。例如：

server {  
    listen 80;  
    server_name example.com;  
    location /static/ {  
        alias /var/www/static/;  
        expires 30d;  # 设置静态资源缓存有效期为30天  
    }  
}

这样，所有访问http://example.com/static/ 开头的URL都将映射到服务器本地的/var/www/static/目录。

配置动态内容服务器：
确保动态内容服务器（如Tomcat、Node.js服务等）运行正常，并监听在特定端口上。
在Nginx中添加另一个location规则，使用proxy_pass指令将非静态资源的请求转发至后端应用服务器。


项目中使用了nginx做反向代理，还需要网关做映射吗？
在项目中，当使用Nginx作为反向代理时，是否还需要使用网关进行映射，取决于具体的项目需求和架构设计。以下是一些考虑因素：

1.路由和映射需求：如果你的项目需要复杂的路由和映射规则，例如基于路径、请求头、请求方法等的动态路由，那么使用网关可能更加灵活和方便。
Nginx虽然也支持路由和映射，但可能不如一些专门的网关工具（如Spring Cloud Gateway）那样灵活。
2.微服务架构：如果你的项目采用微服务架构，并且希望实现服务间的路由、限流、熔断、重试等高级功能，那么使用网关可能是更好的选择。网关可以作为微服务架构中的核心组件，负责处理服务间的通信和协调。
3.安全性考虑：Nginx本身已经提供了很多安全性功能，如SSL/TLS加密、访问控制等。但是，如果你需要更高级的安全性功能，如API认证、授权、审计等，那么使用网关可能更加合适。
网关可以集成这些安全性功能，并提供更细粒度的控制。
4.性能和扩展性：Nginx是一个高性能的反向代理服务器，可以处理大量的并发请求。但是，如果你的项目需要更高的性能和扩展性，例如支持分布式部署、自动扩展等，那么使用网关可能更加合适。
网关通常可以基于云计算和容器化技术进行扩展和部署。

综上所述，是否需要使用网关进行映射取决于你的项目需求和架构设计。如果你的项目需要复杂的路由和映射规则、微服务架构、高级安全性功能或更高的性能和扩展性，那么使用网关可能是更好的选择。
否则，使用Nginx作为反向代理已经可以满足大多数项目的需求。



Netty：
Netty是一个由JBOSS提供的开源框架，用于快速开发高性能、高可靠性的网络应用程序。
它简化和流线化了网络应用的编程开发过程，基于NIO（非阻塞IO）开发，使用异步事件驱动的方式处理网络请求。

Netty的主要特点包括：
异步事件驱动：Netty基于事件驱动编程模型，通过异步方式处理网络请求，避免了阻塞IO带来的性能问题。
基于NIO：Netty基于Java的NIO（非阻塞IO）库开发，能够高效地处理大量并发连接。
简化开发：Netty提供了一套丰富的API和工具类，使得开发者可以更加专注于业务逻辑的实现，而无需深入了解底层网络协议和IO处理的细节。
可扩展性强：Netty的设计采用了模块化结构，各个模块之间耦合度低，可以根据需要灵活地扩展和定制。
社区活跃：Netty拥有庞大的开发者社区，不断有新的功能和优化被加入到框架中，为开发者提供了强有力的支持。

使用netty创建一个服务器：
import io.netty.bootstrap.ServerBootstrap;  
import io.netty.channel.ChannelFuture;  
import io.netty.channel.ChannelInitializer;  
import io.netty.channel.ChannelOption;  
import io.netty.channel.EventLoopGroup;  
import io.netty.channel.nio.NioEventLoopGroup;  
import io.netty.channel.socket.SocketChannel;  
import io.netty.channel.socket.nio.NioServerSocketChannel;  
import io.netty.handler.logging.LogLevel;  
import io.netty.handler.logging.LoggingHandler;  
  
public class ServerBootstrap {  
    public static void main(String[] args) throws Exception {  
        EventLoopGroup bossGroup = new NioEventLoopGroup(1);  
        EventLoopGroup workerGroup = new NioEventLoopGroup();  
        try {  
            ServerBootstrap bootstrap = new ServerBootstrap();  
            bootstrap.group(bossGroup, workerGroup)  
                     .channel(NioServerSocketChannel.class)  
                     .option(ChannelOption.SO_BACKLOG, 128)  
                     .childOption(ChannelOption.SO_KEEPALIVE, true)  
                     .handler(new LoggingHandler(LogLevel.INFO))  
                     .childHandler(new ChannelInitializer<SocketChannel>() {  
                         @Override  
                         public void initChannel(SocketChannel ch) throws Exception {  
                             ch.pipeline().addLast(new ServerHandler());  
                         }  
                     });  
            ChannelFuture future = bootstrap.bind(8080).sync();  
            future.channel().closeFuture().sync();  
        } finally {  
            bossGroup.shutdownGracefully();  
            workerGroup.shutdownGracefully();  
        }  
    }  
}

使用netty创建客户端：
使用netty创建客户端引导程序
import io.netty.bootstrap.Bootstrap;  
import io.netty.channel.ChannelFuture;  
import io.netty.channel.ChannelInitializer;  
import io.netty.channel.ChannelOption;  
import io.netty.channel.EventLoopGroup;  
import io.netty.channel.nio.NioEventLoopGroup;  
import io.netty.channel.socket.SocketChannel;  
import io.netty.channel.socket.nio.NioSocketChannel;  
import io.netty.handler.logging.LogLevel;  
import io.netty.handler.logging.LoggingHandler;  
  
public class ClientBootstrap {  
    public static void main(String[] args) throws Exception {  
        EventLoopGroup group = new NioEventLoopGroup();  
        try {  
            Bootstrap bootstrap = new Bootstrap();  
            bootstrap.group(group)  
                     .channel(NioSocketChannel.class)  
                     .option(ChannelOption.SO_KEEPALIVE, true)  
                     .handler(new LoggingHandler(LogLevel.INFO))  
                     .childHandler(new ChannelInitializer<SocketChannel>() {  
                         @Override  
                         public void initChannel(SocketChannel ch) throws Exception {  
                             ch.pipeline().addLast(new ClientHandler());  
                         }  
                     });  
            ChannelFuture future = bootstrap.connect("localhost", 8080).sync();  
            future.channel().closeFuture().sync();  
        } finally {  
            group.shutdownGracefully();  
        }  
    }  
}

创建客户端处理器
import io.netty.buffer.ByteBuf;  
import io.netty.channel.ChannelHandlerContext;  
import io.netty.channel.ChannelInboundHandlerAdapter;  
import io.netty.handler.codec.http.*;  
import java.nio.charset.Charset;  
  
public class ClientHandler extends ChannelInboundHandlerAdapter {  
    @Override  
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {  
        if (msg instanceof FullHttpResponse) {  
            FullHttpResponse response = (FullHttpResponse) msg;  
            System.out.println(response.content().toString(Charset.forName("UTF-8")));  
        } else {  
            super.channelRead(ctx, msg);  
        }  
    }  
    @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {   
        cause.printStackTrace();   
        ctx.close();   
    }   
}

在实际生活中有哪些因素影响网络传输？
网络传输的性能和可靠性受到许多因素的影响，这些因素可以分为以下几类：

1.带宽（Bandwidth）：带宽是网络连接的最大传输速率，通常以每秒比特数（bps）或兆比特数（Mbps）来衡量。
更高的带宽意味着更快的数据传输速度。网络链路的带宽限制了数据传输的最大速度。

2.延迟（Latency）：延迟是数据从发送端到接收端所需的时间，通常以毫秒（ms）为单位。
延迟包括传播延迟（数据在物理媒体上传播的时间）和处理延迟（路由、交换和处理数据的时间）。较低的延迟通常意味着更快的响应时间。

3.丢包率（Packet Loss）：丢包率表示在数据传输过程中丢失的数据包的百分比。丢包率可能由网络拥塞、网络错误或不稳定的连接引起。
较高的丢包率会导致数据重传和性能下降。

4.拥塞（Congestion）：网络拥塞发生在网络上的数据流量超过了网络链路的容量，导致数据包延迟、丢失和排队。
拥塞可能由网络流量突增、恶意攻击或网络配置问题引起。

5.路由路径（Routing Path）：数据包在网络中的路由路径也会影响传输性能。不同的路由路径可能具有不同的延迟和丢包率。优化的路由路径可以提高性能。

6.网络设备和中间节点（Network Devices and Intermediate Nodes）：网络中的路由器、交换机和防火墙等设备以及它们的配置也会影响传输性能。
设备的性能、带宽和负载均衡策略都会对网络传输产生影响。

7.带宽共享（Bandwidth Sharing）：在共享网络环境中，多个用户或应用程序共享带宽。
网络传输的性能可能受到其他用户或应用程序的使用情况影响，特别是在拥挤的时候。

8.网络拓扑（Network Topology）：网络拓扑指网络中的设备和连接的物理和逻辑布局。
不同的拓扑结构可能导致不同的性能特点，如星型、总线型、环型、网状型等。

9.网络协议和编码（Network Protocols and Encoding）：使用的网络协议和数据编码方式也会影响传输性能。
一些协议和编码方式可能会增加数据包头部大小或引入额外的开销。

10.网络安全（Network Security）：安全策略、防火墙、加密和身份验证等安全机制可能会引入额外的延迟和处理时间，影响网络传输性能。

11.地理位置和距离（Geographical Location and Distance）：物理距离和地理位置对网络延迟和带宽有影响。
数据传输跨越较长的地理距离可能导致较高的延迟。

12.网络流量管理（Traffic Management）：流量管理策略、QoS（Quality of Service）配置和流量限制也可以影响网络传输的性能。

综上所述，网络传输性能受到多种因素的综合影响，包括带宽、延迟、丢包率、拥塞、路由路径、网络设备、带宽共享、网络拓扑、协议、安全性等。
了解这些因素并采取相应的优化措施是确保网络传输在实际生活中高效和可靠的关键。


为什么Java比其他语言更适合开发Web应用程序？

1.跨平台性。Java由于其跨平台的特点，可以在多种操作系统上运行，这使得Java在服务器端程序中占据了主导地位。
2.面向对象。Java是一种面向对象编程语言，具有封装、继承和多态等特性，这使得Java在编程中更易于理解和维护。开源。Java拥有一个庞大的开源社区，提供了大量的开源框架和库，这使得Java在Web开发中具有更强的灵活性和可扩展性。
3.J2EE技术。J2EE是Java在Web开发中的一种重要技术，它提供了一组标准的技术和API，如Servlet、JSP和EJB等，使得开发者可以轻松地构建出高性能、可扩展、可维护且安全的企业级应用程序。
4.高度安全。Java具有某些内置的安全功能，例如密码学、高级身份验证和访问控制，这些功能在很大程度上促成了Java开发服务的蓬勃发展。
5.可扩展性和多线程。Java是高度可扩展的，可以适应Web应用程序的需求，并为开发人员提供水平和垂直扩展的能力。此外，Java的NIO（非阻塞I/O）API促进了在软件的单个副本中创建尽可能多的线程，这极大地提高了应用程序的性能。


Web后端技术：
Servlet：
Servlet（Server Applet）是Java Servlet的简称，称为小服务程序或服务连接器，用Java编写的服务器端程序，具有独立于平台和协议的特性，主要功能在于交互式地浏览和生成数据，生成动态Web内容。
狭义的Servlet是指Java语言实现的一个接口，广义的Servlet是指任何实现了这个Servlet接口的类，一般情况下，人们将Servlet理解为后者。Servlet运行于支持Java的应用服务器中。
从原理上讲，Servlet可以响应任何类型的请求，但绝大多数情况下Servlet只用来扩展基于HTTP协议的Web服务器。
最早支持Servlet标准的是JavaSoft的Java Web Server，此后，一些其它的基于Java的Web服务器开始支持标准的Servlet。

Servlet的工作原理：
Servlet接口定义了Servlet与servlet容器之间的契约。这个契约是：Servlet容器将Servlet类载入内存，并产生Servlet实例和调用它具体的方法。但是要注意的是，在一个应用程序中，每种Servlet类型只能有一个实例。

>用户请求致使Servlet容器调用Servlet的Service（）方法，并传入一个ServletRequest对象和一个ServletResponse对象。
ServletRequest对象和ServletResponse对象都是由Servlet容器（例如TomCat）封装好的，并不需要开发者去实现，开发者可以直接使用这两个对象。
>ServletRequest中封装了当前的Http请求，因此，开发人员不必解析和操作原始的Http数据。
>ServletResponse表示当前用户的Http响应，程序员只需直接操作ServletResponse对象就能把响应轻松的发回给用户。
>对于每一个应用程序，Servlet容器还会创建一个ServletContext对象。这个对象中封装了上下文（应用程序）的环境详情。
>每个应用程序只有一个ServletContext。每个Servlet对象也都有一个封装Servlet配置的ServletConfig对象。


Servlet接口中定义的方法：
public interface Servlet {
    void init(ServletConfig var1) throws ServletException;
 
    ServletConfig getServletConfig();
 
    void service(ServletRequest var1, ServletResponse var2) throws ServletException, IOException;
 
    String getServletInfo();
 
    void destroy();
}


在静态资源中的Content-type是如何体现的？
静态资源：
静态资源通常包括 HTML、CSS、JavaScript、图片、视频等文件。这些文件在服务器上通常是以文件的形式存储，并且服务器（如 Apache、Nginx）在接收到对这些资源的请求时，会直接读取并返回这些文件的内容。

对于静态资源，Content-Type 通常由服务器自动根据文件的扩展名来确定。例如，对于 .html 文件，Content-Type 通常是 text/html；
对于 .css 文件，Content-Type 是 text/css；对于 .js 文件，Content-Type 是 application/javascript；对于图片文件，如 .jpg 或 .png，Content-Type 则分别是 image/jpeg 或 image/png。


Servlet设置Content-type：
Servlet资源：
Servlet 是 Java Web 应用程序中的一部分，用于处理客户端的请求并生成动态内容。Servlet 资源不是以文件的形式存储在服务器上，而是作为 Java 类被加载和执行。
当浏览器请求一个 Servlet 时，服务器（如 Tomcat）会加载并执行相应的 Java 类，然后返回生成的内容。
由于 Servlet 生成的内容是动态的，因此 Content-Type 通常不是由文件的扩展名来确定，而是由 Servlet 代码本身来设置。

在 Servlet 中，可以通过 response.setContentType(String type) 方法来设置 Content-Type。
例如，如果 Servlet 生成的是 HTML 内容，那么可以调用 response.setContentType("text/html")；
如果生成的是 JSON 数据，那么可以调用 response.setContentType("application/json")。

使用注解@webServlet可以设置Servlet路径


ServletRequset接口:
    Servlet容器对于接受到的每一个Http请求，都会创建一个ServletRequest对象，并把这个对象传递给Servlet的Sevice( )方法。其中，ServletRequest对象内封装了关于这个请求的许多详细信息。

public interface ServletRequest {
  
 
    int getContentLength();//返回请求主体的字节数
 
    String getContentType();//返回主体的MIME类型
 
    String getParameter(String var1);//返回请求参数的值
 
}

ServletResponse接口:
    javax.servlet.ServletResponse接口表示一个Servlet响应，在调用Servlet的Service( )方法前，Servlet容器会先创建一个ServletResponse对象，并把它作为第二个参数传给Service( )方法。ServletResponse隐藏了向浏览器发送响应的复杂过程。

public interface ServletResponse {
    String getCharacterEncoding();
 
    String getContentType();
 
    ServletOutputStream getOutputStream() throws IOException;
 
    PrintWriter getWriter() throws IOException;
 
    void setCharacterEncoding(String var1);
 
    void setContentLength(int var1);
 
    void setContentType(String var1);
 
    void setBufferSize(int var1);
 
    int getBufferSize();
 
    void flushBuffer() throws IOException;
 
    void resetBuffer();
 
    boolean isCommitted();
 
    void reset();
 
    void setLocale(Locale var1);
 
    Locale getLocale();
}

在发送任何HTML之前，应该先调用setContentType（）方法，设置响应的内容类型，并将“text/html”作为一个参数传入，这是在告诉浏览器响应的内容类型为HTML，
需要以HTML的方法解释响应内容而不是普通的文本，或者也可以加上“charset=UTF-8”改变响应的编码方式以防止发生中文乱码现象。


ServletContext对象:
ServletContext对象表示Servlet应用程序。每个Web应用程序都只有一个ServletContext对象。在将一个应用程序同时部署到多个容器的分布式环境中，每台Java虚拟机上的Web应用都会有一个ServletContext对象。
有了ServletContext对象，就可以共享从应用程序中的所有资料处访问到的信息，并且可以动态注册Web对象。前者将对象保存在ServletContext中的一个内部Map中。保存在ServletContext中的对象被称作属性。

在Servlet中，ServletContext对象表示整个Web应用程序的上下文，它提供了在Web应用程序的范围内共享信息的方法。ServletContext对象在Web应用程序启动时创建，并在Web应用程序关闭时销毁。

你可以通过以下方式在Servlet中获取ServletContext对象的内容：

通过getServletContext()方法：在Servlet类中，你可以直接调用getServletContext()方法来获取ServletContext对象。
ServletContext context = getServletContext();

获取初始化参数：你可以使用ServletContext对象的getInitParameter(String name)方法来获取在web.xml文件中定义的初始化参数。
String paramValue = context.getInitParameter("paramName");

获取资源：你可以使用ServletContext对象的getResourceAsStream(String path)或getResource(String path)方法来获取Web应用程序中的资源。
InputStream inputStream = context.getResourceAsStream("/WEB-INF/config.properties");  
URL resourceURL = context.getResource("/images/logo.png");

获取属性：你可以使用ServletContext对象的getAttribute(String name)方法来获取之前通过setAttribute(String name, Object object)方法设置的属性。
Object attribute = context.getAttribute("attributeName");

获取上下文路径：你可以使用ServletContext对象的getContextPath()方法来获取Web应用程序的上下文路径。
String contextPath = context.getContextPath();

获取请求分发器：你可以使用ServletContext对象的getRequestDispatcher(String path)方法来获取一个请求分发器对象，用于将请求转发到另一个资源。
RequestDispatcher dispatcher = context.getRequestDispatcher("/someServlet");

请注意，ServletContext对象在整个Web应用程序中都是共享的，因此它通常用于存储需要在整个应用程序中访问的信息，如数据库连接、配置参数等。
但是，由于它是全局的，所以应该避免在其中存储与特定用户或请求相关的信息。



ServletConfig接口:
当Servlet容器初始化Servlet时，Servlet容器会给Servlet的init( )方式传入一个ServletConfig对象。

ServletConfig接口中定义的方法：
public String getInitParameter(String name): 此方法用于获取在 web.xml 文件中为 servlet 配置的初始化参数。这个参数名是字符串类型，返回值也是字符串类型。如果找不到给定名称的参数，此方法将返回 null。
public ServletContext getServletContext(): 此方法返回与该 servlet 关联的 ServletContext 对象。
ServletContext 对象提供了对 Servlet 环境的访问，包括请求和响应对象、会话跟踪、初始化参数、上下文路径等等。
public String getServletName(): 此方法返回 servlet 的名称，该名称在 web.xml 文件中为 servlet 定义。如果找不到 servlet 名称，此方法将返回 null。


GenericServlet抽象类：
在编写一个Servlet对象时，必须实现Servlet接口，在Servlet接口中包含5个方法，创建Servlet时必须都要实现这5个方法，这样操作非常不便。
GenericServlet类简化了此操作，实现了Servlet接口。

public void init() throws ServletException: 这个方法是在 Servlet 实例化后且在任何请求处理前被调用的。它允许 Servlet 进行任何必要的初始化。
public void service(ServletRequest request, ServletResponse response) throws ServletException, IOException: 这个方法处理来自客户端的请求，并向客户端发送响应。所有的 Servlet 请求都是由这个方法处理的。
public String getServletInfo(): 这个方法返回一个字符串，这个字符串包含了关于 servlet 的信息。
public void destroy(): 当 servlet 不再需要时，这个方法被调用。它允许 Servlet 进行任何必要的清理活动，例如释放资源。
此外，GenericServlet 还覆盖了 Servlet 接口中的一些方法，包括 getInitParameter(String name) 和 getServletContext()，它们在 GenericServlet 中的实现与在 Servlet 接口中的定义是相同的。

注意，虽然 GenericServlet 实现了大部分 Servlet 接口的方法，但是它并没有实现所有的方法。例如，Servlet 接口中的 getLoginPage() 和 isUserInRole(String role) 方法在 GenericServlet 中没有实现。
如果需要这两个方法的功能，你可能需要自定义你的 Servlet 类并实现这两个方法。



HttpServlet抽象类:
>HttpServlet抽象类是继承于GenericServlet抽象类而来的。使用HttpServlet抽象类时，还需要借助分别代表Servlet请求和Servlet响应的HttpServletRequest和HttpServletResponse对象。
>HttpServletRequest接口扩展于javax.servlet.ServletRequest接口，HttpServletResponse接口扩展于javax.servlet.servletResponse接口。

public interface HttpServletRequest extends ServletRequest
public interface HttpServletResponse extends ServletResponse

HttpServlet抽象类覆盖了GenericServlet抽象类中的Service( )方法，并且添加了一个自己独有的Service(HttpServletRequest request，HttpServletResponse方法。

HttpServlet抽象类如何实现自己的service方法?
public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException {
    HttpServletRequest request;
    HttpServletResponse response;
    try {
        request = (HttpServletRequest)req;
        response = (HttpServletResponse)res;
    } catch (ClassCastException var6) {
        throw new ServletException("non-HTTP request or response");
    }
 
    this.service(request, response);
}

我们发现，HttpServlet中的service方法把接收到的ServletRequsest类型的对象转换成了HttpServletRequest类型的对象，把ServletResponse类型的对象转换成了HttpServletResponse类型的对象。
之所以能够这样强制的转换，是因为在调用Servlet的Service方法时，Servlet容器总会传入一个HttpServletRequest对象和HttpServletResponse对象，预备使用HTTP。因此，转换类型不会出错。


随后的Service方法：
protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
    String method = req.getMethod();
    long lastModified;
    if (method.equals("GET")) {
        lastModified = this.getLastModified(req);
        if (lastModified == -1L) {
            this.doGet(req, resp);
        } else {
            long ifModifiedSince = req.getDateHeader("If-Modified-Since");
            if (ifModifiedSince < lastModified) {
                this.maybeSetLastModified(resp, lastModified);
                this.doGet(req, resp);
            } else {
                resp.setStatus(304);
            }
        }
    } else if (method.equals("HEAD")) {
        lastModified = this.getLastModified(req);
        this.maybeSetLastModified(resp, lastModified);
        this.doHead(req, resp);
    } else if (method.equals("POST")) {
        this.doPost(req, resp);
    } else if (method.equals("PUT")) {
        this.doPut(req, resp);
    } else if (method.equals("DELETE")) {
        this.doDelete(req, resp);
    } else if (method.equals("OPTIONS")) {
        this.doOptions(req, resp);
    } else if (method.equals("TRACE")) {
        this.doTrace(req, resp);
    } else {
        String errMsg = lStrings.getString("http.method_not_implemented");
        Object[] errArgs = new Object[]{method};
        errMsg = MessageFormat.format(errMsg, errArgs);
        resp.sendError(501, errMsg);
    }
 
}
接下来我们再看看service方法是如何工作的，我们会发现在service方法中还是没有任何的服务逻辑，但是却在解析HttpServletRequest中的方法参数，并调用以下方法之一：doGet,doPost,doHead,doPut,doTrace,doOptions和doDelete。
这7种方法中，每一种方法都表示一个Http方法。doGet和doPost是最常用的。所以，如果我们需要实现具体的服务逻辑，不再需要覆盖service方法了，只需要覆盖doGet或者doPost就好了。

HttpServlet有两个特性是GenericServlet所不具备的：
1.不用覆盖service方法，而是覆盖doGet或者doPost方法。在少数情况，还会覆盖其他的5个方法。
2.使用的是HttpServletRequest和HttpServletResponse对象。


Servlet的生命周期：
init( ),service( ),destroy( )是Servlet生命周期的方法，代表了Servlet的生命周期，即Servlet从“出生”到“工作”再到“死亡 ”的过程。
Servlet容器（例如TomCat）会根据下面的规则来调用这三个方法：
1.init( ),当Servlet第一次被请求时，Servlet容器就会开始调用这个方法来初始化一个Servlet对象出来，
但是这个方法在后续请求中不会在被Servlet容器调用，就像人只能“出生”一次一样。我们可以利用init（ ）方法来执行相应的初始化工作。
调用这个方法时，Servlet容器会传入一个ServletConfig对象进来从而对Servlet对象进行初始化。

2.service( )方法，每当请求Servlet时，Servlet容器就会调用这个方法。就像人一样，需要不停的接受老板的指令并且“工作”。
第一次请求时，Servlet容器会先调用init( )方法初始化一个Servlet对象出来，然后会调用它的service( )方法进行工作，
但在后续的请求中，Servlet容器只会调用service方法了。

3.destory,当要销毁Servlet时，Servlet容器就会调用这个方法，就如人一样，到时期了就得死亡。
在卸载应用程序或者关闭Servlet容器时，就会发生这种情况，一般在这个方法中会写一些清除代码。


在SpringBoot中创建Servlet，继承HttpServlet类：
方式一： 通过注解扫描方式实现
创建一个servlet类

package com.liuhaiyang.springboot.servlet;
 
import javax.servlet.ServletException;
import javax.servlet.annotation.WebServlet;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import java.io.IOException;
 
//servlet第一种方式
@WebServlet(urlPatterns = "/myservlet")  //定义请求路径，通过该路径能访问到这个servlet
public class MyServlet extends HttpServlet {
 
    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        resp.getWriter().println("My Springboot-servlet-1");
        resp.getWriter().flush();
        resp.getWriter().close();
    }
 
    @Override
    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        super.doPost(req, resp);
    }
}
 
在SpringBoot项目的入口类上方使用注解 @ServletComponentScan 注解来扫描Servlet中的注解即可。

package com.liuhaiyang.springboot;
 
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.web.servlet.ServletComponentScan;
 
@SpringBootApplication
@ServletComponentScan(basePackages = "com.liuhaiyang.springboot.servlet")   //第一种的注解
public class SpringbootTest13Application {
 
    public static void main(String[] args) {
        SpringApplication.run(SpringbootTest13Application.class, args);
    }
 
}

方式二：通过 SpringBoot 的配置类实现（组件注册）

再次创建一个Servlet类。

package com.liuhaiyang.springboot.servlet2;
 
import javax.servlet.ServletException;
import javax.servlet.annotation.WebServlet;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import java.io.IOException;
 
 
//servlet第二种方式
public class MyServlet extends HttpServlet {
 
    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        resp.getWriter().println("My Springboot-servlet-2");
        resp.getWriter().flush();
        resp.getWriter().close();
    }
 
    @Override
    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        super.doPost(req, resp);
    }
}

编写一个 Spring Boot 的配置类，在该类中注册 Servlet
创建 ServletConfig 配置类

package com.liuhaiyang.springboot.config;
 
import com.liuhaiyang.springboot.servlet2.MyServlet;
import org.springframework.boot.web.servlet.ServletRegistrationBean;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
 
@Configuration //该注解将此类定义为配置类（相当于一个xml文件）
public class ServletConfig {
    //@Bean是一个方法级别上的注解，主要用于配置类里 相当于<beans> <bean id="" class="" > </beans>
    @Bean
    public ServletRegistrationBean myServletRegistrationBean(){
        ServletRegistrationBean servletRegistrationBean=new ServletRegistrationBean(new MyServlet(),"/myservlet");
        return servletRegistrationBean;
    }
}

在Servlet中，HttpServletRequest和ServletRequest HttpServletResponse和ServletResponse是什么关系？
ServletRequest 与 HttpServletRequest:
>ServletRequest 是一个通用的请求接口，定义了在Web应用中处理HTTP请求时的通用方法。
>HttpServletRequest 是 ServletRequest 的一个子接口，专门用于处理HTTP协议的请求。它提供了更多与HTTP协议相关的方法和属性，如获取请求头、查询参数、请求方法（GET、POST等）等。
>当你编写一个Servlet时，你通常会直接处理 HttpServletRequest，因为它提供了更丰富的功能来处理HTTP请求。

ServletResponse 与 HttpServletResponse:
>ServletResponse 是一个通用的响应接口，定义了在Web应用中构建HTTP响应时的通用方法。
>HttpServletResponse 是 ServletResponse 的一个子接口，专门用于构建HTTP协议的响应。它提供了更多与HTTP协议相关的方法和属性，如设置响应头、发送重定向、设置响应状态码等。
>同样地，在编写Servlet时，你通常会直接处理 HttpServletResponse，因为它允许你更精细地控制HTTP响应的各个方面。


关于转发和重定向的区别：

1.重定向访问服务器两次，转发只访问服务器一次。
2.转发页面的URL不会改变，而重定向地址会改变
3.转发只能转发到自己的web应用内，重定向可以重定义到任意资源路径。
4.转发相当于服务器跳转，相当于方法调用，在执行当前文件的过程中转向执行目标文件，两个文件(当前文件和目标文件)属于同一次请求，前后页共用一个request，可以通过此来传递一些数据或者session信息，request.setAttribute()和 request.getAttribute()。而重定向会产生一个新的request，不能共享request域信息与请求参数
5.由于转发相当于服务器内部方法调用，所以转发后面的代码仍然会执行(转发之后记得return)；重定向代码执行之后是方法执行完成之后进行重定向操作，也就是访问第二个请求，如果是方法的最后一行进行重定向那就会马上进行重定向(重定向也需要return)。
6.无论是RequestDispatcher.forward方法，还是HttpServletResponse.sendRedirect方法，在调用它们之前，都不能有内容已经被实际输出到了客户端。如果缓冲区中已经有了一些内容，这些内容将被从缓冲区中移除。


关于JSP和Servlet的区别：
1.  不同之处在哪？
Servlet在Java代码中通过HttpServletResponse对象动态输出HTML内容
JSP在静态HTML内容中嵌入Java代码，Java代码被动态执行后生成HTML内容

2.  各自的特点
Servlet能够很好地组织业务逻辑代码，但是在Java源文件中通过字符串拼接的方式生成动态HTML内容会导致代码维护困难、可读性差
JSP虽然规避了Servlet在生成HTML内容方面的劣势，但是在HTML中混入大量、复杂的业务逻辑同样也是不可取的

关于浏览器渲染数据：
前端渲染:
前端渲染就是指后端返回JSON数据或者JSONP数据，在前端利用预先写的html模板，循环读取JSON数据或者JSONP数据，进行选取，拼接，并且将这些数据插入页面来达到渲染效果。
后端渲染：
前端进行数据请求，后端用后台模板引擎直接生成html，前端接受到数据之后，直接插入页面。

前端渲染和后端渲染两者的好处和坏处:
1.前端渲染:
好处：网络传输数据量小。不占用服务端运算资源（解析模板），模板在前端（很有可能仅部分在前端），改结构变交互都前端自己来了，改完自己调就行。
坏处：前端耗时较多，对前端工作人员水平要求相对较高。前端代码较多，因为部分以前在后台处理的交互逻辑交给了前端处理。占用少部分客户端运算资源用于解析模板。
2.后端渲染
好处：前端耗时少，即减少了首屏时间，模板统一在后端。前端（相对）省事，不占用客户端运算资源（解析模板）
坏处：占用服务器资源。

前端渲染与后端渲染对比：
    
前端渲染：
页面呈现速度：主要受限于带宽和客户端机器的好坏，优化效果好，可以逐步动态展开内容，感觉上会更快一点。
可维护性强，前后端分离，各施其职，代码一目明了。
SEO友好度(seo=Search(搜索) Engine(引擎) Optimization(优化),即搜索引擎优化)：差，大量使用ajax，多数浏览器不能抓取ajax数据。
编码效率：高，前后端各自只做自己擅长的东西，后端最后只输出接口，不用管页面呈现，只要前后端人员能力不错，效率不会低。
    
后端渲染：
页面呈现速度：快，受限于用户的带宽
可维护性：差（前后端不分离）
SEO友好度：好,后端的数据一步搞定,直接生成相对应的模板网页
编码效率：低（有时候可能手忙脚乱,找不到数据对应的位置或者数据）


什么是过滤器（Filter）
过滤器，是在java web中将你传入的request、response提前过滤掉一些信息，或者提前设置一些参数。
然后再传入Servlet或Struts2的 action进行业务逻辑处理。比如过滤掉非法url（不是login.do的地址请求，如果用户没有登陆都过滤掉），
或者在传入Servlet或Struts2的action前统一设置字符集，或者去除掉一些非法字符。
过滤器处于客户端与Web资源（Servlet、JSP、HTML）之间，客户端与Web资源之间的请求和响应都要通过过滤器进行过滤。

使用Servlet创建过滤器：
在Java Web开发中，过滤器（Filter）是一个常用的组件，它能够对Web应用程序的请求和响应进行预处理和后处理。Servlet规范定义了过滤器API，使得开发人员可以创建自定义的过滤器来处理HTTP请求和响应。以下是一个简单的示例，说明如何使用Servlet创建一个过滤器：

实现Filter接口
首先，你需要创建一个类，该类实现了javax.servlet.Filter接口。这个接口定义了三个方法：init(), doFilter(), 和 destroy()。

java
import javax.servlet.*;  
import javax.servlet.http.HttpServletRequest;  
import javax.servlet.http.HttpServletResponse;  
import java.io.IOException;  
  
public class MyFilter implements Filter {  
  
    @Override  
    public void init(FilterConfig filterConfig) throws ServletException {  
        // 初始化方法，在过滤器创建时调用一次  
        System.out.println("MyFilter initialized.");  
    }  
  
    @Override  
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)  
            throws IOException, ServletException {  
          
        // 在请求处理前执行代码  
        HttpServletRequest req = (HttpServletRequest) request;  
        HttpServletResponse res = (HttpServletResponse) response;  
          
        System.out.println("MyFilter is filtering request for " + req.getRequestURI());  
          
        // 继续过滤器链，即调用下一个过滤器或目标资源  
        chain.doFilter(request, response);  
          
        // 在请求处理后执行代码（如果需要的话）  
        System.out.println("MyFilter has finished filtering request for " + req.getRequestURI());  
    }  
  
    @Override  
    public void destroy() {  
        // 销毁方法，在过滤器销毁时调用一次  
        System.out.println("MyFilter destroyed.");  
    }  
}
在web.xml中配置过滤器
然后，你需要在你的web.xml文件中配置过滤器，以告诉Web容器何时应用这个过滤器。

<web-app ...>  
    <!-- 其他配置 -->  
      
    <filter>  
        <filter-name>MyFilter</filter-name>  
        <filter-class>com.example.MyFilter</filter-class>  
    </filter>  
      
    <filter-mapping>  
        <filter-name>MyFilter</filter-name>  
        <url-pattern>/*</url-pattern> <!-- 应用到所有请求 -->  
    </filter-mapping>  
      
    <!-- 其他配置 -->  
</web-app>

<filter>元素定义了过滤器的名称和类，而<filter-mapping>元素定义了该过滤器应用到哪些URL模式上。/*意味着过滤器会应用到Web应用程序中的所有请求。

使用注解：
如果你的Servlet容器（如Tomcat 7及更高版本、Jetty等）支持Servlet 3.0或更高版本的注解，你可以使用@WebFilter注解来替代web.xml中的配置。


import javax.servlet.annotation.WebFilter;  
import javax.servlet.annotation.WebInitParam;  
  
@WebFilter(urlPatterns = "/*", initParams = {  
    @WebInitParam(name = "param1", value = "value1")  
})  
public class MyFilter implements Filter {  
    // ... 实现方法与之前相同 ...  
}

使用Spring Boot创建过滤器：
在Spring Boot中创建过滤器（Filter）相对简单，因为Spring Boot提供了自动配置和简化配置的功能。
可以通过实现javax.servlet.Filter接口或者通过Spring Boot的@WebFilter注解（如果Servlet API版本和Servlet容器支持的话）来创建过滤器。
不过，更常见的做法是在Spring Boot中通过Java配置来定义过滤器。

创建过滤器类
首先，创建一个实现javax.servlet.Filter接口的类：

import javax.servlet.*;  
import javax.servlet.http.HttpServletRequest;  
import java.io.IOException;  
  
public class MyCustomFilter implements Filter {  
  
    @Override  
    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain)  
            throws IOException, ServletException {  
          
        HttpServletRequest request = (HttpServletRequest) servletRequest;  
        System.out.println("MyCustomFilter is filtering request for " + request.getRequestURI());  
          
        // 继续过滤器链  
        filterChain.doFilter(servletRequest, servletResponse);  
          
        System.out.println("MyCustomFilter has finished filtering request for " + request.getRequestURI());  
    }  
  
    @Override  
    public void init(FilterConfig filterConfig) throws ServletException {  
        // 初始化方法  
    }  
  
    @Override  
    public void destroy() {  
        // 销毁方法  
    }  
}

注册过滤器：
接下来，你需要在Spring Boot配置中注册这个过滤器。你可以使用@Configuration和@Bean注解来定义一个配置类，并在这个类中注册过滤器：

import org.springframework.boot.web.servlet.FilterRegistrationBean;  
import org.springframework.context.annotation.Bean;  
import org.springframework.context.annotation.Configuration;  
  
@Configuration  
public class FilterConfig {  
  
    @Bean  
    public FilterRegistrationBean<MyCustomFilter> myCustomFilterRegistration() {  
        FilterRegistrationBean<MyCustomFilter> registrationBean = new FilterRegistrationBean<>();  
        registrationBean.setFilter(new MyCustomFilter());  
        registrationBean.addUrlPatterns("/*"); // 应用到所有请求  
        return registrationBean;  
    }  
}
在上面的代码中，我们创建了一个FilterRegistrationBean，并将我们的MyCustomFilter实例设置到它里面。然后，我们添加了一个URL模式/*，这表示该过滤器将应用于所有的请求。

使用@Component注解（可选）
如果你的过滤器类不需要任何特定的配置，你可以简单地将其标记为@Component，Spring Boot会自动发现并注册它：

import javax.servlet.*;  
import javax.servlet.annotation.WebFilter;  
import javax.servlet.http.HttpServletRequest;  
import java.io.IOException;  

@Component  
public class MyComponentFilter implements Filter {  
    // ... 实现方法与之前相同 ...  
}

在这种情况下，不需要创建FilterRegistrationBean，因为Spring Boot会自动处理它。但是，如果需要更细粒度的控制（比如指定URL模式），那么使用FilterRegistrationBean是更好的选择。
注：在Servlet中，过滤器执行的顺序按照配置的顺序运行

什么是监听器（Listener)
监听器（Listener）是一种运行在后台的程序，它主要用于监控某些事件在系统中的发生，并且根据这些事件执行特定的处理操作。
在Web应用程序中，监听器可以观察ServletContext、HttpSession以及ServletRequest等对象的变化，以便在这些对象变化时进行相应的处理。

使用Servlet创建一个监听器：

首先，我们需要实现一个监听器接口。假设我们想要创建一个ServletContextListener，用于监听ServletContext的创建和销毁事件：

import javax.servlet.ServletContextEvent;  
import javax.servlet.ServletContextListener;  
  
public class MyServletContextListener implements ServletContextListener {  
  
    @Override  
    public void contextInitialized(ServletContextEvent sce) {  
        // ServletContext被创建时调用  
        System.out.println("ServletContext 初始化");  
        // 在这里可以执行初始化操作，比如加载配置文件等  
    }  
  
    @Override  
    public void contextDestroyed(ServletContextEvent sce) {  
        // ServletContext被销毁时调用  
        System.out.println("ServletContext 销毁");  
        // 在这里可以执行清理操作，比如释放资源等  
    }  
}

然后，我们需要在web.xml文件中配置这个监听器：

<web-app ...>  
    <!-- 其他配置 -->  
      
    <listener>  
        <listener-class>com.example.MyServletContextListener</listener-class>  
    </listener>  
</web-app>

如果你的项目使用的是Servlet 3.0或更高版本，你还可以使用注解来避免在web.xml中手动配置监听器。你只需要在监听器类上使用@WebListener注解即可：


import javax.servlet.annotation.WebListener;  
import javax.servlet.ServletContextEvent;  
import javax.servlet.ServletContextListener;  
  
@WebListener  
public class MyServletContextListener implements ServletContextListener {  
    // 实现与上面相同的contextInitialized和contextDestroyed方法  
}
使用@WebListener注解后，你就不需要在web.xml中手动添加<listener>配置了，Servlet容器会在启动时自动发现并注册这个监听器。

注意：监听器类应该放在WEB-INF/classes目录或其子目录下，或者打包在WEB-INF/lib目录下的JAR文件中，以确保Servlet容器能够找到和加载它们。
当Web应用程序启动或停止时，Servlet容器会自动调用监听器的相应方法。


什么是拦截器（Interceptor）
拦截器是一种面向方面/切面编程（AOP Aspect-Oriented Programming），而面向切面就是将多个模块的通用服务进行分离，
如权限管理、日志服务，他们在多个模块中都会用到，就可以将其各自封装为一个可重用模块。

而这些通用服务的具体实现是通过拦截器来完成，比如用户客户端访问一些保密模块都应先通过权限审查的拦截器来进行权限审查，
确定用户是否具有该项操作的权限后方能向下执行。在面向切面编程的就是在你的service或者一个方法前调用一个方法，
或者在方法后调用一个方法，比如动态代理就是拦截器的简单实现，在你调用方法前打印出字符串（或者做其它业务逻辑的操作），
也可以在你调用方法后打印出字符串，甚至在你抛出异常的时候做业务逻辑的操作。

拦截器与过滤器区别
1、拦截器是基于java的反射机制的，而过滤器是基于函数回调（职责链）
2、过滤器依赖与servlet容器，而拦截器不依赖与servlet容器
3、拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用
4、拦截器可以访问action上下文、值栈里的对象，而过滤器不能
5、在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次
6、拦截器可以获取IOC容器中的各个bean，而过滤器不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。

通俗理解：
（1）过滤器（Filter）：当你有一堆东西的时候，你只希望选择符合你要求的某一些东西。定义这些要求的工具，就是过滤器。（理解：就是一堆字母中取一个B）
（2）拦截器（Interceptor）：在一个流程正在进行的时候，你希望干预它的进展，甚至终止它进行，这是拦截器做的事情。（理解：就是一堆字母中，干预它，通过验证的少点，顺便干点别的东西

触发时机与执行顺序
触发时机：
拦截器与过滤器触发时机不一样
过滤器是在请求进入容器后，但请求进入servlet之前进行预处理的。请求结束返回也是，是在servlet处理完后，返回给前端之前。

过滤器包裹servlet，servlet包裹住拦截器。

执行顺序 ：
过滤前 - 拦截前 - Action处理 - 拦截后 - 过滤后。个人认为过滤是一个横向的过程，首先把客户端提交的内容进行过滤(例如未登录用户不能访问内部页面的处理)；
过滤通过后，拦截器将检查用户提交数据的验证，做一些前期的数据处理，接着把处理后的数据发给对应的Action；
Action处理完成返回后，拦截器还可以做其他过程(还没想到要做啥)，再向上返回到过滤器的后续操作。

例子：自定义拦截器
@Component
public class MyInterceptor implements HandlerInterceptor {
  @Override
  public boolean preHandle(HttpServletRequest request, HttpServletResponse response,
                           Object handler) throws Exception {
		return true;
		}
}

需要再写一个自己的配置类：
@Configuration
public class MyConfiguration implements WebMvcConfigurer {
	 @Override
	 public void addInterceptors(InterceptorRegistry registry) {
		  //将我们自定义的拦截器加入配置
	 	 InterceptorRegistration interceptor = registry.addInterceptor(new MyInterceptor());
		  //设置拦截的url
		  interceptor.addPathPatterns("/user/**");
	 }
}


使用Spring MVC拦截器实现控制权限：
首先，你需要创建一个实现了HandlerInterceptor接口的类。在这个类中，你可以重写preHandle方法来进行权限控制。

java
import javax.servlet.http.HttpServletRequest;  
import javax.servlet.http.HttpServletResponse;  
import org.springframework.web.servlet.HandlerInterceptor;  
import org.springframework.web.servlet.ModelAndView;  
  
public class AuthInterceptor implements HandlerInterceptor {  
  
    @Override  
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {  
        // 检查用户是否登录或者是否有权限访问  
        String username = (String) request.getSession().getAttribute("username");  
        if (username == null || !isUserAuthorized(username, request)) {  
            // 如果用户未登录或未授权，重定向到登录页面  
            response.sendRedirect(request.getContextPath() + "/login");  
            return false; // 返回false表示请求处理到此为止，不再继续执行后续的Controller方法  
        }  
        return true; // 返回true表示请求可以继续执行  
    }  
  
    private boolean isUserAuthorized(String username, HttpServletRequest request) {  
        // 这里应该实现具体的权限检查逻辑  
        // 例如，可以检查用户名是否匹配某个允许访问的列表  
        return true; // 示例中默认返回true，表示所有用户都有权限  
    }  
  
    @Override  
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {  
        // 请求处理完成后调用，通常用于视图渲染前的处理  
    }  
  
    @Override  
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {  
        // 整个请求完成后调用，通常用于资源清理  
    }  
}
然后，你需要在Spring MVC的配置中注册这个拦截器，并指定它应该拦截哪些URL。这可以通过Java配置或XML配置来完成。

Java配置示例：
import org.springframework.beans.factory.annotation.Autowired;  
import org.springframework.context.annotation.Configuration;  
import org.springframework.web.servlet.config.annotation.InterceptorRegistry;  
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;  
  
@Configuration  
public class WebConfig implements WebMvcConfigurer {  
  
    @Autowired  
    private AuthInterceptor authInterceptor;  
  
    @Override  
    public void addInterceptors(InterceptorRegistry registry) {  
        registry.addInterceptor(authInterceptor)  
                .addPathPatterns("/secure/**") // 拦截所有以"/secure/"开头的URL  
                .excludePathPatterns("/secure/login"); // 排除"/secure/login"这个URL，不需要权限检查  
    }  
}

XML配置示例：

<mvc:interceptors>  
    <mvc:interceptor>  
        <mvc:mapping path="/secure/**" />  
        <mvc:exclude-mapping path="/secure/login" />  
        <bean class="com.example.AuthInterceptor" />  
    </mvc:interceptor>  
</mvc:interceptors>
在这个配置中，AuthInterceptor会被注册为一个拦截器，并配置为拦截所有以/secure/开头的URL，但排除/secure/login这个URL。


SpringBoot配置嵌入式Servlet容器：
1、找到接口ConfigurableEmbeddedServletContainer
2、找到接口EmbeddedServletContainerCustomizer
3、注册Servlet、Filter、Listener
	>ServletRegistrationBean
	>FilterRegistrationBean
	>ServletListenerRegistrationBean

配置Servlet:
@WebServlet(urlPatterns = "/portal/*")
public class HelloServlet extends HttpServlet {
    public HelloServlet() {
        super();
    }
 
    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        String name = req.getParameter("name");
        System.out.println("=====================Hello "+ name +", this is doGet Filter===================");
        PrintWriter writer = resp.getWriter();
        writer.write("Your request parameter is " + name);
        writer.flush();
        writer.close();
    }
 
    @Override
    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        System.out.println("=====================Hello doPost Filter===================");
        super.doPost(req, resp);
    }
 
    @Override
    protected void doPut(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        System.out.println("=====================Hello doPut Filter===================");
        super.doPut(req, resp);
    }
}

配置Filter：
@WebFilter(urlPatterns = "/*", filterName = "helloFilter")
public class HelloFilter implements Filter {
 
    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
        System.out.println("================init filter===================");
    }
 
    @Override
    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {
        System.out.println("=================this is filer method====================");
        filterChain.doFilter(servletRequest, servletResponse);
    }
 
    @Override
    public void destroy() {
        System.out.println("==========================destroy method===================");
    }
}

配置Listener：
package com.learn.listener;
 
import javax.servlet.ServletContextEvent;
import javax.servlet.ServletContextListener;
import javax.servlet.annotation.WebListener;
 
/**
 * springBoot整合Listener
 *
 *<listener>
 *	<listener-class>com.learn.listener.FirstListener</listener-class>
 *</listener>
 */
@WebListener
public class MyListener implements ServletContextListener {
 
	@Override
	public void contextDestroyed(ServletContextEvent arg0) {
		// TODO Auto-generated method stub
 
	}
 
	@Override
	public void contextInitialized(ServletContextEvent arg0) {
		System.out.println("Listener...init......");
 
	}
 
}

Java WEB 中什么是前后端不分离？
在Java WEB开发中，前后端不分离和前后端分离是两种常见的架构设计方式。
前后端不分离（Monolithic Architecture）：在这种架构中，前端和后端是紧密耦合的，它们运行在同一个应用程序中。
这种架构的设计初衷是为了简化开发和维护，尤其是在小型应用或初创公司中。下面是一些前后端不分离的例子：

1.Spring Boot：Spring Boot是一个全栈的Java框架，它可以创建独立的、生产级别的Spring应用程序。
Spring Boot可以构建Web应用，并且内置了Tomcat服务器，前端和后端代码都放在同一个应用中。
2.JavaServer Pages (JSP)：JSP是一种Java技术，让你可以在HTML中嵌入Java代码。
JSP在服务器上运行，并动态生成HTML页面返回给客户端。这种方式下，前端和后端的代码是混在一起的。


前后端分离（Microservices Architecture）：与前后端不分离相反，前后端分离将前端和后端的职责完全分开。
前端通常负责展示数据和用户交互，而后端则负责处理业务逻辑和数据持久化。这种架构有助于提高可扩展性、模块化和独立性。
下面是一些前后端分离的例子：

1.Spring Cloud + React/Vue：这是一种常见的微服务架构。Spring Cloud用于构建后端微服务，如API网关、服务发现、负载均衡等。
而React或Vue等前端框架则负责构建用户界面。
2.RESTful API + Front-end SPA：后端提供RESTful API来处理业务逻辑和数据，
而前端使用Single Page Application (SPA)框架（如Angular、React或Vue）来获取数据并展示给用户。

总之：这两种架构都有其优缺点。前后端不分离的架构简单易用，但在大型项目中可能限制了可扩展性和模块化。
前后端分离的架构提供了更高的灵活性，但同时也增加了开发和维护的复杂性。


路由控制:前后端分离的情况后端在任何都会返回一个json数据，不涉及路由处理，即页面跳转全是由前端自己完成。
不分离的情况，路由跳转一般由后端完成，而且携带数据，通过重定向或请求转发实现，依赖Servlet中的内置对象。
返回数据:前后端分离的情况后端返回的整体划分上只有执行逻辑响应的消息实体类和对应的需要展示的数据分页类对应的JSON。
不分离后端返回的是经过视图解析器处理前的字符串。

请求方式:前后端分离的情况以Rest风格的方式处理为主,当执行增删改查对应http请求方法POST,DELEE,PUT,GET,
一般以异步请求为主,不分离情况一般同步异步结合。请求上以GET和POST为主。

注解使用：前后端分离，接收的数据是Json时需要@RequestBody作用在方法参数上。
用于将POST请求数据转化为实体类。返回数据时使用@ResponseBody用于将POST请求数据转化为实体类。
返回数据时使用@ResponseBody,作用在方法上,当然对于全局的也可以使用@RespController注解。
前后端不分离一般使用@RequestMapping，请求方式不用管。前后端不分离一般使用@RequestMapping，请求方式不用管。



后端技术：
Servlet → Java spring → springMVC
JDBC → ORM框架
以前：J2EE应用框架 J2EE开发框架主要有Hibernate，Spring，Struts2，EXTJS，Json。
现在：SpringBoot、SpringCloud

JDBC:
JDBC（Java Database Connectivity）
JDBC 是 Java 数据库连接（Java Database Connectivity）的缩写，它是一种用于执行 SQL 语句的 Java API。JDBC 可以让 Java 应用程序连接到几乎所有的关系数据库，执行查询和更新数据库中的数据，并处理结果。
JDBC 是一种底层的 API，它为开发者提供了操作数据库的标准方法，但它本身并不包含数据库的实现，而是由数据库厂商提供 JDBC 驱动来实现对特定数据库的访问。

JDBC 基本步骤
使用 JDBC 访问数据库的基本步骤通常包括：

加载 JDBC 驱动：通过 Class.forName() 方法加载数据库的 JDBC 驱动程序。
建立数据库连接：通过 DriverManager.getConnection() 方法建立到数据库的连接。
创建 Statement：通过连接对象创建 Statement 对象，用于执行 SQL 语句。
执行 SQL 语句：使用 Statement 对象执行 SQL 语句，并返回结果。
处理结果：根据 SQL 语句的不同，可能需要处理 ResultSet 结果集或更新计数。
关闭连接：关闭 ResultSet、Statement 和 Connection 对象，以释放 JDBC 资源。

示例代码：
import java.sql.Connection;  
import java.sql.DriverManager;  
import java.sql.ResultSet;  
import java.sql.Statement;  
  
public class JDBC {  
    public static void main(String[] args) {  
        // 数据库 URL，用户名和密码  
        String url = "jdbc:mysql://localhost:3306/your_database_name?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=UTC";  
        String user = "your_username";  
        String password = "your_password";  
  
        try {  
            // 加载 JDBC 驱动  
            Class.forName("com.mysql.cj.jdbc.Driver");  
  
            // 建立连接  
            Connection conn = DriverManager.getConnection(url, user, password);  
  
            // 创建 Statement  
            Statement stmt = conn.createStatement();  
  
            // 执行查询  
            String sql = "SELECT id, name FROM users";  
            ResultSet rs = stmt.executeQuery(sql);  
  
            // 处理结果  
            while (rs.next()) {  
                int id = rs.getInt("id");  
                String name = rs.getString("name");  
                System.out.println("ID: " + id + ", Name: " + name);  
            }  
  
            // 关闭资源  
            rs.close();  
            stmt.close();  
            conn.close();  
        } catch (Exception e) {  
            e.printStackTrace();  
        }  
    }  
}


Hibernate：
Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，它将POJO与数据库表建立映射关系，是一个全自动的orm框架，hibernate可以自动生成SQL语句，自动执行，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。
Hibernate可以应用在任何使用JDBC的场合，既可以在Java的客户端程序使用，也可以在Servlet/JSP的Web应用中使用，最具革命意义的是，Hibernate可以在应用EJB的JavaEE架构中取代CMP，完成数据持久化的重任。

创建映射配置文件：
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE hibernate-mapping PUBLIC 
    "-//Hibernate/Hibernate Mapping DTD 3.0//EN"
    "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd">
    
<hibernate-mapping>
	<!-- 建立类与表的映射 -->
	<class name="com.meimeixia.hibernate.demo01.Customer" table="cst_customer">
		<!-- 建立类中的属性与表中的主键相对应 -->
		<id name="cust_id" column="cust_id">
			<!-- 主键的生成策略，后面会讲，现在使用的是本地生成策略 -->
			<generator class="native" />
		</id>
		
		<!-- 建立类中的普通属性和表中的字段相对应 -->
		<property name="cust_name" column="cust_name" />
		<property name="cust_source" column="cust_source" />
		<property name="cust_industry" column="cust_industry" />
		<property name="cust_level" column="cust_level" />
		<property name="cust_phone" column="cust_phone" />
		<property name="cust_mobile" column="cust_mobile" />
	</class>
</hibernate-mapping>

创建核心配置文件：
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE hibernate-configuration PUBLIC
	"-//Hibernate/Hibernate Configuration DTD 3.0//EN"
	"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd">
	
<hibernate-configuration>
	<session-factory>
		<!-- 下面是三个必须要有的配置 -->
		<!-- 配置连接MySQL数据库的基本参数 -->
		<property name="hibernate.connection.driver_class">com.mysql.jdbc.Driver</property>
		<property name="hibernate.connection.url">jdbc:mysql:///hibernate_demo01</property>
		<property name="hibernate.connection.username">root</property>
		<property name="hibernate.connection.password">liayun</property>
		
		<!-- 配置Hibernate的方言 -->
		<property name="hibernate.dialect">org.hibernate.dialect.MySQLDialect</property>
		
		<!-- 下面两个是可选的配置！ -->
		<!-- 打印sql语句 -->
		<property name="hibernate.show_sql">true</property>
		<!-- 格式化sql语句 -->
		<property name="hibernate.format_sql">true</property>
		
		<!-- 告诉Hibernate的核心配置文件加载哪个映射文件 -->
		<mapping resource="com/meimeixia/hibernate/demo01/Customer.hbm.xml"/>
	</session-factory>
</hibernate-configuration>

创建单元测试类：
package com.meimeixia.hibernate.demo01;

import org.hibernate.Session;
import org.hibernate.SessionFactory;
import org.hibernate.Transaction;
import org.hibernate.cfg.Configuration;
import org.junit.Test;

/**
 * Hibernate的入门案例
 * @author 
 *
 */
public class HibernateDemo1 {
	
	//保存用户的案例
	@Test
	public void demo1() {
		//1. 加载Hibernate的核心配置文件
		Configuration configuration = new Configuration().configure();
		//如果在Hibernate的核心配置文件没有设置加载哪个映射文件，则可手动加载映射文件
		//configuration.addResource("com/meimeixia/hibernate/demo01/Customer.hbm.xml");
		
		//2. 创建SessionFactory对象，类似于JDBC中的连接池
		SessionFactory sessionFactory = configuration.buildSessionFactory();
		
		//3. 通过SessionFactory获取到Session对象，类似于JDBC中的Connection
		Session session = sessionFactory.openSession();
		
		//4. 手动开启事务，（最好是手动开启事务）
		Transaction transaction = session.beginTransaction();
		
		//5. 编写代码
		Customer customer = new Customer();
		customer.setCust_name("张小敬aaa");
		
		session.save(customer);//保存一个用户
		
		//6. 事务提交
		transaction.commit();
		
		//7. 释放资源
		session.close();
		sessionFactory.close();
	}
	
}

MVC设计模式：MVC指MVC模式的某种框架，它强制性地使应用程序的输入、处理和输出分开。
使用MVC应用程序被分成三个核心部件：模型、视图、控制器。

>M即model模型是指模型表示业务规则。在MVC的三个部件中，模型拥有最多的处理任务。即Bean层、mapper层和service层。
被模型返回的数据是中立的，模型与数据格式无关，这样一个模型能为多个视图提供数据，由于应用于模型的代码只需写一次就可以被多个视图重用，所以减少了代码的重复性。

>V即View视图是指用户看到并与之交互的界面。比如由html元素组成的网页界面，或者软件的客户端界面。
MVC的好处之一在于它能为应用程序处理很多不同的视图。在视图中其实没有真正的处理发生，它只是作为一种输出数据并允许用户操作的方式。

>C即controller控制器是指控制器接受用户的输入并调用模型和视图去完成用户的需求，控制器本身不输出任何东西和做任何处理。
它只是接收请求并决定调用哪个模型构件去处理请求，然后再确定用哪个视图来显式返回的数据。

MVC(Model模型、View视图、Control控制层)，将软件进行分层达到松耦合的效果。
通用的软件编程思想, 在MVC设计模式中认为, 任何软件都可以分三层：
控制层（Controller）、数据处理模型（Model）、负责展示数据的视图（View）。
在MVC设计思想中要求一个符合MVC设计思想的软件应该保证上面这三部分相互独立，互不干扰，每一个部分只负责自己擅长的部分。
如果某一个模块发生变化，应该尽量做到不影响其他两个模块。提高代码的可读性，实现程序间的松耦合、提高代码复用性。

Struts2：
Struts2框架是一个用于开发Java EE网络应用程序的开放源代码网页应用程序架构。它利用并延伸了Java Servlet API，鼓励开发者采用MVC架构。
Struts2以WebWork优秀的设计思想为核心，吸收了Struts框架的部分优点，提供了一个更加整洁的MVC设计模式实现的Web应用程序框架。

Strut2核心过滤器：web.xml
<?xml version="1.0" encoding="UTF-8"?>
<web-app version="2.5" 
	xmlns="http://java.sun.com/xml/ns/javaee" 
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xsi:schemaLocation="http://java.sun.com/xml/ns/javaee 
	http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd">
  <display-name></display-name>	
  <welcome-file-list>
    <welcome-file>index.jsp</welcome-file>
  </welcome-file-list>
  
  <!-- 配置struts2的核心过滤器 -->
  <!-- 所有请求都要通过核心过滤器 -->
  <filter>
  	<filter-name>struts2</filter-name>
  	<filter-class>org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter</filter-class>
  </filter>
  
  <filter-mapping>
  	<filter-name>struts2</filter-name>
  	<url-pattern>/*</url-pattern>
  </filter-mapping>
  
</web-app>

编写启动类：
package com.banana.struts.demo1;
/*
 * Struts2的Action类
 */
public class HelloAction {
	public String execute(){
		System.out.print("HelloAction执行了......");
		return "success";
	}
}

编写struts.xml来控制转向：
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE struts PUBLIC
	"-//Apache Software Foundation//DTD Struts Configuration 2.3//EN"
	"http://struts.apache.org/dtds/struts-2.3.dtd">

<struts>
	<!-- Struts2为了管理Action的配置，通过包进行管理 -->
	<!-- 配置Struts2的包 -->
	<package name="hello" extends="struts-default" namespace="/">
		<!-- 配置Action -->
		<action name="hello" class="com.banana.struts.demo1.HelloAction">
		<!-- 页面的跳转 -->
			<result name="success">/demo1/success.jsp</result>
		</action>
	</package>
</struts>

添加页面：
<%@ page language="java" import="java.util.*" pageEncoding="UTF-8"%>
<%
String path = request.getContextPath();
String basePath = request.getScheme()+"://"+request.getServerName()+":"+request.getServerPort()+path+"/";
%>

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <base href="<%=basePath%>">
    <title>My JSP 'demo1' starting page</title>
  </head>
  
  <body>
    <h1>Struts2的入门</h1>
    <h3><a href="${ pageContext.request.contextPath }/hello.action">Struts2的入门</a></h3>
  </body>
  
</html>

Struts2被弃用原因：被爆出安全漏洞
Struts2 漏洞概要
2017年3月6日，Apache Struts2被曝存在远程命令执行漏洞，漏洞编号:S2-045，CVE编号:CVE-2017-5638，官方评级为高危，
该漏洞是由于在使用基于Jakarta插件的文件上传功能条件下，恶意用户可以通过修改HTTP请求头中的Content-Type值来触发该漏洞，进而执行任意系统命令，导致系统被黑客入侵。


SSH：
SSH框架是Struts、Spring、Hibernate的一个合成框架，目前市场上比较流行的框架中也有它的身影。
Struts是一个基于MVC模式的应用框架，如果学过Servlet,那么其本质和Servlet差不多。
MVC模式主要包括模型(Model)，视图(View)，控制器(Controller)，而Struts主要作为控制器来建立模型和视图的数据交互。
Struts通过拦截器处理客户的各种请求。Spring使用基本的JavaBean来完成以前只可能由EJB完成的事情，
Spring的核-fl，主要控制翻转(IOC)和面向切面(AOP)，简单的说Spring是一种分层的轻量级开源框架。
Spring更像是一个容器，将所有配置的Struts和Hibernate中的东西都放置进来，只要能够做好配置，它就会找到相应的位置，进行处理。
Hibernate是一个开源代码的对象映射框架，是根据JDBC技术基础衍生而来的，它将直接操作原来的数据库变为直接操作数据表后生成的Java类，
实现了对象编程思维来操纵数据库。SSH框架中的各种技术相互协调、配合。实现了这一强大的框架。


SSM：SSM（Spring+SpringMVC+MyBatis）框架集由Spring、MyBatis两个开源框架整合而成（SpringMVC是Spring中的部分内容），常作为数据源较简单的web项目的框架。

1.spring MVC ＋ spring +mybatis，是标准的MVC设计模式，将整个系统划分为显示层，Controller层，Service层，DAO层四层。
使用Spring MVC负责请求的转发和视图管理；spring实现业务对象管理，mybatis作为数据对象的持久化引擎。
2、Spring是一个开源框架，Spring是一个轻量级的控制反转（IoC）和面向切面（AOP）的容器框架，还能更好的让其他框架整合。
3、Spring MVC框架是有一个MVC框架，通过实现Model-View-Controller模式来很好地将数据、业务与展现进行分离。
4、MyBatis 是一个基于Java的持久层框架


Java Spring框架：
Spring 框架就像一个家族，有众多衍生产品例如 boot、security、jpa等等。但他们的基础都是Spring 的 ioc和 aop ioc 提供了依赖注入的容器 aop ，解决了面向横切面的编程，然后在此两者的基础上实现了其他延伸产品的高级功能。
Spring MVC是基于 Servlet 的一个 MVC 框架 主要解决 WEB 开发的问题，因为 Spring 的配置非常复杂，各种XML、 JavaConfig、hin处理起来比较繁琐。于是为了简化开发者的使用，从而创造性地推出了Spring boot，约定优于配置，简化了spring的配置流程。
Spring的工作原理其实是，Spring在启动时读取bean配置信息，这里bean配置信息包括xml文件、Java类#Configuration、注解#AutoWired，存放到Bean注册表，并根据Bean注册表实例化Bean，实际上就是实例化Bean的实现类，然后将Bean实例放到Spring容器中。

Spring的IOC是通过反射来实现的：
在Spring中，通过IOC可以将实现类、参数信息等配置在其对应的配置文件中，当需要更改实现类或参数信息时，只需要修改配置文件即可。
Spring的IOC实现原理利用的就是Java的反射机制，Spring的工厂类会帮我们完成配置文件的读取、利用反射机制注入对象等工作，我们可以通过bean的名称获取对应的对象。

使用XML配置获取Bean的方式：
在Spring框架中，XML配置文件是一种传统的配置方式，用于定义和管理应用程序中的Bean。通过XML配置文件，我们可以定义Bean的属性、构造函数参数、依赖关系以及其他相关配置。
以下是使用XML配置文件在Spring中获取Bean的详细步骤：

创建XML配置文件
首先，需要创建一个XML配置文件，通常命名为applicationContext.xml或beans.xml，并将其放在类路径下（如src/main/resources目录）。

<?xml version="1.0" encoding="UTF-8"?>  
<beans xmlns="http://www.springframework.org/schema/beans"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
       xsi:schemaLocation="http://www.springframework.org/schema/beans  
       http://www.springframework.org/schema/beans/spring-beans.xsd">  
  
    <!-- Bean定义 -->  
    <bean id="myBean" class="com.example.MyBeanClass">  
        <property name="propertyName" value="propertyValue" />  
        <property name="anotherBean" ref="anotherBeanId" />  
    </bean>
</beans>

加载XML配置文件
在Java代码中，需要加载XML配置文件来创建Spring IoC容器。通常使用ClassPathXmlApplicationContext或FileSystemXmlApplicationContext类来加载配置文件。

import org.springframework.context.ApplicationContext;  
import org.springframework.context.support.ClassPathXmlApplicationContext;  
  
public class MyApp {  
    public static void main(String[] args) {  
        ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml");  
          
        // 获取Bean  
        MyBeanClass myBean = (MyBeanClass) context.getBean("myBean");  
          
        // 使用Bean  
        myBean.doSomething();  
    }  
}

在上面的示例中，ClassPathXmlApplicationContext用于加载名为applicationContext.xml的XML配置文件，并创建Spring IoC容器。
然后，通过调用context.getBean("myBean")方法，我们获取了id为myBean的Bean的实例，并将其转型为MyBeanClass类型。最后，我们可以调用Bean的方法来进行业务逻辑处理。

注意：XML配置方式虽然直观且易于理解，但在大型项目中可能会显得繁琐和难以维护。
因此，许多现代Spring项目倾向于使用基于Java的配置（如使用@Configuration和@Bean注解），这样可以更好地利用Java的类型安全性和IDE的支持。
不过，XML配置仍然是一个有效且被广泛使用的选择，特别是对于那些更喜欢XML格式或者需要在没有Java源代码访问权限的情况下进行配置的场景。


使用注解管理Bean的方式：
在Spring框架中，注解（Annotation）提供了一种声明式的方式来管理Bean，而不是传统的基于XML的配置方式。通过使用注解，你可以更简洁、更直接地在Java代码中定义和管理Bean。

以下是在Spring中使用注解管理Bean的一些关键步骤：

启用注解支持：
要在Spring中启用注解支持，你需要在Spring配置文件中添加<context:annotation-config/>标签，或者在Java配置中使用@Configuration和@EnableAnnotationConfiguration注解（如果你使用的是较新版本的Spring，通常只需添加@Configuration注解）。

<!-- 在XML配置中启用注解 -->  
<beans xmlns="http://www.springframework.org/schema/beans"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
       xmlns:context="http://www.springframework.org/schema/context"  
       xsi:schemaLocation="http://www.springframework.org/schema/beans  
       http://www.springframework.org/schema/beans/spring-beans.xsd  
       http://www.springframework.org/schema/context  
       http://www.springframework.org/schema/context/spring-context.xsd">  
  
    <context:annotation-config/>  
  
    <!-- 其他bean定义 -->  
</beans>

或者，在Java配置中：

@Configuration  
public class AppConfig {  
    // ... 其他配置 ...  
}


使用@Component及其衍生注解：
Spring提供了几个注解来标记组件类，使其作为Bean被Spring容器管理。这些注解包括@Component、@Service、@Repository和@Controller。它们都是@Component的特化版本，用于表示不同类型的组件。

@Component：通用组件，可以标记任何Spring管理的组件。
@Service：用于标记业务逻辑组件。
@Repository：用于标记数据访问组件，如DAO。
@Controller：用于标记Web层的组件（在Spring MVC中）。

示例：
@Service  
public class MyService {  
    // ... 服务实现 ...  
}

自动装配：
Spring的@Autowired注解用于自动装配bean之间的依赖关系。Spring容器会自动查找匹配的bean并注入到相应的字段或方法中。

示例：
@Service  
public class MyService {  
    private final MyRepository myRepository;  
  
    @Autowired  
    public MyService(MyRepository myRepository) {  
        this.myRepository = myRepository;  
    }  
}

使用@Qualifier解决歧义：
当存在多个相同类型的bean时，可以使用@Qualifier注解来指定要注入的bean的名称。

示例：
@Autowired  
@Qualifier("specificBeanName")  
private MyBean myBean;

使用@Scope定义作用域：
@Scope注解用于定义bean的作用域。默认的作用域是singleton，但也可以设置为prototype、request、session等。

示例：
@Service  
@Scope("prototype")  
public class MyPrototypeService {  
    // ... 服务实现 ...  
}
使用@Resource和@Inject：
除了@Autowired，你还可以使用JSR-250的@Resource注解或JSR-330的@Inject注解来进行依赖注入。这些注解提供了与@Autowired类似的功能，但具有一些不同的特性和用法。

7. 自定义注解和Bean定义：
你还可以创建自定义的注解，并使用@Bean方法在Java配置类中定义bean。这提供了一种更灵活的方式来定义和注册bean。

8. 使用@ComponentScan扫描组件：
@ComponentScan注解用于指定Spring应该扫描哪个包以查找带有@Component、@Service、@Repository和@Controller注解的类，并将它们注册为bean。你可以将其添加到配置类上。

9. 使用Java配置类替代XML：
除了使用注解标记组件类，你还可以使用Java配置类完全替代XML配置文件。这通过@Configuration类和使用@Bean方法来实现。这种方法提供了更类型安全和更强大的配置能力。


问：Spring里的注解@AutoWired和@Resource注解有什么区别
Spring框架中的@Autowired和@Resource注解都是用于依赖注入的，但它们之间存在一些重要的区别：

>来源不同：@Autowired是Spring框架提供的注解，主要用于实现Spring框架中的依赖注入功能。而@Resource是Java EE规范中的注解，它是JSR-250规范的一部分，因此在Java EE和Spring中都可以使用。
查找顺序不同：当Spring容器进行依赖注入时，@Autowired首先会尝试按类型（byType）查找匹配的bean，如果找到多个同类型的bean，则会根据bean的名称（name）或@Qualifier注解来进一步确定。
而@Resource的查找顺序则相反，它首先会按名称（byName）查找，如果找不到匹配的bean，则会根据类型查找。

>支持的参数不同：@Autowired只有一个可选参数required，表示是否自动装配。如果设置为false，那么在找不到匹配的bean时，不会抛出异常。
而@Resource可以设置更多的参数，包括name、type、authenticationType、mappedName、shareable和lookup等，提供了更丰富的配置选项。

>依赖注入的用法支持不同：@Autowired支持构造方法注入、属性注入和Setter方法注入。而@Resource只支持属性注入和Setter方法注入，不支持构造方法注入。
>IDEA编译器的提示不同：在IntelliJ IDEA等集成开发环境中，当使用@Autowired注解注入Mapper对象时，可能会收到关于找不到bean的警告或错误提示。而使用@Resource注解时，通常不会出现此类问题。

总结来说，@Autowired和@Resource都是Spring中用于实现依赖注入的注解，但它们在来源、查找顺序、支持的参数、用法支持和IDE提示等方面存在区别。
在选择使用哪个注解时，可以根据项目的具体需求和上下文环境进行考虑。


Spring AOP（Aspect-Oriented Programming）是面向切面编程的一种实现技术，主要用于实现程序功能的统一维护。
Spring AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。
Spring AOP的三种使用方式：

注解方式：通过注解的方式配置切入点，然后定义增强处理。
XML方式：通过XML配置文件的方式配置切入点，然后定义增强处理。
Java方式：通过Java配置类的方式配置切入点，然后定义增强处理。


Spring AOP是运用反射来实现的：
Spring AOP即面向切面编程，是Spring核心之一，其可以快速横向扩展。在Spring中，每一个代理实例都有一个关联的调用处理程序对象，它实现了接口InvocationHandler，该接口的invoke方法用于处理代理实例上的方法调用。
在Spring中，当需要使用该Service时，Spring会自动将对应的Bean注入到需要它的地方，这个过程是自动的，对于开发人员来说是透明的，无需手动进行配置。

SpringAOP创建步骤：
1.需要先自定义一个注解
package com.itheima.anno;
import java.lang.annotation.ElementType ;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy
import java.lang.annotation.Target;
@Retention(RetentionPolicy.RUNTIME)
@sarget(ElementType.METHOD)
public @interface Log{

}

2.创建切面类
@Slf4j
@Component
@Aspect //切新类
public class LogAspect {

@Autowired
private HttpServletRequest request;

@Autowired
private perateLogMapper operateLogMapper;

@Around("@annotation(com.itheima.anno.Log)")
public Obiect recordLog(ProceedingdoinPoint joinPoint) throws Throwable {
//操作人ID-当前登录员工ID
//获取请求头中的iwt令牌，解析令牌
String jwt=request.getHeader(name: "token");
Claims claims=JwtUtils.parseuWT(jwt);
Integer operateUser =(Integer)claims.get("id");
//操作时间
LocalDateTime operateTime = LocalDateTime.now();
//操作类名
String className = joinPoint.getTarget().getclass().getName ();
//操作方法名
String methodName = joinPoint.getSignature().getName ();
//操作方法参数
Object result =joinPoint.proceed();
String methodParams =Arrays.tostring(args);
long begin =System.currentTimeMillis();//调用原始目标方法运行
long end=System.currentTimeMillis();
//方法返回值
String returnValue = JsONObject.touSONstring(result);
//操作耗时
Long costTime =end-begin;
//记录操作日志
OperateLog operateLog = new OperateLog( id: null,operateUser,operaoperate,operateTime,className,methodName,methodName,returnValue,costTime)
LogMapper.insert(operateLog);
log.info("AOP记录操作日志:{}"operateLog);
return result;
    }
}
3.在需要的方法上添加注解


SpringMVC：
SpringMVC是一个基于Java的Web应用开发框架，用于构建灵活、可扩展的Web应用程序。它是Spring框架的一部分，提供了一种模型-视图-控制器（MVC）的架构模式，用于将应用程序的不同方面分离开来。
SpringMVC使用了前端控制器（Front Controller）设计模式，其中DispatcherServlet充当中央调度器，负责接收所有的客户端请求并将其分派给相应的处理程序（Controller）。
它还提供了丰富的功能，如请求映射、数据绑定、数据验证、视图解析和国际化支持等，以简化Web应用程序的开发过程。
通过使用SpringMVC，开发人员可以轻松地构建可维护和可扩展的Web应用程序，实现松耦合和高度可测试性。它还与其他Spring框架模块（如Spring IoC和Spring AOP）紧密集成，提供了全面的企业级开发解决方案。

SpringMVC中所有的请求都会封装成ModelAndView

Spring MVC的功能
Spring MVC提供了一种轻度耦合的方式来开发web应用。
Spring MVC是Spring的一个模块，是一个web框架。通过Dispatcher Servlet, ModelAndView 和 View Resolver，开发web应用变得很容易。
解决的问题领域是网站应用程序或者服务开发——URL路由、Session、模板引擎、静态Web资源等等。

在SpringMVC中设置配置文件：
在Spring MVC中，配置文件通常是用来设置各种bean、定义URL映射、配置视图解析器、设置拦截器等。这些配置可以放在XML文件中，或者在现代的Spring Boot应用中，也可以使用Java配置类来替代XML配置。

以下是使用XML配置文件配置Spring MVC的示例：

创建Spring MVC配置文件：
首先，你需要在你的项目资源目录下（通常是src/main/resources）创建一个XML配置文件，例如spring-mvc-servlet.xml。

配置DispatcherServlet：
DispatcherServlet是Spring MVC的核心组件，它负责接收HTTP请求，并将其分发给相应的控制器处理。

<beans xmlns="http://www.springframework.org/schema/beans"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
       xsi:schemaLocation="http://www.springframework.org/schema/beans  
       http://www.springframework.org/schema/beans/spring-beans.xsd  
       http://www.springframework.org/schema/context  
       http://www.springframework.org/schema/context/spring-context.xsd">  
  
    <!-- 配置DispatcherServlet -->  
    <bean class="org.springframework.web.servlet.DispatcherServlet"  
          id="dispatcherServlet">  
        <property name="contextConfigLocation" value="/WEB-INF/spring-mvc-servlet.xml"/>  
    </bean>  
  
    <!-- 配置Servlet映射 -->  
    <bean class="org.springframework.web.servlet.handler.SimpleUrlHandlerMapping">  
        <property name="mappings">  
            <props>  
                <prop key="/hello.do">helloController</prop>  
            </props>  
        </property>  
    </bean>  
  
    <!-- 配置控制器 -->  
    <bean id="helloController" class="com.example.HelloController"/>  
  
    <!-- 配置视图解析器 -->  
    <bean class="org.springframework.web.servlet.view.InternalResourceViewResolver">  
        <property name="prefix" value="/WEB-INF/views/"/>  
        <property name="suffix" value=".jsp"/>  
    </bean>  
  
    <!-- 其他配置，例如拦截器、消息转换器等 -->  
  
</beans>
在web.xml中配置DispatcherServlet
你还需要在web.xml中配置Servlet容器来加载并映射DispatcherServlet。


<web-app ...>  
    <servlet>  
        <servlet-name>dispatcherServlet</servlet-name>  
        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>  
        <init-param>  
            <param-name>contextConfigLocation</param-name>  
            <param-value>/WEB-INF/spring-mvc-servlet.xml</param-value>  
        </init-param>  
        <load-on-startup>1</load-on-startup>  
    </servlet>  
  
    <servlet-mapping>  
        <servlet-name>dispatcherServlet</servlet-name>  
        <url-pattern>*.do</url-pattern>  
    </servlet-mapping>  
</web-app>

在现代的Spring Boot应用中，推荐使用Java配置类来代替XML配置，因为Java配置提供了类型安全、重构支持以及更简洁的配置方式。以下是Java配置类的简单示例：

import org.springframework.context.annotation.ComponentScan;  
import org.springframework.context.annotation.Configuration;  
import org.springframework.web.servlet.config.annotation.EnableWebMvc;  
import org.springframework.web.servlet.config.annotation.ViewResolverRegistry;  
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;  
import org.springframework.web.servlet.view.InternalResourceViewResolver;  
  
@Configuration  
@EnableWebMvc  
@ComponentScan(basePackages = "com.example")  
public class WebConfig implements WebMvcConfigurer {  
  
    @Override  
    public void configureViewResolvers(ViewResolverRegistry registry) {  
        registry.jsp().prefix("/WEB-INF/views/").suffix(".jsp");  
    }  
  
    // 其他配置方法，例如配置拦截器、消息转换器等  
  
}

注：在Spring Boot应用中，通常不需要在web.xml中配置Servlet，因为Spring Boot会自动配置并注册DispatcherServlet。你只需要将Java配置类放在Spring Boot的主配置类所在的包或子包中，或者在主配置类上使用@Import注解来导入Java配置类。

Spring MVC获取请求参数的过程：
1、通过ServletAPI获取
将HttpServletRequest作为控制器方法的形参，此时HttpServletRequest类型的参数表示封装了当前请求的请求报文的对象
@RequestMapping("/testParam")
public String testParam(HttpServletRequest request){
String username = request.getParameter("username");
String password = request.getParameter("password");
System.out.println("username:"+username+",password:"+password);
return "success";
}

2、通过控制器方法的形参获取请求参数
在控制器方法的形参位置，设置和请求参数同名的形参，当浏览器发送请求，匹配到请求映射时，在DispatcherServlet中就会将请求参数赋值给相应的形参

<a th:href="@{/testRest/1/admin}">测试路径中的占位符-->/testRest</a><br>
@RequestMapping("/testRest/{id}/{username}")

@RequestMapping("/testParam")
public String testParam(String username, String password){
System.out.println("username:"+username+",password:"+password);
return "success";
}


注：
若请求所传输的请求参数中有多个同名的请求参数，此时可以在控制器方法的形参中设置字符串
数组或者字符串类型的形参接收此请求参数
若使用字符串数组类型的形参，此参数的数组中包含了每一个数据
若使用字符串类型的形参，此参数的值为每个数据中间使用逗号拼接的结果

3、@RequestParam
@RequestParam是将请求参数和控制器方法的形参创建映射关系
@RequestParam注解一共有三个属性：
value：指定为形参赋值的请求参数的参数名
required：设置是否必须传输此请求参数，默认值为true
若设置为true时，则当前请求必须传输value所指定的请求参数，若没有传输该请求参数，且没有设置
defaultValue属性，则页面报错400：Required String parameter 'xxx' is not present；若设置为
false，则当前请求不是必须传输value所指定的请求参数，若没有传输，则注解所标识的形参的值为
null
defaultValue：不管required属性值为true或false，当value所指定的请求参数没有传输或传输的值
为""时，则使用默认值为形参赋值

4、@RequestHeader
@RequestHeader是将请求头信息和控制器方法的形参创建映射关系
@RequestHeader注解一共有三个属性：value、required、defaultValue，用法同@RequestParam

5、@CookieValue
@CookieValue是将cookie数据和控制器方法的形参创建映射关系
@CookieValue注解一共有三个属性：value、required、defaultValue，用法同@RequestParam

6、通过POJO获取请求参数
可以在控制器方法的形参位置设置一个实体类类型的形参，此时若浏览器传输的请求参数的参数名和实体类中的属性名一致，那么请求参数就会为此属性赋值

7、解决获取请求参数的乱码问题
解决SpringMVC中的乱码问题通常涉及到对请求和响应的字符编码进行统一设置。通过配置过滤器、SpringMVC消息转换器或者在Controller中指定编码，可以有效地避免乱码问题的出现。同时，也需要确保客户端和数据库连接使用的字符集与服务器端设置一致。

Spring MVC的工作流程：
1.客户端发送请求：当客户端（如浏览器）发起一个HTTP请求时，它会通过URL地址和请求方法（如GET、POST等）来向服务器发送请求。
2.DispatcherServlet接收请求：在Spring MVC中，所有的请求都会先经过一个称为DispatcherServlet的中央控制器。当请求到达服务器后，Servlet容器会将请求交给DispatcherServlet进行处理。
3.HandlerMapping寻找处理器：DispatcherServlet会调用HandlerMapping来确定请求对应的处理器（Handler）。HandlerMapping会根据请求的URI等信息，找到匹配的Controller。
4.Controller处理请求：一旦确定了请求对应的处理器，DispatcherServlet就会将请求转发给目标Controller。Controller会执行相应的业务逻辑，并返回一个ModelAndView对象。这个对象包含了视图需要的数据以及要使用的视图名称。
5.ModelAndView处理：DispatcherServlet接收到Controller返回的ModelAndView对象后，会进一步处理。它会提取Model中的数据，准备用于视图的渲染。
6.视图解析：DispatcherServlet通过ViewResolver视图解析器找到对应的视图对象View。ViewResolver会根据ModelAndView对象中的视图名称，解析出具体的视图实现。
7.视图渲染：视图对象View负责渲染模型数据，生成最终的响应内容。这通常涉及到将模型数据填充到视图的模板中，生成HTML、JSON或其他格式的响应。
8.返回响应给客户端：视图渲染完成后，DispatcherServlet会将响应返回给客户端，客户端（如浏览器）会接收到这个响应并展示给用户。

以下组件通常使用框架提供实现：
DispatcherServlet：作为前端控制器，整个流程控制的中心，控制其它组件执行，统一调度，降低组件之间的耦合性，提高每个组件的扩展性。
HandlerMapping：通过扩展处理器映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。
HandlAdapter：通过扩展处理器适配器，支持更多类型的处理器。
ViewResolver：通过扩展视图解析器，支持更多类型的视图解析，例如：jsp、freemarker、pdf、excel等。


域对象共享数据
1、使用ServletAPI向request域对象共享数据
<form th:action="@{/testpojo}" method="post">
用户名：<input type="text" name="username"><br>
密码：<input type="password" name="password"><br>
性别：<input type="radio" name="sex" value="男">男<input type="radio"
name="sex" value="女">女<br>
年龄：<input type="text" name="age"><br>
邮箱：<input type="text" name="email"><br>
<input type="submit">
</form>
@RequestMapping("/testpojo")
public String testPOJO(User user){
System.out.println(user);
return "success";
}
//最终结果-->User{id=null, username='张三', password='123', age=23, sex='男',
email='123@qq.com'}
<!--配置springMVC的编码过滤器-->
<filter>
<filter-name>CharacterEncodingFilter</filter-name>
<filterclass>org.springframework.web.filter.CharacterEncodingFilter</filter-class>
<init-param>
<param-name>encoding</param-name>
<param-value>UTF-8</param-value>
</init-param>
<init-param>
<param-name>forceResponseEncoding</param-name>
<param-value>true</param-value>
</init-param>
</filter>
<filter-mapping>
<filter-name>CharacterEncodingFilter</filter-name>
<url-pattern>/*</url-pattern>
</filter-mapping>
注：
SpringMVC中处理编码的过滤器一定要配置到其他过滤器之前，否则无效

总结：
SpringMVC本质上是对Java Web Servlet的封装，使得开发Java Web项目没有那么臃肿。
SpringMVC是后端渲染式的后台开发框架，没有做到前后端分离，对前端开发人员也要求有Java Web的开发环境。
前端人员作出的静态Html页面，也需要进行整合，改为Spring MVC里的View。
	


SpringBoot：
Spring Boot 是由 Pivotal 团队提供的用于简化 Spring 应用开发的框架。它通过默认配置、自动化配置和内嵌服务器的支持，大幅减少了开发者配置和部署 Spring 应用的复杂度。

主要特点
简化配置：Spring Boot 通过自动化配置减少了大量的 XML 配置和繁琐的 Java 配置。
快速启动：通过内嵌的 Tomcat、Jetty 或 Undertow 等服务器，能够快速启动并运行 Spring 应用。
生产准备：提供了内置的监控、度量和外部化配置等特性，适合生产环境使用。
依赖管理：通过使用 Spring Boot Starter，可以方便地引入相关依赖包。
自动化配置：根据项目中的依赖自动配置 Spring 应用。

核心组件
Spring Boot Starter：一组方便的依赖描述符，帮助你快速开始。
Spring Boot Auto Configuration：自动配置 Spring 应用。
Spring Boot CLI：命令行工具，帮助你用 Groovy 快速开发 Spring Boot 应用。
Spring Boot Actuator：提供了监控和管理功能，如健康检查、度量指标、信息、环境等。

Spring Boot的功能及实现原理:
Spring Boot通过自动配置简化了Spring框架的使用，它并没有提供Spring框架的新功能，而是极大地减少了项目配置的工作量。Spring Boot与Spring框架紧密集成，旨在提升开发者的开发体验。
它集成了众多常用的第三方库（如Jackson、JDBC、Mongo、Redis、Mail等），这些库在Spring Boot项目中几乎可以做到开箱即用，无需过多配置。因此，开发者可以更加专注于业务逻辑的实现，而不是配置细节。
对于Web项目，Spring Boot通常与Spring MVC结合使用，但工作流程和Spring MVC保持一致，因为业务逻辑处理是由Spring MVC负责的，而不是Spring Boot。
使用Spring Boot后，项目的初始化、配置以及部署方式发生了变化。例如，不再需要单独安装Tomcat等容器服务器，只需通过Maven或Gradle等工具打包成jar包，就可以直接运行网站。然而，这些变化并不影响业务逻辑和流程的实现。

SpringBoot的配置机制：
>自动配置：Spring Boot 通过自动配置机制，根据项目中的依赖关系和条件，自动地创建和配置应用程序所需的组件。这种机制大大简化了配置过程，减少了手动配置的工作量。
>默认配置：Spring Boot 提供了大量的默认配置，这些配置适用于大多数应用场景。通过使用默认配置，您可以轻松地启动应用程序，而无需编写过多的配置文件。
>自定义配置：如果默认配置不能满足您的需求，您可以根据需要自定义配置。Spring Boot 支持使用 application.properties 或 application.yml 文件来定义自定义配置。您可以在这些文件中设置自己的属性，以满足特定应用程序的需求。
>外部化配置：Spring Boot 支持将配置文件放置在项目外部，以便于在不同的环境中使用相同的配置。您可以将配置文件放在项目的根目录或 classpath 根目录之外的位置，然后在应用程序中通过设置配置文件的路径来加载它们。
>热重载配置：Spring Boot 支持配置文件的热重载，这意味着当您更改配置文件时，应用程序将自动重新加载配置，而无需重启整个应用程序。这使得开发过程更加高效，因为您无需等待应用程序重新启动就可以看到配置更改的效果。
>命令行参数：Spring Boot 还支持通过命令行参数来覆盖配置文件中的属性值。这使得您可以在启动应用程序时指定特定的属性值，以便进行测试或生产环境的差异化配置。


SpringBoot的启动方法：
@SpringBootApplication 是 Spring Boot 应用的启动类上常用的注解，它结合了多个核心注解，简化了 Spring Boot 应用的配置。这个注解是由以下三个注解组合而成的：

1. @SpringBootConfiguration
2. @EnableAutoConfiguration
3. @ComponentScan

详细解析@SpringBootApplication：

package org.springframework.boot.autoconfigure;

import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.ComponentScan;
import org.springframework.boot.SpringBootConfiguration;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;

import java.lang.annotation.*;

@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(excludeFilters = {
    @ComponentScan.Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),
    @ComponentScan.Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class)
})
public @interface SpringBootApplication {
    // Additional attributes omitted for brevity
}
```

组成注解的详细解析

1. @SpringBootConfiguration

这个注解是 @Configuration的一个特化，表明这是一个 Spring 配置类。


package org.springframework.boot;
import org.springframework.context.annotation.Configuration;
import java.lang.annotation.*;

@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Configuration
public @interface SpringBootConfiguration {
}
```

@Configuration 本身是 Spring 的核心注解之一，表示一个类可以使用 Spring IoC 容器作为 bean 定义的来源。@SpringBootConfiguration 本质上是一个带有@Configuration 注解的类，这意味着它可以定义 Spring beans。

2. @EnableAutoConfiguration

@EnableAutoConfiguration 告诉 Spring Boot 根据应用程序的类路径设置、其他 bean 和各种属性配置自动配置 Spring 应用上下文。这是 Spring Boot 自动配置机制的核心注解。


package org.springframework.boot.autoconfigure;

import org.springframework.context.annotation.Import;
import org.springframework.boot.autoconfigure.condition.ConditionalOnClass;
import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;

import java.lang.annotation.*;

@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@AutoConfigurationPackage
@Import(AutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration {
    Class<?>[] exclude() default {};

    String[] excludeName() default {};
}
```

- @AutoConfigurationPackage：自动配置包，标记启动类所在包及其子包作为自动配置扫描包。
- @Import(AutoConfigurationImportSelector.class)：导入 AutoConfigurationImportSelector，该类实现了自动配置的核心逻辑。

3. @ComponentScan
@ComponentScan 告诉 Spring 在启动类的包及其子包中查找其他组件、配置和服务，允许 Spring 自动扫描和注册这些组件。


package org.springframework.context.annotation;

import java.lang.annotation.*;

@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Repeatable(ComponentScans.class)
public @interface ComponentScan {
    String[] value() default {};
    String[] basePackages() default {};
    Class<?>[] basePackageClasses() default {};
    boolean lazyInit() default false;
}
```

@ComponentScan 默认扫描启动类所在包及其子包中的所有组件（如 @Component、@Service、@Repository 和 @Controller 等），这意味着您通常会将启动类放在根包中，以便扫描整个项目的组件。



Spring Boot 启动类：
package com.example.demo;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class DemoApplication {

    public static void main(String[] args) {
        SpringApplication.run(DemoApplication.class, args);
    }
}

在这个示例中，@SpringBootApplication 注解使得 DemoApplication 类成为 Spring Boot 应用的启动类。通过调用 SpringApplication.run(DemoApplication.class, args)，Spring Boot 框架将自动执行以下步骤：
 
1. 创建 Spring 应用上下文（ApplicationContext）。
2. 注册并启动所有@ComponentScan 扫描到的 bean。
3. 根据 @EnableAutoConfiguration 注解自动配置 Spring 应用上下文。
4. 执行任何 ApplicationRunner 或 CommandLineRunner bean。

总结
@SpringBootApplication 是一个复合注解，结合了 @SpringBootConfiguration、@EnableAutoConfiguration 和 @ComponentScan，简化了 Spring Boot 应用的配置。通过使用这个注解，开发者可以快速配置和启动一个 Spring Boot 应用，而无需编写大量的配置代码。


用Spring Boot解释一下约定大于配置大于编码：
"约定大于配置大于编码" 是一个设计原则，它强调了在设计软件时，应该优先考虑使用通用的、广泛接受的约定，其次是配置选项，最后才是直接编写代码。这个原则旨在提高软件的可维护性、可读性和可扩展性。在Spring Boot框架中，这个原则得到了很好的体现。

约定
在Spring Boot中，很多常见的配置和编码工作都通过约定来简化。例如：

项目结构约定：Spring Boot约定了项目的标准结构，例如src/main/java用于存放Java源代码，src/main/resources用于存放配置文件等。
端口号约定：默认情况下，Spring Boot应用会监听8080端口。除非有特别的需求，否则开发者无需手动配置端口。
日志约定：Spring Boot使用Logback作为默认的日志框架，并提供了默认的日志配置。
这些约定让开发者可以快速地开始一个新的Spring Boot项目，而无需花费太多时间在配置和编码上。

配置
当默认的约定不满足需求时，Spring Boot提供了丰富的配置选项。这些配置可以通过多种方式实现：

application.properties 或 application.yml：这是Spring Boot中最常用的配置方式。开发者可以在这些文件中定义各种属性，如数据库连接信息、服务器端口等。
环境变量：Spring Boot也支持通过环境变量来配置应用。
命令行参数：在启动应用时，可以通过命令行参数来覆盖配置文件中的设置。
通过这些配置选项，开发者可以灵活地调整应用的行为，而无需修改代码。

编码
当然，有些复杂的功能或定制化的需求可能无法通过约定或配置来实现，这时就需要编写代码了。Spring Boot提供了丰富的API和扩展点，使得开发者可以轻松地实现各种功能。
但即使需要编码，Spring Boot也尽可能地通过注解、自动配置等方式简化了编码的复杂性。

总结
在Spring Boot中，"约定大于配置大于编码"原则的应用使得开发者可以更加高效地构建应用。通过遵循通用的约定和利用强大的配置功能，开发者可以快速地搭建起一个功能完善的应用，
并在需要时通过编码来实现更高级的功能。这种设计方式不仅提高了开发效率，也增强了应用的可维护性和可读性。


SpringBoot的自动配置：
Spring Boot的自动配置是其核心功能之一，它极大地简化了Spring应用程序的初始搭建和配置过程。
自动配置的核心思想是根据项目中的依赖和类路径下的特定类，自动创建和配置Spring容器中的bean，从而省去了手动配置的大量工作。

以下是Spring Boot自动配置的主要原理和实现方式：
>条件化配置：Spring Boot通过条件注解（如@ConditionalOnClass、@ConditionalOnMissingBean等）来决定是否加载某个自动配置类或bean。
这些条件可以是类路径下是否存在某个类、是否存在某个bean等。只有当满足特定条件时，相关的自动配置才会生效。

>自动配置类：自动配置类通过@Configuration注解标识，它们包含了Spring容器需要自动创建的bean的定义。这些bean的创建和配置会根据条件化注解的条件来确定。
自动配置类通常存放在spring-boot-autoconfigure模块的jar包中。

>META-INF/spring.factories文件：Spring Boot通过在类路径下的META-INF/spring.factories文件中定义自动配置类，告诉Spring Boot哪些配置类需要被加载。
这个文件是一个属性文件，其中包含了自动配置类的全限定名。

>属性绑定：Spring Boot还允许开发者在application.properties或application.yml文件中定义外部化配置，这些配置会被自动绑定到相关的bean上。
这大大简化了配置的复杂性，使得开发者能够更专注于业务逻辑的实现。

>Starter依赖：Spring Boot提供了一系列的“Starter”依赖，这些依赖包含了构建某个特定类型应用程序所需的所有基本依赖和自动配置。
例如，spring-boot-starter-web包含了构建Web应用程序所需的Spring MVC和嵌入式Servlet容器等依赖和自动配置。


问：@SpringBootApplication注解是如何完成自动配置功能的？
@SpringBootApplication注解是SpringBoot中的一个复合注解，它包含了@Configuration、@EnableAutoConfiguration和@ComponentScan三个关键注解。
这个注解的作用是启用SpringBoot的自动配置功能，并告诉Spring Boot如何扫描和加载应用程序的组件。

下面我们来详细解析@SpringBootApplication注解是如何完成自动配置功能的：

@Configuration：
这个注解用于告诉Spring这是一个配置类，类似于XML配置文件中的<beans>标签。在这个类中，开发者可以使用@Bean注解来定义和初始化Spring容器管理的bean。

@EnableAutoConfiguration：
这个注解是自动配置的关键所在。当使用@EnableAutoConfiguration注解时，Spring Boot会根据项目中引入的依赖和类路径下的特定类，自动配置应用程序所需的bean。

条件注解：Spring Boot使用条件注解（如@ConditionalOnClass、@ConditionalOnMissingBean等）来决定是否创建和配置某个bean。这些条件可以是类路径上是否存在某个类、是否存在某个bean等。
自动配置类：Spring Boot提供了许多自动配置类，这些类包含了一些预定义的bean，并且会根据项目的条件进行创建和配置。
META-INF/spring.factories文件：自动配置类通过META-INF/spring.factories文件进行注册。Spring Boot在启动时，会扫描这个文件，并加载其中定义的自动配置类。

@ComponentScan：
这个注解用于告诉Spring Boot哪些包下的类需要被扫描并注册为Spring容器管理的bean。默认情况下，@SpringBootApplication注解会扫描启动类所在的包及其子包。

综合这三个注解，@SpringBootApplication完成了以下功能：

通过@Configuration定义了配置类。
通过@EnableAutoConfiguration启用了自动配置功能，根据项目的依赖和条件自动创建和配置bean。
通过@ComponentScan扫描并注册应用程序的组件。

当开发者在Spring Boot应用程序的主类上使用@SpringBootApplication注解时，Spring Boot会自动完成上述所有步骤，从而简化了应用程序的配置和初始化过程。
开发者只需要关注业务逻辑的实现，而不需要花费大量时间在繁琐的配置上。


SpringBoot中定制Web扩展配置：WebMvcConfigurerAdapter接口
这是一个接口，里边声明了很多的方法，例如拦截器，视图解析器，格式转化器。
如果想在webmvc中添加自己的功能，只需要去WebMvcConfigurer看看想增加什么功能，然后按照源码去写。

Spring与SpringBoot中一些常用注解：
@SpringBootApplication：实现自动化装配，启动SpringBoot，包含@Configuration、@EnableAutoConfiguration、@ComponentScan通常用在主类上
@Component：普通pojo注入spring容器
@Service：Service注入Spring容器
@Controller：Controller注入容器
@Repository：Dao注入容器
@ResponseBody：将java对象转为json格式的数据
@RestController：该注解是@Controller和@ResponseBody的结合体，一般用于类，作用等于在类上面添加了@ResponseBody和@Controller
@AutoWired：默认按类型装配，如果发现找到多个bean，则按照name方式比对，如果还有多个，则报出异常
@Qualifier：spring的注解，按名字注入 一般当出现两个及以上bean时,不知道要注入哪个，结合@AutoWired使用
@Resource：默认按名称注入例如@Resource(name = “zhaozhao”)则根据name属性注入找不到则报错，若无name属性则根据属性名称注入，如果匹配不成功则按照类型匹配匹配不成功则报错。
@RequestMapping：@RequestMapping（url），通过该注解就可以通过配置的url进行访问，方式可以是get或post请求，两种方式均可
@GetMapping：只是这个限定了只能是Get请求
@PostMapping：只是这个限定了只能是Post请求
@Value：用于获取bean的属性，一般用于读取配置文件的数据，作用在变量上 
@ConfigurationProperties：用于注入Bean属性，然后再通过当前Bean获取注入值，作用在类上 
@PropertySource：用于指定要读取的配置文件，可以和@Value或@ConfigurationProperties配合使用
@Configuration：作用于类上表示这是一个配置类，可理解为用spring的时候xml里面的< beans>标签 
@Bean：产生bean对象加入容器，作用于方法，可理解为用spring的时候xml里面的标签
@RequestParam：获取查询参数。即url?name=这种形式
@RequestBody：获取Body的参数，一般用于post获取参数 
@PathVariable：获取路径参数。即url/{id}这种形式 
@RequestHeader：获取请求头的信息 
@CookieValue：获取Cookie的信息

Spring Boot的启动类实现方式：
1、程序从main方法开始运行
2、使用SpringApplication.run()加载主程序类
3、主程序类需要标注@SpringBootApplication
4、@EnableAutoConfiguration是核心注解；
5、@Import导入所有的自动配置场景
6、@AutoConfigurationPackage定义默认的包扫描规则
7、程序启动扫描加载主程序类所在的包以及下面所有子包的组件；

Spring Boot使用的全局配置文件：
>application.properties
>application.yml

配置文件放在src/main/resources目录或者类路径/config下
.yml是YAML(YAML Aint Markup Language)编写的文件格式，YAML是一种直观的能够被电脑识别的的数据数据序列化格式，比json、xml等更适合做配置文件


Java程序通过设计模式、数据结构与算法等技术组成框架

设计模式：
设计模式（Design Patterns）是在软件设计中针对特定问题的最佳解决方案。它们是在软件开发过程中，对常见问题的反复出现的解决方案的总结与抽象。
这些模式描述了在特定上下文中，针对某类问题的一组相互协作的类、接口和通信机制，用于解决某一类通用设计问题。
设计模式是经验的总结，是软件开发人员在长期开发过程中，针对某一类问题的解决方案的抽象和提炼。
使用设计模式可以帮助我们提高代码的可重用性、可维护性和可扩展性，同时降低系统的复杂度，提高软件开发的效率和质量。

设计模式的目的是提供一套可复用的设计问题的解决方案，提高代码的可维护性、可重用性和可读性。具体来说，设计模式有以下几个主要目的：
1.代码重用：设计模式提供了一种抽象化和标准化的方式，使开发者能够针对常见的设计问题重用已有的解决方案，而无需每次都从头开始设计。
2.使代码易于理解：设计模式为软件设计提供了通用的术语和视觉表示，使得其他开发者能够更容易地理解代码的结构和意图。
3.提高软件的可维护性：通过使用设计模式，代码的结构更加清晰，各部分之间的耦合度降低，使得软件更易于维护和扩展。
4.促进团队协作：团队成员之间共享对设计模式的理解和经验，可以促进团队之间的沟通和协作，提高开发效率。
5.系统灵活性：设计模式有助于构建一个灵活且可扩展的系统，能够应对不断变化的需求。


设计模式的七大原则：
开闭原则：
软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。
对扩展开放：意味着软件实体应该允许在不修改原有代码的情况下，通过添加新功能来扩展其行为。
对修改关闭：意味着一旦软件实体被开发并投入使用后，应该尽量避免对其进行修改，尤其是修改核心业务逻辑。

单一指责原则：一个类只做一件事。
该原则强调一个类或者一个模块应该只有一个引起变化的原因，也就是说，它应该只负责一项职责。
当一个类或模块承担过多的职责时，它的可维护性和可读性都会受到影响，因为任何一个职责的变化都可能导致整个类或模块需要重新设计或修改。

依赖倒置原则：
高层模块不应该依赖低层模块，两者都应该依赖其抽象。
抽象不应该依赖细节，细节应该依赖抽象。

这意味着在软件设计中，我们应该尽量通过接口或抽象类去定义类的行为，而不是直接依赖于具体的实现类。这样，当具体实现发生变化时，只要它们仍然遵循接口或抽象类的约定，高层模块就不需要修改，从而提高了系统的灵活性和可维护性。

依赖倒置原则在Java里要遵循以下规范：
1. 依赖于抽象，而不是依赖于具体类
高层模块（业务逻辑）不应该依赖于低层模块（具体实现），而应该依赖于抽象（接口或抽象类）。低层模块也应该依赖于这些抽象。

示例代码：
interface PaymentProcessor {
    void processPayment(double amount);
}

class PayPalProcessor implements PaymentProcessor {
    @Override
    public void processPayment(double amount) {
        // PayPal payment logic
    }
}

class ShoppingCart {
    private PaymentProcessor paymentProcessor;

    public ShoppingCart(PaymentProcessor paymentProcessor) {
        this.paymentProcessor = paymentProcessor;
    }

    public void checkout(double amount) {
        paymentProcessor.processPayment(amount);
    }
}

在这个例子中，ShoppingCart 依赖于 PaymentProcessor 接口，而不是 PayPalProcessor 具体类。

2. 高层模块不应该直接创建低层模块的实例
高层模块不应该直接实例化低层模块的对象。这可以通过依赖注入（Dependency Injection, DI）来实现，即通过构造函数、setter 方法，或依赖注入框架（如 Spring）传递依赖对象。

示例代码：
class ShoppingCart {
    private PaymentProcessor paymentProcessor;

    public ShoppingCart(PaymentProcessor paymentProcessor) {
        this.paymentProcessor = paymentProcessor;
    }
}

public static void main(String[] args) {
    PaymentProcessor processor = new PayPalProcessor(); // 创建具体实现
    ShoppingCart cart = new ShoppingCart(processor); // 注入依赖
    cart.checkout(100.0);
}

这里，ShoppingCart 类并不直接创建 PayPalProcessor 对象，而是通过依赖注入来获得它。

3. 遵循接口分离原则（ISP）
接口应该尽可能小，避免臃肿。这样可以确保依赖于接口的类不会被迫实现它们不需要的方法。通过精简的接口，每个模块只依赖于它所需要的功能。

示例代码：
interface Printer {
    void print(Document doc);
}

interface Scanner {
    void scan(Document doc);
}

class MultiFunctionPrinter implements Printer, Scanner {
    @Override
    public void print(Document doc) {
        // Print logic
    }

    @Override
    public void scan(Document doc) {
        // Scan logic
    }
}

Printer 和 Scanner 是两个独立的接口，遵循了接口分离原则。

4. 使用依赖注入框架
在实际开发中，使用依赖注入框架（如 Spring、Guice）可以更容易地遵循依赖倒置原则。框架会自动管理对象的创建和依赖关系，使代码更加松耦合。

示例代码：
@Service
public class ShoppingCartService {
    private final PaymentProcessor paymentProcessor;

    @Autowired
    public ShoppingCartService(PaymentProcessor paymentProcessor) {
        this.paymentProcessor = paymentProcessor;
    }

    public void checkout(double amount) {
        paymentProcessor.processPayment(amount);
    }
}

在这个示例中，ShoppingCartService 中的 PaymentProcessor 依赖通过 Spring 框架自动注入，减少了直接依赖具体实现的风险。

5. 避免静态方法和静态依赖
静态方法和依赖于静态类的设计会破坏依赖倒置原则，因为它们使高层模块依赖于具体实现，难以替换或测试。因此，尽量避免使用静态依赖。

示例代码：
class FileLogger {
    public static void log(String message) {
        // Log message to file
    }
}

class BusinessService {
    public void doBusinessLogic() {
        FileLogger.log("Business logic executed."); // 违反依赖倒置原则
    }
}

在这个示例中，BusinessService 依赖于 FileLogger 的静态方法，无法轻易替换或测试。

迪米特原则：
设计模式中的迪米特原则（Law of Demeter，简称LoD），又称为最少知识原则（Least Knowledge Principle，简称LKP），是面向对象设计中的一个重要原则。
该原则的核心思想是尽量减少一个对象对另一个对象内部结构的了解，即一个对象应该对其他对象有最少的了解。这样做的目的是降低类之间的耦合度，提高模块的相对独立性，从而提高系统的可维护性和可扩展性。

定义与解释
迪米特原则要求一个类应该对自己需要耦合或调用的类知道得最少，被调用或耦合的类的内部是如何复杂都与我无关。换句话说，一个类只需要知道其他类提供的公共方法（public methods），而不需要知道这些方法的内部实现细节。

核心观念
类间解耦：迪米特原则强调类之间的低耦合，即减少类之间的直接依赖关系。
只和朋友交流：这里的“朋友”指的是与当前对象有直接关联的对象，如成员变量、方法的输入输出参数等。一个对象应该只与它的“朋友”通信，而不应该与“陌生人”通信。


实现方式
减少公共方法和属性：在设计类时，应该尽量减少类的公共方法和属性，只暴露必要的接口给外部使用。
使用中介类：如果两个类之间需要通信，但又不想直接建立依赖关系，可以通过引入中介类来实现间接通信。
封装：通过封装将类的内部实现细节隐藏起来，只提供公共的接口供外部调用。


优缺点
优点：
降低耦合度：减少了类之间的直接依赖关系，提高了系统的灵活性和可维护性。
提高复用性：由于类之间的耦合度降低，类的复用性得到提高。
提高扩展性：在需要添加新功能或修改现有功能时，可以更容易地进行扩展和维护。
缺点：
增加系统复杂度：过度使用迪米特原则可能会导致系统中出现大量的中介类，从而增加系统的复杂性和理解难度。
降低通信效率：由于类之间的通信需要通过中介类进行转发，可能会降低系统的通信效率。

示例代码：
// Book 类，代表书籍  
class Book {  
    private String title;  
  
    public Book(String title) {  
        this.title = title;  
    }  
  
    public String getTitle() {  
        return title;  
    }  
  
    // 其他书籍相关的方法...  
}  
  
// Library 类，代表图书馆（在这个示例中，我们主要用它来存储书籍）  
class Library {  
    private List<Book> books = new ArrayList<>();  
  
    public void addBook(Book book) {  
        books.add(book);  
    }  
  
    // 获取所有书籍的方法（实际中可能不会这么直接暴露，但为了示例）  
    public List<Book> getBooks() {  
        return books;  
    }  
  
    // 其他图书馆管理的方法...  
}  
  
// Librarian 类，图书管理员，作为中介类  
class Librarian {  
    private Library library;  
  
    public Librarian(Library library) {  
        this.library = library;  
    }  
  
    // 读者请求借书，图书管理员处理  
    public Book borrowBook(String title) {  
        for (Book book : library.getBooks()) {  
            if (book.getTitle().equals(title)) {  
                // 假设这里有一个借书逻辑，比如从列表中移除书籍  
                // 但为了简化，我们只是返回书籍对象  
                return book;  
            }  
        }  
        return null; // 没有找到书籍  
    }  
  
    // 其他图书管理的方法...  
}  
  
// Reader 类，代表读者  
class Reader {  
    private Librarian librarian;  
  
    public Reader(Librarian librarian) {  
        this.librarian = librarian;  
    }  
  
    // 读者借书，通过图书管理员进行  
    public void borrowBook(String title) {  
        Book book = librarian.borrowBook(title);  
        if (book != null) {  
            System.out.println("Borrowed: " + book.getTitle());  
        } else {  
            System.out.println("Book not found.");  
        }  
    }  
  
    // 读者其他与书籍交互的方法...  
}  
  
// 主类，用于演示  
public class Demo {  
    public static void main(String[] args) {  
        Library library = new Library();  
        library.addBook(new Book("Java Programming"));  
        library.addBook(new Book("Design Patterns"));  
  
        Librarian librarian = new Librarian(library);  
        Reader reader = new Reader(librarian);  
  
        reader.borrowBook("Java Programming");  
        reader.borrowBook("Non-Existent Book");  
    }  
}
在这个示例中，Reader类不需要直接知道Book类的内部结构，也不需要直接与Library类交互。相反，它通过Librarian类（图书管理员）来借书，这符合迪米特原则的要求。
Librarian类作为中介，封装了与Library和Book的交互细节，从而降低了Reader与这些类之间的耦合度。


接口隔离原则：
接口隔离原则是一种面向对象设计原则，其核心思想是客户端不应该依赖它不需要的接口，或者说一个类对另一个类的依赖应该建立在最小的接口上。
这一原则鼓励将庞大的接口拆分成更小的、更具体的接口，使得客户端（使用接口的类）只依赖于它实际需要的接口方法。这样做的好处包括降低客户端与接口之间的耦合度，提高系统的灵活性、可扩展性和可维护性。

接口隔离原则的具体要求
最小接口：一个类对另一个类的依赖应该建立在最小的接口上。
细化接口：尽量细化接口，避免接口中的方法过多，接口中的方法应该尽量少（但也不是越少越好，要适度）。
高聚合、低耦合：符合“高聚合、低耦合”的设计思想，使得类具有很好的可读性、可扩展性和可维护性。


合成复用原则：
设计模式中的合成复用原则（Composite/Aggregate Reuse Principle, CARP），又称为组合/聚合复用原则，是面向对象设计中的一个重要原则。其核心思想是尽量使用对象组合（Composition）或聚合（Aggregation）的方式，而不是使用继承（Inheritance）来达到复用的目的。这一原则有助于构建更加灵活、松耦合的系统，降低类与类之间的依赖关系，从而提高系统的可维护性和可扩展性。

定义：尽量使用对象组合/聚合，而不是继承来达到复用的目的。这意味着在新的对象里通过关联关系（包括组合关系和聚合关系）来使用一些已有的对象，使之成为新对象的一部分；新对象通过委派调用已有对象的方法达到复用功能的目的。

合成复用原则的优点
降低耦合度：组合/聚合关系比继承关系更加灵活，降低了类与类之间的耦合度，一个类的变化对其他类造成的影响相对较少。
提高系统的可扩展性：通过组合/聚合，可以很容易地添加新的功能或替换已有的功能，而不需要修改原有的类结构。
增强复用性：组合/聚合复用允许在不破坏原有类结构的前提下，通过组合不同的对象来复用代码。

合成复用原则的应用场景
当两个类之间存在“Has-A”（有一个）关系时，应优先考虑使用组合或聚合。例如，一个班级（Class）有多个学生（Student），这里班级和学生之间就是组合关系。
当需要复用已有类的功能，但又不想暴露其实现细节时，可以使用组合/聚合来隐藏实现细节，实现黑箱复用。
在设计系统时，如果发现类之间的继承关系过于复杂，导致系统难以理解和维护，可以考虑使用组合/聚合来重构系统。

示例说明
假设有一个Vehicle（交通工具）类和一个Engine（发动机）类。如果我们想让Vehicle类具有发动机的功能，但不希望Vehicle直接继承自Engine（因为这样做会破坏类的封装性，并且可能导致Vehicle类继承到一些不需要的属性和方法），我们可以使用组合/聚合的方式来实现。

// 发动机类  
public class Engine {  
    public void start() {  
        System.out.println("发动机启动");  
    }  
}  
  
// 交通工具类，使用组合方式复用发动机功能  
public class Vehicle {  
    private Engine engine;  
  
    public Vehicle(Engine engine) {  
        this.engine = engine;  
    }  
  
    public void start() {  
        engine.start(); // 委派调用发动机的方法  
        System.out.println("交通工具启动");  
    }  
}  
  
// 使用示例  
public class Main {  
    public static void main(String[] args) {  
        Engine engine = new Engine();  
        Vehicle car = new Vehicle(engine);  
        car.start(); // 输出：发动机启动，交通工具启动  
    }  
}
在这个示例中，Vehicle类通过组合的方式复用了Engine类的功能，而没有使用继承。这样做的好处是Vehicle类不需要关心Engine类的具体实现细节，只需要知道如何使用它即可。
同时，这也使得系统更加灵活和可扩展，例如我们可以很容易地替换Engine类的实现或添加新的功能到Vehicle类中。


里氏替换原则：
里氏替换原则是指任何基类（父类）可以出现的地方，子类一定可以出现。换言之，所有引用基类的地方必须能透明地使用其子类的对象。这一原则强调了子类对象对基类对象的可替换性，从而保证了程序的稳定性和可维护性。

主要内容
子类必须实现父类的抽象方法：当父类有抽象方法时，子类必须实现这些方法，以保证子类能够替代父类完成抽象方法所声明的功能。
子类不得重写父类的非抽象方法：子类不应该改变父类已有的功能（即不应该重写父类的非抽象方法），以保持父类行为的稳定性和一致性。如果子类需要修改父类的行为，应该通过添加新的方法或重写父类的抽象方法来实现。
子类中可以增加自己特有的方法：子类可以在继承父类的基础上，增加自己特有的属性和方法，以扩展父类的功能。这是面向对象多态性的体现，也是子类能够替代父类的重要原因之一。

方法的输入参数和返回类型：
当子类覆盖或实现父类的方法时，方法的输入参数（前置条件）可以比父类方法的输入参数更宽松。这意味着子类方法的参数类型可以是父类方法参数类型的父类或者相同类型，或者能够接受更广泛的输入类型。
当子类的方法实现父类的抽象方法时，方法的返回结果（后置条件）要比父类方法的返回结果范围更严格。这意味着子类方法的返回类型应该是父类方法返回类型的子类或者相同类型，以确保子类方法返回的结果能够完全替代父类方法返回的结果。

优点
里氏替换原则的优点主要包括：
提高代码的健壮性：通过确保子类能够完全替代父类，降低了程序出错的可能性，提高了代码的可靠性和稳定性。
降低维护成本：遵循里氏替换原则的设计使得系统更加灵活和可扩展，降低了因修改父类而需要重新设计和测试子类的成本。
促进良好的继承设计：里氏替换原则为良好的继承设计提供了一个明确的规范，有助于开发者在设计和实现类时遵循最佳实践。

应用场景
里氏替换原则广泛应用于面向对象的设计和开发中，特别是在构建复杂的软件系统和框架时。它要求开发者在设计和实现类时，要充分考虑子类对父类的可替换性，确保系统能够稳定、可靠地运行。


Java的23种设计模式：
设计模式的分类，根据目的来划分，主要可以分为三种类型：创建型模式、结构型模式和行为型模式。

一、创建型模式
目的：用于描述“怎样创建对象”，其主要特点是“将对象的创建与使用分离”。
作用：这类模式主要解决的是对象的创建问题，通过某种方式隐藏对象创建的细节，使得程序的扩展性更好。

具体模式：
单例模式：确保一个类只有一个实例，并提供一个全局访问点。
工厂方法模式：定义一个用于创建对象的接口，让子类决定实例化哪一个类。
抽象工厂模式：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。
建造者模式：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。
原型模式：用原型实例指定创建对象的种类，并且通过拷贝这个原型来创建新的对象。

二、结构型模式
目的：用于描述如何将类或对象按某种布局组成更大的结构。
作用：这类模式关注于如何通过不同的方式来组织类和对象，以形成更大的软件结构。

具体模式：
适配器模式：将一个类的接口转换成客户端所期待的另一种接口形式，使类之间的接口不兼容问题可以通过一个中间类来解决。
桥接模式：将抽象部分与实现部分分离，使它们都可以独立地变化。
装饰模式：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰模式相比生成子类更为灵活。
组合模式：将对象组合成树形结构以表示“部分-整体”的层次结构，组合模式使得用户对单个对象和组合对象的使用具有一致性。
外观模式：为子系统中的一组接口提供一个一致的界面，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。
享元模式：运用共享技术有效地支持大量细粒度的对象。
代理模式：为其他对象提供一种代理以控制对这个对象的访问。

三、行为型模式
目的：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。
作用：这类模式不仅描述对象和类的组织结构，还描述它们之间的通信方式。

具体模式：
模版方法模式：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模版方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。
命令模式：将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化；对请求排队或记录请求日志，以及支持可撤销的操作。
迭代器模式：提供一种方法顺序访问一个聚合对象中各个元素，而又不需暴露该对象的内部表示。
观察者模式：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。
中介者模式：用一个中介对象来封装一系列的对象交互，中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。
备忘录模式：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可将该对象恢复到原先保存的状态。
解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中的句子。
状态模式：允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。
策略模式：定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式让算法的变化独立于使用算法的客户。
职责链模式：为请求创建一个接收者对象的链。这条链上的对象每一个都对请求进行某些处理，并可以传递给链上的下一个对象；直到有一个对象处理这个请求为止。
访问者模式：表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。


1、创建型-工厂方法模式：
工厂方法模式分为三种：

（1）简单工厂模式：
简单工厂模式（Simple Factory Pattern）是一种创建型设计模式，它用于创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。
简单工厂模式并不属于GoF（四人帮）的23种设计模式之一，但它经常被用作学习工厂方法模式和抽象工厂模式的基础。
在简单工厂模式中，我们定义一个工厂类，它根据传入的参数决定实例化哪一个类。所有的产品类都继承自一个共同的父类或接口。
以下是一个简单工厂模式的代码示例，我们将创建不同类型的日志记录器（如控制台日志记录器和文件日志记录器），它们都实现了日志记录的功能。

首先，定义一个日志记录器的接口：
public interface Logger {  
    void log(String message);  
}

然后，实现两个具体的日志记录器类：
public class ConsoleLogger implements Logger {  
    @Override  
    public void log(String message) {  
        System.out.println("Console: " + message);  
    }  
}  
  
public class FileLogger implements Logger {  
    @Override  
    public void log(String message) {  
        // 这里只是简单模拟，实际应写入文件  
        System.out.println("File: " + message);  
    }  
}


接下来，创建一个简单工厂类，用于根据传入的类型参数返回相应的日志记录器实例：
public class LoggerFactory {  
    // 使用getLogger方法获取Logger对象  
    public static Logger getLogger(String type) {  
        if (type == null) {  
            return null;  
        }  
        if (type.equalsIgnoreCase("console")) {  
            return new ConsoleLogger();  
        } else if (type.equalsIgnoreCase("file")) {  
            return new FileLogger();  
        }  
          
        // 可以抛出一个异常或者返回一个默认类型的Logger  
        throw new IllegalArgumentException("Invalid logger type: " + type);  
    }  
}


最后，使用工厂类来获取日志记录器实例，并调用其log方法：
public class FactoryPatternDemo {  
    private static final String LOGGER_TYPE = "console";  

    public static void main(String[] args) {  
        Logger logger = LoggerFactory.getLogger(LOGGER_TYPE);  
  
        if (logger != null) {  
            logger.log("This is a debug message");  
        }  
  
        // 尝试获取不同类型的Logger  
        logger = LoggerFactory.getLogger("file");  
        if (logger != null) {  
            logger.log("This is an error message");  
        }  
    }  
}
在这个例子中，LoggerFactory类是一个简单工厂类，它根据传入的字符串类型（"console"或"file"）来决定实例化哪个日志记录器类。客户端代码通过调用LoggerFactory.getLogger()方法来获取Logger对象，而不需要知道具体日志记录器类的实现细节。
这种方式简化了对象的创建过程，并使得系统更加灵活和可扩展。


（2）工厂方法模式：
工厂方法模式（Factory Method Pattern）是一种常用的类创建型设计模式，其核心在于定义一个创建产品对象的工厂接口，将实际创建工作推迟到子类当中。
该模式遵循了面向对象设计原则中的开闭原则（对扩展开放，对修改关闭），通过增加新的具体产品类和工厂子类来扩展系统，而无需修改原有的代码。

定义与概念
定义：工厂方法模式定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类的实例化推迟到子类中进行。
核心结构：
抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法来创建产品。
具体工厂（Concrete Factory）：实现了抽象工厂中的抽象方法，完成具体产品的创建。
抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。
具体产品（Concrete Product）：实现了抽象产品角色所定义的接口，由具体工厂来创建。

特点与适用场景
特点：
开闭原则：遵循开闭原则，增加新的产品时无需修改原有代码，只需添加新的具体产品类和对应的工厂子类。
解耦：客户端与具体产品类解耦，客户端只需要知道所需产品的工厂类即可。
灵活性：允许系统在不修改具体工厂角色的情况下引进新的产品。
适用场景：
当一个类不知道它所需要的对象的类时，在工厂方法模式中，客户端不需要知道具体产品类的类名，只需要知道所对应的工厂即可。
当创建对象的任务由多个具体子工厂中的某一个完成时，客户端可以动态指定使用哪个工厂子类来创建对象。
当需要创建的对象较少且种类不会增加时，可以使用工厂方法模式来封装对象的创建过程。


优缺点
优点：
扩展性好：新增产品时，只需增加具体产品类和对应的工厂子类，无需修改原有代码。
符合开闭原则：系统对扩展开放，对修改关闭，易于维护和升级。
解耦：客户端与具体产品类解耦，降低了系统的耦合度。

缺点：
类的个数增加：每增加一个产品，就需要增加一个具体产品类和一个具体工厂类，增加了系统的复杂度。
理解难度增加：引入了更多的抽象类和接口，增加了系统的抽象性和理解难度。
实现难度增加：在实现时可能需要用到反射等技术，增加了系统的实现难度。

实现步骤
定义抽象产品类：定义产品的规范，描述产品的主要特性和功能。
定义抽象工厂类：提供创建产品的接口，即定义工厂方法。
实现具体产品类：实现抽象产品类定义的接口，具体实现产品的功能。
实现具体工厂类：实现抽象工厂类中的工厂方法，完成具体产品的创建。
客户端调用：客户端通过抽象工厂类的接口调用具体工厂类的方法，来创建具体产品类的对象。

当然，以下是一个工厂方法模式的示例代码。在这个例子中，我们将创建一个简单的图形绘制系统，其中可以绘制不同的形状（如圆形和矩形）。我们将使用工厂方法模式来创建这些形状的对象。
首先，我们定义一个抽象的图形接口（Shape），然后创建实现了这个接口的具体图形类（Circle 和 Rectangle）。接着，我们定义一个抽象的图形工厂接口（ShapeFactory），以及实现了这个接口的具体工厂类（CircleFactory 和 RectangleFactory）。
最后，我们有一个客户端类（GraphicTest），它使用这些工厂来创建图形对象并调用它们的绘制方法。

1. 抽象图形接口（Product）
public interface Shape {  
    void draw();  
}

2. 具体图形类（Concrete Product）
// Circle.java  
public class Circle implements Shape {  
    @Override  
    public void draw() {  
        System.out.println("Inside Circle::draw() method.");  
    }  
}  
  
// Rectangle.java  
public class Rectangle implements Shape {  
    @Override  
    public void draw() {  
        System.out.println("Inside Rectangle::draw() method.");  
    }  
}

3. 抽象图形工厂接口（Abstract Factory）
public interface ShapeFactory {  
    Shape getShape();  
}

4. 具体图形工厂类（Concrete Factory）
// CircleFactory.java  
public class CircleFactory implements ShapeFactory {  
    @Override  
    public Shape getShape() {  
        return new Circle();  
    }  
}  
  
// RectangleFactory.java  
public class RectangleFactory implements ShapeFactory {  
    @Override  
    public Shape getShape() {  
        return new Rectangle();  
    }  
}

5. 客户端类（Client）
public class GraphicTest {  
    public static void main(String[] args) {  
        // 获取 Circle 的对象，并调用它的 draw 方法  
        ShapeFactory circleFactory = new CircleFactory();  
        Shape shape1 = circleFactory.getShape();  
        shape1.draw();  
  
        // 获取 Rectangle 的对象，并调用它的 draw 方法  
        ShapeFactory rectangleFactory = new RectangleFactory();  
        Shape shape2 = rectangleFactory.getShape();  
        shape2.draw();  
  
        // 输出结果  
        // Inside Circle::draw() method.  
        // Inside Rectangle::draw() method.  
    }  
}
在这个例子中，Shape 是抽象产品接口，Circle 和 Rectangle 是实现了这个接口的具体产品类。ShapeFactory 是抽象工厂接口，它声明了一个用于获取形状对象的 getShape 方法。
CircleFactory 和 RectangleFactory 是实现了这个接口的具体工厂类，它们分别返回 Circle 和 Rectangle 类型的对象。最后，在 GraphicTest 客户端类中，我们通过不同的工厂实例来创建不同类型的图形对象，并调用它们的 draw 方法。



（3）静态工厂方法模式：通过将对象的创建过程封装在一个专门的工厂类中，并根据传入的参数决定创建哪个类的对象，从而达到解耦的目的。

定义：简单工厂模式定义一个用于创建对象的接口，但让子类决定要实例化的类是哪一个。工厂类通过传入的参数来决定创建哪种类的对象，并返回这个对象。
角色：
产品（Product）：定义了工厂的创建目标，所有被创建的对象都充当这个角色的某个具体类的实例。
具体产品（Concrete Product）：具体产品角色是简单工厂模式的创建目标，所有被创建的对象都充当这个角色的某个具体类的实例。
工厂（Factory）：即创建者，提供一个用于创建对象的接口。这个接口根据传入的参数来返回不同类的实例。

特点与适用场景
特点：
解耦：客户端只需要知道工厂类和产品的抽象类型，不需要知道具体的产品类。
扩展方便：当需要新增产品时，只需要增加新的具体产品类和相应的创建逻辑到工厂类中（但可能违反开闭原则）。
简单易用：客户端只需要传入相应的参数，即可获取所需的对象。
适用场景：
工厂类负责创建的对象比较少。
客户端不关心对象的创建过程，只关心如何使用对象。
系统中的产品较少且结构相对稳定，不需要频繁增加新产品。

优缺点
优点：
解耦：客户端不需要知道具体产品的类名，只需要知道工厂类和相应的参数。
封装性好：将对象的创建过程封装在工厂类中，客户端通过工厂类获取对象，降低了客户端与具体产品之间的耦合。
扩展相对容易（在一定范围内）：通过修改工厂类的代码可以扩展新的产品，但可能会违反开闭原则。

缺点：
工厂类职责过重：如果产品较多，工厂类的代码会变得复杂且难以维护。
违反开闭原则：每当新增一个产品时，都需要修改工厂类的代码，增加了系统的扩展难度和修改风险。
静态工厂方法不易于继承和重写：由于工厂方法是静态的，因此不能被继承和重写，这限制了工厂模式的灵活性和可扩展性。

示例代码：

// 抽象产品  
interface Operation {  
    double getResult(double a, double b);  
}  
  
// 具体产品  
class OperationAdd implements Operation {  
    public double getResult(double a, double b) {  
        return a + b;  
    }  
}  
  
class OperationSub implements Operation {  
    public double getResult(double a, double b) {  
        return a - b;  
    }  
}  
  
// 工厂类  
class OperationFactory {  
    public static Operation createOperation(String operate) {  
        switch (operate) {  
            case "+":  
                return new OperationAdd();  
            case "-":  
                return new OperationSub();  
            // 可以继续添加其他运算的实现  
            default:  
                return null;  
        }  
    }  
}  
  
// 客户端代码  
public class Main {  
    public static void main(String[] args) {  
        Operation add = OperationFactory.createOperation("+");  
        System.out.println("10 + 5 = " + add.getResult(10, 5));  
  
        Operation sub = OperationFactory.createOperation("-");  
        System.out.println("10 - 5 = " + sub.getResult(10, 5));  
    }  
}
在这个例子中，Operation是抽象产品角色，OperationAdd和OperationSub是具体产品角色，OperationFactory是工厂类。客户端通过传入运算符作为参数来获取相应的运算对象，并调用其getResult方法来执行运算。
这样，客户端就无需关心运算对象的创建过程，只需关心如何使用这些对象。



2、创建型-抽象工厂模式：
抽象工厂模式主要用于创建相关对象的家族。当一个产品族中需要被设计在一起工作时，
通过抽象工厂模式，能够保证客户端始终只使用同一个产品族中的对象；并且通过隔离具体类的生成，
使得客户端不需要明确指定具体生成类；所有的具体工厂都实现了抽象工厂中定义的公共接口，
因此只需要改变具体工厂的实例，就可以在某种程度上改变整个软件系统的行为。
但该模式的缺点在于添加新的行为时比较麻烦，如果需要添加一个新产品族对象时，
需要更改接口及其下所有子类，这必然会带来很大的麻烦。

	//抽象类
	public interface Sender {
		public void Send();
	}
	
	//两个具体实现类
    public class MailSender implements Sender {
    	@Override
    	public void Send() {
    		System.out.println("this is mailsender!");
    	}
    }
    public class MailSender implements Sender {
    	@Override
    	public void Send() {
    		System.out.println("this is mailsender!");
    	}
    }
    
    //提供的接口类
    public interface Provider {
		public Sender produce();
	}

	//两个工厂类
    public class SendMailFactory implements Provider {
    	@Override
    	public Sender produce(){
    		return new MailSender();
    	}
    }
    public class SendSmsFactory implements Provider{
    	@Override
    	public Sender produce() {
    		return new SmsSender();
    	}
    }

	//测试类
	public class Test {
		public static void main(String[] args) {
			Provider provider = new SendMailFactory();
			Sender sender = provider.produce();
			sender.Send();
		}
	}
        





3、创建型-建造者模式：
建造者模式将复杂产品的创建步骤分解在在不同的方法中，使得创建过程更加清晰，从而更精确控制复杂对象的产生过程；
通过隔离复杂对象的构建与使用，也就是将产品的创建与产品本身分离开来，使得同样的构建过程可以创建不同的对象；
并且每个具体建造者都相互独立，因此可以很方便地替换具体建造者或增加新的具体建造者，用户使用不同的具体建造者即可得到不同的产品对象。

建造者模式结构组成
Product: 表示被构造的复杂对象,其中包含需要构建的部件属性。
Builder: 创建一个产品对象的各个部件指定抽象接口。
ConcreteBuilder： 实现Builder的接口以构造和装配该产品的各个部件，定义并明确它所创建的表示。
Director： 调用具体建造者角色以创建产品对象。

	//抽象类
	public interface Sender {
		public void Send();
	}
	
	//两个具体实现类
    public class MailSender implements Sender {
    	@Override
    	public void Send() {
    		System.out.println("this is mailsender!");
    	}
    }
    public class MailSender implements Sender {
    	@Override
    	public void Send() {
    		System.out.println("this is mailsender!");
    	}
    }
    
    public class Builder {
		private List<Sender> list = new ArrayList<Sender>();
	
		public void produceMailSender(int count){
			for(int i=0; i<count; i++){
				list.add(new MailSender());
			}
		}
	
		public void produceSmsSender(int count){
			for(int i=0; i<count; i++){
				list.add(new SmsSender());
			}
		}
	}

	//测试类
	public class Test {
		public static void main(String[] args) {
			Builder builder = new Builder();
			builder.produceMailSender(10);
		}
	}


4、创建型-单例模式：
        单例模式可以确保系统中某个类只有一个实例，该类自行实例化并向整个系统提供这个实例的公共访问点，除了该公共访问点，不能通过其他途径访问该实例。单例模式的优点在于：
	>系统中只存在一个共用的实例对象，无需频繁创建和销毁对象，节约了系统资源，提高系统的性能
	>可以严格控制客户怎么样以及何时访问单例对象。
	>单例模式的写法有好几种，主要有三种：懒汉式单例、饿汉式单例、登记式单例。

单例模式（Singleton Pattern）是一种常用的软件设计模式，旨在确保一个类仅有一个实例，并提供一个全局访问点来获取该实例。以下是九种常见的单例模式实现方式：

1. 饿汉式（静态常量）
这是最简单的单例模式实现方式。类加载时就完成了初始化，所以类加载较慢，但获取对象的速度快。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    // 创建一个类的唯一实例  
    private static final Singleton instance = new Singleton();  
      
    // 提供一个全局的静态方法，用于获取唯一的实例  
    public static Singleton getInstance() {  
        return instance;  
    }  
}
2. 饿汉式（静态代码块）
与静态常量类似，但实例的初始化放在静态代码块中。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    // 静态代码块中初始化实例  
    private static Singleton instance;  
    static {  
        instance = new Singleton();  
    }  
      
    // 提供一个全局的静态方法，用于获取唯一的实例  
    public static Singleton getInstance() {  
        return instance;  
    }  
}
3. 懒汉式（线程不安全）
在第一次调用getInstance()方法时才创建实例，但这种方法在多线程环境下是不安全的。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    // 注意，这里没有使用volatile关键字，所以不是线程安全的  
    private static Singleton instance;  
      
    public static Singleton getInstance() {  
        if (instance == null) {  
            instance = new Singleton();  
        }  
        return instance;  
    }  
}
4. 懒汉式（线程安全，同步方法）
通过在方法上添加synchronized关键字来保证线程安全，但会降低效率。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    private static Singleton instance;  
      
    public static synchronized Singleton getInstance() {  
        if (instance == null) {  
            instance = new Singleton();  
        }  
        return instance;  
    }  
}
5. 懒汉式（线程安全，同步代码块）
只对实例化部分进行同步，减少同步范围，提高效率。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    private static Singleton instance;  
      
    public static Singleton getInstance() {  
        if (instance == null) {  
            synchronized (Singleton.class) {  
                if (instance == null) {  
                    instance = new Singleton();  
                }  
            }  
        }  
        return instance;  
    }  
}
6. 双重检查锁定（Double-Checked Locking）
进一步优化懒汉式，使用volatile关键字防止指令重排序，提高效率。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    // volatile防止指令重排序  
    private static volatile Singleton instance;  
      
    public static Singleton getInstance() {  
        if (instance == null) {  
            synchronized (Singleton.class) {  
                if (instance == null) {  
                    instance = new Singleton();  
                }  
            }  
        }  
        return instance;  
    }  
}
7. 静态内部类
利用classloader机制保证单例的唯一性，同时实现了延迟加载。

public class Singleton {  
    // 构造方法私有化  
    private Singleton() {}  
      
    // 静态内部类  
    private static class SingletonHolder {  
        private static final Singleton INSTANCE = new Singleton();  
    }  
      
    public static Singleton getInstance() {  
        return SingletonHolder.INSTANCE;  
    }  
}
8. 枚举方式
利用枚举的特性实现单例模式，自动支持序列化机制，绝对防止多次实例化。

public enum Singleton {  
    INSTANCE;  
      
    // 可以添加其他方法  
    public void whateverMethod() {  
    }  
}

区分饿汉式单例和懒汉式单例：
饿汉式：好处：线程安全 坏处：对象加载时间过长
懒汉式：好处：延迟对象的创建 坏处：线程不安全




5、创建型-原型模式：
原型模式也是用于对象的创建，通过将一个对象作为原型，对其进行复制克隆，产生一个与源对象类似的新对象。
原型模式包含如下角色：
抽象原型类：规定了具体原型对象必须实现的的 clone() 方法。
具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。
访问类：使用具体原型类中的 clone() 方法来复制新的对象。


在 Java 中，原型模式的核心是就是原型类 Prototype，Prototype 类需要具备以下两个条件：
实现 Cloneable 接口：
重写 Object 类中的 clone() 方法，用于返回对象的拷贝；
Object 类中的 clone() 方法默认是浅拷贝，如果想要深拷贝对象，则需要在 clone() 方法中自定义自己的复制逻辑。

@Data
@AllArgsConstructor
@NoArgsConstructor
public class Student implements Cloneable {
    private String name;
    private String sex;
    private Integer age;
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
 
    public static void main(String[] args) throws Exception{
        Student stu1 = new Student("张三", "男", 18);
        Student stu2 = (Student)stu1.clone();
        stu2.setName("李四");
        System.out.println(stu1);// Student(name=张三, sex=男, age=18)
        System.out.println(stu2);// Student(name=李四, sex=男, age=18)
    }
}

可以看到，把一个学生复制过来，只是改了姓名而已，其他属性完全一样没有改变，
需要注意的是，一定要在被拷贝的对象上实现Cloneable接口，否则会抛出CloneNotSupportedException异常。


浅复制：将一个对象复制后，基本数据类型的变量会重新创建，而引用类型指向的还是原对象所指向的内存地址。

@Data
public class Clazz implements Cloneable {
    private String name;
    private Student student;
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
}
@Data
@AllArgsConstructor
@NoArgsConstructor
public class Student implements Serializable {
    private String name;
    private String sex;
    private Integer age;
}
 
    public static void main(String[] args) throws Exception{
        Clazz clazz1 = new Clazz();
        clazz1.setName("高三一班");
        Student stu1 = new Student("张三", "男", 18);
        clazz1.setStudent(stu1);
        System.out.println(clazz1); // Clazz(name=高三一班, student=Student(name=张三, sex=男, age=18))
        Clazz clazz2 = (Clazz)clazz1.clone();
        Student stu2 = clazz2.getStudent();
        stu2.setName("李四");
        System.out.println(clazz1); // Clazz(name=高三一班, student=Student(name=李四, sex=男, age=18))
        System.out.println(clazz2); // Clazz(name=高三一班, student=Student(name=李四, sex=男, age=18))
    }
	
可以看到，当修改了stu2的姓名时，stu1的姓名同样也被修改了，这说明stu1和stu2是同一个对象，这就是浅克隆的特点，
对具体原型类中的引用类型的属性进行引用的复制。同时，这也可能是浅克隆所带来的弊端，因为结合该例子的原意，
显然是想在班级中新增一名叫李四的学生，而非让所有的学生都改名叫李四，于是我们这里就要使用深克隆。
	
深复制：将一个对象复制后，不论是基本数据类型还有引用类型，都是重新创建的。
        使用原型模式进行创建对象不仅简化对象的创建步骤，还比 new 方式创建对象的性能要好的多，
		因为 Object 类的 clone() 方法是一个本地方法，直接操作内存中的二进制流，特别是复制大对象时，性能的差别非常明显；

@Data
public class Clazz implements Cloneable, Serializable {
    private String name;
    private Student student;
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
 
    protected Object deepClone() throws IOException, ClassNotFoundException {
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(bos);
        oos.writeObject(this);
        ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());
        ObjectInputStream ois = new ObjectInputStream(bis);
        return ois.readObject();
    }
}
 
    public static void main(String[] args) throws Exception{
        Clazz clazz1 = new Clazz();
        clazz1.setName("高三一班");
        Student stu1 = new Student("张三", "男", 18);
        clazz1.setStudent(stu1);
        Clazz clazz3 = (Clazz)clazz1.deepClone();
        Student stu3 = clazz3.getStudent();
        stu3.setName("王五");
        System.out.println(clazz1); // Clazz(name=高三一班, student=Student(name=张三, sex=男, age=18))
        System.out.println(clazz3); // Clazz(name=高三一班, student=Student(name=王五, sex=男, age=18))
    }

可以看到，当修改了stu3的姓名时，stu1的姓名并没有被修改了，这说明stu3和stu1已经是不同的对象了，说明Clazz中的Student也被克隆了，不再指向原有对象地址，这就是深克隆。
这里需要注意的是，Clazz类和Student类都需要实现Serializable接口，否则会抛出NotSerializableException异常。


6、结构型-适配器模式：
适配器模式主要用于将一个类或者接口转化成客户端希望的格式，使得原本不兼容的类可以在一起工作，
将目标类和适配者类解耦；同时也符合“开闭原则”，可以在不修改原代码的基础上增加新的适配器类；
将具体的实现封装在适配者类中，对于客户端类来说是透明的，而且提高了适配者的复用性，但是缺点在于更换适配器的实现过程比较复杂。

所以，适配器模式比较适合以下场景：
（1）系统需要使用现有的类，而这些类的接口不符合系统的接口。
（2）使用第三方组件，组件接口定义和自己定义的不同，不希望修改自己的接口，但是要使用第三方组件接口的功能。
下面有个非常形象的例子很好地说明了什么是适配器模式：

适配器模式（Adapter）包含以下主要角色：

目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。
适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。
适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。

// 适配者 220V电压
public class AC220 {
    public int output() {
        System.out.println("输出220V交流电");
        return 220;
    }
}
// 目标 5V
public interface DC5 {
    public int output5();
}
// 适配器类（电源适配器）
public class PowerAdapter extends AC220 implements DC5 {
    @Override
    public int output5() {
        int output220 = super.output();
        int output5 = output220 / 44;
        System.out.println(output220 + "V适配转换成" + output5 + "V");
        return output5;
    }
}
    // 测试
    public static void main(String[] args) {
        PowerAdapter powerAdapter = new PowerAdapter();
        // 输出220V交流电
        powerAdapter.output();
        // 输出220V交流电
        // 220V适配转换成5V
        powerAdapter.output5();
    }

通过上面代码例子可以看出，类适配器有一个很明显的缺点，就是违背了合成复用原则。
结合上面的例子，假如我不是220V的电压了，是380V电压呢？那就要多建一个380V电压的适配器了。
同理，由于Java是单继承的原因，如果不断的新增适配者，那么就要无限的新增适配器，于是就有了对象适配器。

对象适配器：
// 电源接口
public interface Power {
    int output();
}
// 适配者 220V电压
public class AC220 implements Power {
    @Override
    public int output() {
        System.out.println("输出220V交流电");
        return 220;
    }
}
// 目标 5V
public interface DC5 {
    public int output5();
}
@AllArgsConstructor
public class PowerAdapter implements DC5 {
 
    // 适配者
    private Power power;
    @Override
    public int output5() {
        int output220 = power.output();
        int output5 = output220 / 44;
        System.out.println(output220 + "V适配转换成" + output5 + "V");
        return output5;
    }
}
    // 测试
    public static void main(String[] args) {
        DC5 powerAdapter = new PowerAdapter(new AC220());
        // 输出220V交流电
        // 220V适配转换成5V
        powerAdapter.output5();
    }
	
可以看到，上面代码中，只实现了目标接口，并没有继承适配者，而是将适配者类实现适配者接口，
在适配器中引入适配者接口，当我们需要使用不同的适配者通过适配器进行转换时，就无需再新建适配器类了，
如上面例子，假如我需要380V的电源转换成5V的，那么客户端只需要调用适配器时传入380V电源的类即可，就无需再新建一个380V电源的适配器了
（PS：上述逻辑代码中output220 / 44请忽略，可以根据实际情况编写实际的通用逻辑代码）。

接口适配器：
接口适配器主要是解决类臃肿的问题，我们可以把所有相近的适配模式的方法都放到同一个接口里面，去实现所有方法，
当客户端需要哪个方法直接调用哪个方法即可。如上面例子所示，我们只是转换成了5V电压，
那假如我要转换成12V，24V，30V...呢？那按照上面的写法就需要新建12V，24V，30V...的接口，这样就会导致类过于多了。
那么我们就可以把5V，12V，24V，30V...这些转换方法，通通都写到一个接口里去，
这样当我们需要转换哪种就直接调用哪种即可。

// 这里例子 输出不同直流电接口
public interface DC {
    int output5();
    int output12();
    int output24();
    int output30();
}
// 适配器类（电源适配器）
@AllArgsConstructor
public class PowerAdapter implements DC {
    private Power power;
    @Override
    public int output5() {
        // 具体实现逻辑
        return 5;
    }
    @Override
    public int output12() {
        // 具体实现逻辑
        return 12;
    }
    @Override
    public int output24() {
        // 具体实现逻辑
        return 24;
    }
    @Override
    public int output30() {
        // 具体实现逻辑
        return 30;
    }
}

7、结构型-装饰器模式：
装饰器模式可以动态给对象添加一些额外的职责从而实现功能的拓展，在运行时选择不同的装饰器，从而实现不同的行为；
比使用继承更加灵活，通过对不同的装饰类进行排列组合，创造出很多不同行为，得到功能更为强大的对象；
符合“开闭原则”，被装饰类与装饰类独立变化，用户可以根据需要增加新的装饰类和被装饰类，在使用时再对其进行组合，原有代码无须改变。
但是装饰器模式也存在缺点，首先会产生很多的小对象，增加了系统的复杂性，
第二是排错比较困难，对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为烦琐。

装饰（Decorator）模式中的角色：
抽象构件（Component）角色 ：定义一个抽象接口以规范准备接收附加责任的对象。
具体构件（Concrete Component）角色 ：实现抽象构件，通过装饰角色为其添加一些职责。
抽象装饰（Decorator）角色 ： 继承或实现抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。
具体装饰（ConcreteDecorator）角色 ：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。

// 炒饭类
public class FriedRice {
    String getDesc() {
        return "炒饭";
    }
    Integer getPrice() {
        return 5;
    }
}
// 配料表
public abstract class Ingredients extends FriedRice{
    private FriedRice friedRice;
    public Ingredients(FriedRice friedRice) {
        this.friedRice = friedRice;
    }
    String getDesc() {
        return this.friedRice.getDesc();
    }
    Integer getPrice() {
        return this.friedRice.getPrice();
    }
}
// 鸡蛋配料
public class Egg extends Ingredients {
    public Egg(FriedRice friedRice) {
        super(friedRice);
    }
    String getDesc() {
        return super.getDesc() + "+鸡蛋";
    }
    Integer getPrice() {
        return super.getPrice() + 2;
    }
}
// 火腿配料
public class Ham extends Ingredients {
    public Ham(FriedRice friedRice){
        super(friedRice);
    }
    String getDesc() {
        return super.getDesc() + "+火腿";
    }
    Integer getPrice() {
        return super.getPrice() + 3;
    }
}
    // 测试方法
    public static void main(String[] args) {
        FriedRice friedRice = new FriedRice();
        System.out.println(friedRice.getDesc() + friedRice.getPrice() + "元"); // 炒饭5元
        friedRice = new Egg(friedRice);
        System.out.println(friedRice.getDesc() + friedRice.getPrice() + "元"); // 炒饭+鸡蛋7元
        friedRice = new Egg(friedRice);
        System.out.println(friedRice.getDesc() + friedRice.getPrice() + "元");// 炒饭+鸡蛋+鸡蛋9元
        friedRice = new Ham(friedRice);
        System.out.println(friedRice.getDesc() + friedRice.getPrice() + "元");// 炒饭+鸡蛋+鸡蛋+火腿12元
    }

装饰器模式与代理模式对比：
装饰器模式就是一种特殊的代理模式。
装饰器模式强调自身的功能扩展，用自己说了算的透明扩展，可动态定制的扩展；代理模式强调代理过程的控制。
获取目标对象构建的地方不同，装饰者是从外界传递进来的，可以通过构造方法传递；静态代理是在代理类内部创建，以此来隐藏目标对象。
代理模式使用了面对对象中的多态；装饰器模式使用了面对对象中的继承。

适用场景：
用于扩展一个类的功能或者给一个类添加附加职责。
动态的给一个对象添加功能，这些功能同样也可以再动态的撤销。

优点：
装饰器是继承的有力补充，比继承灵活，不改变原有对象的情况下动态地给一个对象扩展功能，即插即用。
通过使用不同装饰类以及这些装饰类的排列组合，可实现不同效果。
装饰器完全遵守开闭原则。

缺点：
会出现更多的代码，更多的类，增加程序的复杂性。
动态装饰时，多层装饰会更复杂。
	
8、结构型-代理模式：
代理模式的设计动机是通过代理对象来访问真实对象，通过建立一个对象代理类，
由代理对象控制原对象的引用，从而实现对真实对象的操作。在代理模式中，代理对象主要起到一个中介的作用，
用于协调与连接调用者(即客户端)和被调用者(即目标对象)，在一定程度上降低了系统的耦合度，同时也保护了目标对象。
但缺点是在调用者与被调用者之间增加了代理对象，可能会造成请求的处理速度变慢。

静态代理：静态代理就是指我们在给一个类扩展功能的时候，我们需要去书写一个静态的类，
相当于在之前的类上套了一层，这样我们就可以在不改变之前的类的前提下去对原有功能进行扩展，
静态代理需要代理对象和目标对象实现一样的接口。

// 火车站接口，有卖票功能
public interface TrainStation {
    void sellTickets();
}
// 广州火车站卖票
public class GuangzhouTrainStation implements TrainStation {
    @Override
    public void sellTickets() {
        System.out.println("广州火车站卖票啦");
    }
}
// 代售点卖票（代理类）
public class ProxyPoint implements TrainStation {
    // 目标对象（代理火车站售票）
    private GuangzhouTrainStation station = new GuangzhouTrainStation();
    @Override
    public void sellTickets() {
        System.out.println("代售加收5%手续费");
        station.sellTickets();
    }
    public static void main(String[] args) {
        ProxyPoint proxyPoint = new ProxyPoint();
        // 代售加收5%手续费
        // 广州火车站卖票啦
        proxyPoint.sellTickets();
    }
}
    // 测试
    public static void main(String[] args) {
        ProxyPoint proxyPoint = new ProxyPoint();
        // 代售加收5%手续费
        // 火车站卖票啦
        proxyPoint.sellTickets();
    }
	
动态代理：
代理类在代码运行时创建的代理称之为动态代理。
动态代理中代理类并不是预先在Java代码中定义好的，而是运行时由JVM动态生成，并且可以代理多个目标对象。	
// 火车站接口，有卖票功能
public interface TrainStation {
    void sellTickets();
}
// 广州火车站卖票
public class GuangzhouTrainStation implements TrainStation {
    @Override
    public void sellTickets() {
        System.out.println("广州火车站卖票啦");
    }
}
// 深圳火车站卖票
public class ShenzhenTrainStation implements TrainStation {
    @Override
    public void sellTickets() {
        System.out.println("深圳火车站卖票啦");
    }
}
// 代售点卖票（代理类）
public class ProxyPoint implements InvocationHandler {
    private TrainStation trainStation;
    public TrainStation getProxyObject(TrainStation trainStation) {
        this.trainStation = trainStation;
        Class<? extends TrainStation> clazz = trainStation.getClass();
        return (TrainStation) Proxy.newProxyInstance(clazz.getClassLoader(), clazz.getInterfaces(), this);
    }
    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("代售火车票收取5%手续费");
        return method.invoke(this.trainStation, args);
    }
}
    // 测试
    public static void main(String[] args) {
        ProxyPoint proxy = new ProxyPoint();
        TrainStation guangzhouTrainStation = proxy.getProxyObject(new GuangzhouTrainStation());
        // 代售火车票收取5%手续费
        // 广州火车站卖票啦
        guangzhouTrainStation.sellTickets();
        TrainStation shenzhenTrainStation = proxy.getProxyObject(new ShenzhenTrainStation());
        // 代售火车票收取5%手续费
        // 深圳火车站卖票啦
        shenzhenTrainStation.sellTickets();
    }

9、结构型-桥接模式：
桥接模式也称为桥梁模式、接口模式或者柄体（Handle and Body）模式，是将抽象部分与他的具体实现部分分离，使它们都可以独立地变化，通过组合的方式建立两个类之间的联系，而不是继承。
桥接（Bridge）模式包含以下主要角色：
抽象化（Abstraction）角色 ：定义抽象类，并包含一个对实现化对象的引用。
扩展抽象化（Refined Abstraction）角色 ：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。
实现化（Implementor）角色 ：定义实现化角色的接口，供扩展抽象化角色调用。
具体实现化（Concrete Implementor）角色 ：给出实现化角色接口的具体实现。

// 视频接口
public interface Video {
    void decode(String fileName);
}
 
// MP4格式类
public class Mp4 implements Video{
    @Override
    public void decode(String fileName) {
        System.out.println("MP4视频文件："+ fileName);
    }
}
// RMVB格式类
public class Rmvb implements Video{
    @Override
    public void decode(String fileName) {
        System.out.println("rmvb文件：" + fileName);
    }
}
// 操作系统抽象类
@AllArgsConstructor
public abstract class OperatingSystem {
    Video video;
    public abstract void play(String fileName);
 
}
// iOS系统
public class Ios extends OperatingSystem {
    public Ios(Video video){
        super(video);
    }
    @Override
    public void play(String fileName) {
        video.decode(fileName);
    }
}
// windows系统
public class Windows extends OperatingSystem {
    public Windows(Video video){
        super(video);
    }
    @Override
    public void play(String fileName) {
        video.decode(fileName);
    }
}
视频类和操作系统类之间通过OperatingSystem类桥接关联起来。

在JDBC中使用桥接模式：
在Java中我们使用 JDBC 连接数据库时，在各个数据库之间进行切换，基本不需要动太多的代码，
原因就是使用了桥接模式，JDBC 提供统一接口，每个数据库提供各自的实现，
然后由桥接类创建一个连接数据库的驱动，使用某一个数据库的时候只需要切换一下就行。

在 JDBC 中，桥接模式的实现化角色 (Implementor) 为的 Driver 接口，
具体实现化 (Concrete Implementor) 角色对应 MysqlDriver、OracleDriver 和 MariadbDriver，
扩展抽象化 (Refined Abstraction) 角色对应 DriverManager，不具有抽象化 (Abstraction) 角色作为扩展抽象化角色的父类。

适用场景：
在抽象和具体实现之间需要增加更多的灵活性的场景。
一个类存在两个（或多个）独立变化的维度，而这两个（或多个）维度都需要独立进行扩展。
不希望使用继承，或因为多层继承导致系统类的个数剧增。

优点：
分离抽象部分及其具体实现部分。
提高了系统的扩展性。
符合开闭原型。
符合合成复用原则。

缺点：
增加了系统的理解与设计难度。
需要正确地识别系统中两个独立变化的维度。

桥接模式和适配器模式的异同：
桥接模式和适配器模式都是设计模式中常用的两种模式，它们各自有着独特的特点和应用场景，同时也存在一些共同点和差异。

共同点
目的相似：两者都是为了解决不同接口或类之间的兼容性问题，使得原本无法直接协作的组件能够协同工作。
灵活性强：通过引入中间层或适配器类，提高了系统的灵活性和可扩展性。
差异
桥接模式	适配器模式
定义	将抽象与实现分离，使它们可以独立变化。桥接模式用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。	将一个类的接口转换成客户希望的另一个接口，使类因接口不兼容而不能一起工作的类可以一起工作。
目的	分离抽象化和实现，使两者的接口可以不同，目的是分离。	改变已有的两个接口，让它们相容。
出发点	桥接是先有桥，才有两端的东西。如果还什么都没有，但是想分开实现，那么桥接是一个选择。	适配是先有两边的东西，才有适配器。如果拿到两个已有模块，想让他们同时工作，那么使用的就是适配器。
关注点	关注于抽象和实现之间的分离，使得它们可以独立变化，便于维护和扩展。	关注于接口之间的转换，使得不兼容的接口能够协同工作。
应用场景	当一个类存在两个或多个独立变化的维度，且这些维度需要被独立扩展时，可以使用桥接模式。	当需要使用一些已存在的类，而这些类的接口和当前系统不兼容时，可以使用适配器模式进行接口转换。
优点	1. 抽象和实现分离，扩展能力强。2. 实现细节对客户透明。	1. 提高了类的复用性。2. 增加了类的透明度。3. 灵活性好。
缺点	由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，这增加了系统的理解与设计难度。	过多地使用适配器，会让系统非常零乱，不易整体进行把握。

总结
桥接模式和适配器模式都是设计模式中用于解决接口或类之间兼容性问题的重要模式。桥接模式更侧重于抽象和实现的分离，以应对系统中可能的多维变化；
而适配器模式则更侧重于接口之间的转换，以解决现有系统间的兼容性问题。在实际应用中，应根据具体需求和场景选择合适的模式。

10、结构型-外观模式：
外观模式又称门面模式，提供了一个统一的接口，用来访问子系统中的一群接口。

特征：门面模式定义了一个高层接口，让子系统更容易使用。

外观（Facade）模式包含以下主要角色：
外观（Facade）角色：为多个子系统对外提供一个共同的接口。
子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。

public class Light {
    public void on() {
        System.out.println("开灯");
    }
    public void off() {
        System.out.println("关灯");
    }
}
public class Tv {
    public void on() {
        System.out.println("开电视");
    }
    public void off() {
        System.out.println("关电视");
    }
}
public class Fan {
    public void on() {
        System.out.println("开风扇");
    }
    public void off() {
        System.out.println("关风扇");
    }
}

public class SmartSpeaker {
    private Light light;
    private Tv tv;
    private Fan fan;
    public SmartSpeaker() {
        light = new Light();
        tv = new Tv();
        fan = new Fan();
    }
    public void say(String order) {
        if (order.contains("起床")) {
            getUp();
        } else if (order.contains("睡觉")) {
            sleep();
        } else {
            System.out.println("我还听不懂你说的啥！");
        }
    }
    public void getUp() {
        System.out.println("起床");
        light.on();
        tv.on();
        fan.off();
    }
    public void sleep() {
        System.out.println("睡觉");
        light.off();
        tv.off();
        fan.on();
    }
}
    public static void main(String[] args) {
        SmartSpeaker smartSpeaker = new SmartSpeaker();
        //睡觉
        //关灯
        //关电视
        //开风扇
        smartSpeaker.say("我要睡觉了!");
        //起床
        //开灯
        //开电视
        //关风扇
        smartSpeaker.say("我起床了!");
        //我还听不懂你说的啥！
        smartSpeaker.say("Emmm");
    }


适用场景：
对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。
当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。
当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。

优点：
简化了调用过程，无需深入了解子系统，以防给子系统带来风险。
减少系统依赖、松散耦合。
更好地划分访问层次，提高了安全性。
遵循迪米特法则，即最少知道原则。

缺点：
当增加子系统和扩展子系统行为时，可能容易带来未知风险。
不符合开闭原则。
某些情况下可能违背单一职责原则。


11、结构型-组合模式：
组合模式也称为整体-部分（Part-Whole）模式，它的宗旨是通过将单个对象（叶子结点）和组合对象（树枝节点）用相同的接口进行表示。
作用：使客户端对单个对象和组合对象保持一致的方式处理。

组合模式主要包含三种角色：
抽象根节点（Component）：定义系统各层次对象的共有方法和属性，可以预先定义一些默认行为和属性。
树枝节点（Composite）：定义树枝节点的行为，存储子节点，组合树枝节点和叶子节点形成一个树形结构。
叶子节点（Leaf）：叶子节点对象，其下再无分支，是系统层次遍历的最小单位。

// 菜单组件
public abstract class MenuComponent {
    String name;
    Integer level;
    public void add(MenuComponent menuComponent) {
        throw new UnsupportedOperationException("不支持添加操作!");
    }
    public void remove(MenuComponent menuComponent) {
        throw new UnsupportedOperationException("不支持删除操作!");
    }
    public MenuComponent getChild(Integer i) {
        throw new UnsupportedOperationException("不支持获取子菜单操作!");
    }
    public String getName() {
        throw new UnsupportedOperationException("不支持获取名字操作!");
    }
    public void print() {
        throw new UnsupportedOperationException("不支持打印操作!");
    }
}
// 菜单类
public class Menu extends MenuComponent {
    private List<MenuComponent> menuComponentList = new ArrayList<>();
    public Menu(String name,int level){
        this.level = level;
        this.name = name;
    }
    @Override
    public void add(MenuComponent menuComponent) {
        menuComponentList.add(menuComponent);
    }
    @Override
    public void remove(MenuComponent menuComponent) {
        menuComponentList.remove(menuComponent);
    }
    @Override
    public MenuComponent getChild(Integer i) {
        return menuComponentList.get(i);
    }
    @Override
    public void print() {
        for (int i = 1; i < level; i++) {
            System.out.print("--");
        }
        System.out.println(name);
        for (MenuComponent menuComponent : menuComponentList) {
            menuComponent.print();
        }
    }
}
// 子菜单类
public class MenuItem extends MenuComponent {
    public MenuItem(String name,int level) {
        this.name = name;
        this.level = level;
    }
    @Override
    public void print() {
        for (int i = 1; i < level; i++) {
            System.out.print("--");
        }
        System.out.println(name);
    }
}
    // 测试方法
    public static void main(String[] args) {
        //创建一级菜单
        MenuComponent component = new Menu("系统管理",1);
 
        MenuComponent menu1 = new Menu("用户管理",2);
        menu1.add(new MenuItem("新增用户",3));
        menu1.add(new MenuItem("修改用户",3));
        menu1.add(new MenuItem("删除用户",3));
 
        MenuComponent menu2 = new Menu("角色管理",2);
        menu2.add(new MenuItem("新增角色",3));
        menu2.add(new MenuItem("修改角色",3));
        menu2.add(new MenuItem("删除角色",3));
        menu2.add(new MenuItem("绑定用户",3));
 
        //将二级菜单添加到一级菜单中
        component.add(menu1);
        component.add(menu2);
 
        //打印菜单名称(如果有子菜单一块打印)
        component.print();
    }
// 测试结果
系统管理
--用户管理
----新增用户
----修改用户
----删除用户
--角色管理
----新增角色
----修改角色
----删除角色
----绑定用户
 

12、结构型-享元模式：
享元模式又称为轻量级模式，是对象池的一种实现，类似于线程池，线程池可以避免不停的创建和销毁多个对象，消耗性能。
提供了减少对象数量从而改善应用所需的对象结构的方式。宗旨：共享细粒度对象，将多个对同一对象的访问集中起来。

享元（Flyweight ）模式中存在以下两种状态：
内部状态，即不会随着环境的改变而改变的可共享部分。
外部状态，指随环境改变而改变的不可以共享的部分。享元模式的实现要领就是区分应用中的这两种状态，并将外部状态外部化。

享元模式的主要有以下角色：
抽象享元角色（Flyweight）：通常是一个接口或抽象类，在抽象享元类中声明了具体享元类公共的方法，
这些方法可以向外界提供享元对象的内部数据（内部状态），同时也可以通过这些方法来设置外部数据（外部状态）。

具体享元（Concrete Flyweight）角色 ：它实现了抽象享元类，称为享元对象；
在具体享元类中为内部状态提供了存储空间。通常我们可以结合单例模式来设计具体享元类，
为每一个具体享元类提供唯一的享元对象。

非享元（Unsharable Flyweight)角色 ：并不是所有的抽象享元类的子类都需要被共享，
不能被共享的子类可设计为非共享具体享元类；当需要一个非共享具体享元类的对象时可以直接通过实例化创建。

享元工厂（Flyweight Factory）角色 ：负责创建和管理享元角色。当客户对象请求一个享元对象时，
享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。

// 抽象接口
public interface ITicket {
    void show(String seat);
}
public class TrainTicket implements ITicket {
    private String from;
    private String to;
    private Integer price;
    public TrainTicket(String from, String to) {
        this.from = from;
        this.to = to;
    }
    @Override
    public void show(String seat) {
        this.price = new Random().nextInt(500);
        System.out.println(from + "->" + to + ":" + seat + "价格:" + this.price);
    }
}
// 工厂类
public class TicketFactory {
    private static Map<String, ITicket> pool = new ConcurrentHashMap<>();
    public static ITicket getTicket(String from, String to) {
        String key = from + "->" + to;
        if (pool.containsKey(key)) {
            System.out.println("使用缓存获取火车票:" + key);
            return pool.get(key);
        }
        System.out.println("使用数据库获取火车票:" + key);
        ITicket ticket = new TrainTicket(from, to);
        pool.put(key, ticket);
        return ticket;
    }
}
    // 测试
    public static void main(String[] args) {
        ITicket ticket = getTicket("北京", "上海");
        //使用数据库获取火车票:北京->上海
        //北京->上海:二等座价格:20
        ticket.show("二等座");
        ITicket ticket1 = getTicket("北京", "上海");
        //使用缓存获取火车票:北京->上海
        //北京->上海:商务座价格:69
        ticket1.show("商务座");
        ITicket ticket2 = getTicket("上海", "北京");
        //使用数据库获取火车票:上海->北京
        //上海->北京:一等座价格:406
        ticket2.show("一等座");
        System.out.println(ticket == ticket1);//true
        System.out.println(ticket == ticket2);//false
    }
	
注意：
可以看到ticket和ticket2是使用数据库查询的，而ticket1是使用缓存查询的，
同时ticket == ticket1返回的是true，ticket == ticket2返回的是false，证明ticket和ticket1是共享的对象。

适用场景：
一个系统有大量相同或者相似的对象，造成内存的大量耗费。
对象的大部分状态都可以外部化，可以将这些外部状态传入对象中。
在使用享元模式时需要维护一个存储享元对象的享元池，而这需要耗费一定的系统资源，因此，应当在需要多次重复使用享元对象时才值得使用享元模式。

优点：
减少对象的创建，降低内存中对象的数量，降低系统的内存，提高效率。
减少内存之外的其他资源占用。

缺点：
关注内、外部状态。
关注线程安全问题。
使系统、程序的逻辑复杂化。

Java 中，String 类型就是使用享元模式，String 对象是 final 类型，对象一旦创建就不可改变。
而 Java 的字符串常量都是存在字符串常量池中的，JVM 会确保一个字符串常量在常量池中只有一个拷贝。
而且提到共享池，我们也很容易联想到 Java 里面的JDBC连接池，通过连接池的管理，实现了数据库连接的共享，
不需要每一次都重新创建连接，节省了数据库重新创建的开销，提升了系统的性能！


 13、行为型-策略模式：
策略模式又叫政策模式（Policy Pattern），它是将定义的算法家族分别封装起来，让它们之间可以互相替换，从而让算法的变化不会影响到使用算法的用户。可以避免多重分支的if......else和switch语句。

策略模式的主要角色如下：

抽象策略（Strategy）类：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。
具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现或行为。
环境（Context）类：持有一个策略类的引用，最终给客户端调用。

策略模式适用用于以下几种场景：
（1）应用程序需要实现特定的功能服务，而该程序有多种实现方式使用，所以需要动态地在几种算法中选择一种
（2）一个类定义了多种行为算法，并且这些行为在类的操作中以多个条件语句的形式出现，
就可以将相关的条件分支移入它们各自的Strategy类中以代替这些条件语句。

// 会员卡接口
public interface VipCard {
    public void discount();
}
public class GoldCard implements VipCard {
    @Override
    public void discount() {
        System.out.println("金卡打7折");
    }
}
public class SilverCard implements VipCard {
    @Override
    public void discount() {
        System.out.println("银卡打8折");
    }
}
public class CopperCard implements VipCard {
    @Override
    public void discount() {
        System.out.println("铜卡打9折");
    }
}
public class Normal implements VipCard {
    @Override
    public void discount() {
        System.out.println("普通会员没有折扣");
    }
}
// 会员卡容器类
public class VipCardFactory {
    private static Map<String, VipCard> map = new ConcurrentHashMap<>();
    static {
        map.put("gold", new GoldCard());
        map.put("silver", new SilverCard());
        map.put("copper", new CopperCard());
    }
    public static VipCard getVIPCard(String level) {
        return map.get(level) != null ? map.get(level) : new Normal();
    }
 
}
    // 测试方法
    public static void main(String[] args) {
        //金卡打7折
        VipCardFactory.getVIPCard("gold").discount();
        //银卡打8折
        VipCardFactory.getVIPCard("silver").discount();
        //普通会员没有折扣
        VipCardFactory.getVIPCard("other").discount();
    }
	
总结：
适用场景：
系统中有很多类，而它们的区别仅仅在于它们的行为不同。
系统需要动态地在几种算法中选择一种。
需要屏蔽算法规则。

优点：
符合开闭原则。
避免使用多重条件语句。
可以提高算法的保密性和安全性。
易于扩展。

缺点：
客户端必须知道所有的策略，并且自行决定使用哪一个策略类。
代码中会产生非常多的策略类，增加维护难度。
	
14、行为型-模板方法：
模板方法模式通常又叫模板模式，是指定义一个算法的骨架，并允许之类为其中的一个或者多个步骤提供实现。模板方法模式使得子类可以在不改变算法结构的情况下，重新定义算法的某些步骤。

模板方法（Template Method）模式包含以下主要角色：

抽象类（Abstract Class）：负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。
模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。
基本方法：是实现算法各个步骤的方法，是模板方法的组成部分。基本方法又可以分为三种：
抽象方法(Abstract Method) ：一个抽象方法由抽象类声明、由其具体子类实现。
具体方法(Concrete Method) ：一个具体方法由一个抽象类或具体类声明并实现，其子类可以进行覆盖也可以直接继承。
钩子方法(Hook Method) ：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。一般钩子方法是用于判断的逻辑方法，这类方法名一般为isXxx，返回值类型为boolean类型。
具体子类（Concrete Class）：实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的组成步骤。

public abstract class DayOffProcess {
    // 请假模板
    public final void dayOffProcess() {
        // 领取申请表
        this.pickUpForm();
        // 填写申请信息
        this.writeInfo();
        // 签名
        this.signUp();
        // 提交到不同部门审批
        this.summit();
        // 行政部备案
        this.filing();
    }
    private void filing() {
        System.out.println("行政部备案");
    }
    protected abstract void summit();
    protected abstract void signUp();
    private void writeInfo() {
        System.out.println("填写申请信息");
    }
    private void pickUpForm() {
        System.out.println("领取申请表");
    }
}
public class ZhangSan extends DayOffProcess {
    @Override
    protected void summit() {
        System.out.println("张三签名");
    }
    @Override
    protected void signUp() {
        System.out.println("提交到技术部审批");
    }
}
public class Lisi extends DayOffProcess {
    @Override
    protected void summit() {
        System.out.println("李四签名");
    }
    @Override
    protected void signUp() {
        System.out.println("提交到市场部审批");
    }
}
    // 测试方法
    public static void main(String[] args) {
        DayOffProcess zhangsan = new ZhangSan();
        //领取申请表
        //填写申请信息
        //提交到技术部审批
        //张三签名
        //行政部备案
        zhangsan.dayOffProcess();
        DayOffProcess lisi = new Lisi();
        //领取申请表
        //填写申请信息
        //提交到市场部审批
        //李四签名
        //行政部备案
        lisi.dayOffProcess();
    }


适用场景：
一次性实现一个算法不变的部分，并将可变的行为留给子类来实现。
各子类中公共的行为被提取出来并集中到一个公共的父类中，从而避免代码重复。

优点：
利用模板方法将相同处理逻辑的代码放到抽象父类中，可以提高代码的复用性。
将不同的代码不同的子类中，通过对子类的扩展增加新的行为，提高代码的扩展性。
把不变的行为写在父类上，去除子类的重复代码，提供了一个很好的代码复用平台，符合开闭原则。

缺点：
类数目的增加，每一个抽象类都需要一个子类来实现，这样导致类的个数增加。
类数量的增加，间接地增加了系统实现的复杂度。
继承关系自身缺点，如果父类添加新的抽象方法，所有子类都要改一遍。


15、行为型-责任链模式：
职责链可以将请求的处理者组织成一条链，并将请求沿着链传递，如果某个处理者能够处理请求则处理，否则将该请求交由上级处理。客户端只需将请求发送到职责链上，无须关注请求的处理细节，通过职责链将请求的发送者和处理者解耦了，这也是职责链的设计动机。
职责链模式可以简化对象间的相互连接，因为客户端和处理者都没有对方明确的信息，同时处理者也不知道职责链中的结构，处理者只需保存一个指向后续者的引用，而不需要保存所有候选者的引用。
另外职责链模式增加了系统的灵活性，我们可以任意增加或更改处理者，甚至更改处理者的顺序，不过有可能会导致一个请求无论如何也得不到处理，因为它可能被放置在链末端。

所以责任链模式有以下几个优点：
（1）降低耦合度，将请求的发送者和接收者解耦。反映在代码上就是不需要在类中写很多丑陋的 if….else 语句，如果用了职责链，相当于我们面对一个黑箱，只需将请求递交给其中一个处理者，然后让黑箱内部去负责传递就可以了。
（2）简化了对象，使得对象不需要链的结构。
（3）增加系统的灵活性，通过改变链内的成员或者调动他们的次序，允许动态地新增或者删除处理者
（4）增加新的请求处理类很方便。

但是责任链模式也存在一些缺点：
（1）不能保证请求一定被成功处理
（2）系统性能将受到一定影响，并且可能会造成循环调用。
（3）可能不容易观察运行时的特征，而且在进行代码调试时不太方便，有碍于除错。
 

// 用户实体类
@Data
public class User {
    private String username;
    private String password;
    private String role;
}
// handler抽象类
public abstract class Handler {
    protected Handler next;
    // 返回handler方便链式操作
    public void next(Handler next) {
        this.next = next;
    }
    // 流程开始的方法
    public abstract void doHandler(User user);
}
// 校验用户名或者密码是否为空
public class ValidateHandler extends Handler {
    @Override
    public void doHandler(User user) {
        if (StringUtils.isBlank(user.getUsername()) || StringUtils.isBlank(user.getPassword())) {
            System.out.println("用户名或者密码为空!");
            return;
        }
        System.out.println("校验通过");
        next.doHandler(user);
    }
}
// 登录校验，校验用户名是否匹配密码
public class LoginHandler extends Handler {
    @Override
    public void doHandler(User user) {
        if (!"pyy52hz".equals(user.getUsername()) || !"123456".equals(user.getPassword())) {
            System.out.println("用户名或者密码不正确!请检查!");
            return;
        }
        user.setRole("admin");
        System.out.println("登陆成功!角色为管理员!");
        next.doHandler(user);
    }
}
// 权限校验
public class AuthHandler extends Handler {
    @Override
    public void doHandler(User user) {
        if (!"admin".equals(user.getRole())) {
            System.out.println("无权限操作!");
            return;
        }
        System.out.println("角色为管理员,可以进行下一步操作!");
    }
}
// 登录流程
public class LoginService {
    public void login(User user) {
        Handler validateHandler = new ValidateHandler();
        Handler loginHandler = new LoginHandler();
        Handler authHandler = new AuthHandler();
        validateHandler.next(loginHandler);
        loginHandler.next(authHandler);
        validateHandler.doHandler(user);
    }
}
    // 测试方法
    public static void main(String[] args){
      User user = new User();
      //校验通过
      //用户名或者密码不正确!请检查!
      user.setUsername("pyy52hz");
      user.setPassword("1234567");
      LoginService loginService = new LoginService();
      loginService.login(user);
      //校验通过
      //登陆成功!角色为管理员!
      //角色为管理员,可以进行下一步操作!
      user.setUsername("pyy52hz");
      user.setPassword("123456");
      loginService.login(user);
    }


       
16、行为型-观察者模式：
观察者模式又称为 发布-订阅模式，定义了对象之间一对多依赖关系，当目标对象(被观察者)的状态发生改变时，它的所有依赖者(观察者)都会收到通知。
一个观察目标可以对应多个观察者，而这些观察者之间没有相互联系，所以能够根据需要增加和删除观察者，使得系统更易于扩展，符合开闭原则；
并且观察者模式让目标对象和观察者松耦合，虽然彼此不清楚对方的细节，但依然可以交互，目标对象只知道一个具体的观察者列表，
但并不认识任何一个具体的观察者，它只知道他们都有一个共同的接口。

但观察者模式的缺点在于如果存在很多个被观察者的话，那么将需要花费一定时间通知所有的观察者，如果观察者与被观察者之间存在循环依赖的话，那么可能导致系统崩溃，并且观察者模式没有相应的机制让观察者知道被观察对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。

// 抽象观察者接口
public interface Observer {
    void update(String message);
}
// 微信用户类 具体的观察者
@AllArgsConstructor
public class WeixinUser implements Observer {
    private String name;
    @Override
    public void update(String message) {
        System.out.println(name + "接收到了消息(观察到了):" + message);
    }
}
// 被观察者接口
public interface Observable {
    // 新增用户(新增观察者)
    void add(Observer observer);
    // 移除用户,或者说用户取消订阅(移除观察者)
    void del(Observer observer);
    // 发布 推送消息
    void notify(String message);
}
// 具体的被观察者(公众号)
public class Subject implements Observable {
    // 观察者列表(订阅用户)
    private List<Observer> list = new ArrayList<>();
    @Override
    public void add(Observer observer) {
        list.add(observer);
    }
    @Override
    public void del(Observer observer) {
        list.remove(observer);
    }
    // 给每一个观察者(订阅者)推送消息
    @Override
    public void notify(String message) {
        list.forEach(observer -> observer.update(message));
    }
 
}
    // 测试
    public static void main(String[] args){
      Observable o = new Subject();
      WeixinUser user1 = new WeixinUser("张三");
      WeixinUser user2 = new WeixinUser("李四");
      WeixinUser user3 = new WeixinUser("王五");
      o.add(user1);
      o.add(user2);
      o.add(user3);
      o.notify("薛之谦演唱会要来到广州啦!");
      // 运行结果
      // 张三接收到了消息(观察到了):薛之谦演唱会要来到广州啦!
      // 李四接收到了消息(观察到了):薛之谦演唱会要来到广州啦!
      // 王五接收到了消息(观察到了):薛之谦演唱会要来到广州啦!
    }
 
JDK实现
在 Java 中，通过java.util.Observable类和 java.util.Observer接口定义了观察者模式，只要实现它们的子类就可以编写观察者模式实例。

1 Observable类
Observable类是抽象目标类（被观察者），它有一个Vector集合成员变量，用于保存所有要通知的观察者对象，下面是它最重要的 3 个方法：
void addObserver(Observer o) 方法：用于将新的观察者对象添加到集合中。
void notifyObservers(Object arg) 方法：调用集合中的所有观察者对象的update方法，通知它们数据发生改变。通常越晚加入集合的观察者越先得到通知。
void setChange() 方法：用来设置一个boolean类型的内部标志，注明目标对象发生了变化。当它为true时，notifyObservers() 才会通知观察者。

2 Observer 接口
Observer 接口是抽象观察者，它监视目标对象的变化，当目标对象发生变化时，观察者得到通知，并调用 update 方法，进行相应的工作。

3 代码实现
// 具体的被观察者(公众号)
@Data
@AllArgsConstructor
public class Subject extends Observable {
    // 公众号的名字
    private String name;
    // 公众号发布消息
    public void notifyMessage(String message) {
        System.out.println(this.name + "公众号发布消息:" + message + "请关注用户留意接收!");
        super.setChanged();
        super.notifyObservers(message);
    }
}
@AllArgsConstructor
public class WeixinUser implements Observer {
    private String name;
    /**
     * @param o 被观察者
     * @param arg 被观察者带过来的参数，此例子中是公众号发布的消息
     */
    @Override
    public void update(Observable o, Object arg) {
        System.out.println(name + "关注了公众号(被观察者):" + ((Subject)o).getName() + ",接收到消息:" + arg);
    }
}
    // 测试
    public static void main(String[] args){
        WeixinUser user1 = new WeixinUser("张三");
        WeixinUser user2 = new WeixinUser("李四");
        WeixinUser user3 = new WeixinUser("王五");
        Subject subject = new Subject("演唱会消息发布");
        subject.addObserver(user1);
        subject.addObserver(user2);
        subject.addObserver(user3);
        subject.notifyMessage("薛之谦演唱会要来到广州啦!");
        // 返回结果
        // 演唱会消息发布公众号发布消息:薛之谦演唱会要来到广州啦!请关注用户留意接收!
        // 王五关注了公众号(被观察者):演唱会消息发布,接收到消息:薛之谦演唱会要来到广州啦!
        // 李四关注了公众号(被观察者):演唱会消息发布,接收到消息:薛之谦演唱会要来到广州啦!
        // 张三关注了公众号(被观察者):演唱会消息发布,接收到消息:薛之谦演唱会要来到广州啦!
    }

适用场景：
当一个抽象模型包含两个方面内容，其中一个方面依赖于另一个方面。
其他一个或多个对象的变化依赖于另一个对象的变化。
实现类似广播机制的功能，无需知道具体收听者，只需分发广播，系统中感兴趣的对象会自动接收该广播。
多层级嵌套使用，形成一种链式触发机制，使得事件具备跨域（跨越两种观察者类型）通知。

优点：
观察者和被观察者是松耦合（抽象耦合）的，符合依赖倒置原则。
分离了表示层（观察者）和数据逻辑层（被观察者），并且建立了一套触发机制，使得数据的变化可以相应到多个表示层上。
实现了一对多的通讯机制，支持事件注册机制，支持兴趣分发机制，当被观察者触发事件时，只有感兴趣的观察者可以接收到通知。

缺点：
如果观察者数量过多，则事件通知会耗时较长。
事件通知呈线性关系，如果其中一个观察者处理事件卡壳，会影响后续的观察者接收该事件。
如果观察者和被观察者之间存在循环依赖，则可能造成两者之间的循环调用，导致系统崩溃。

17、行为型-访问者模式：
访问者模式是一种将数据结构与数据操作分离的设计模式。是指封装一些作用于某种数据结构中的各元素的操作。

特征：可以在不改变数据结构的前提下定义作用于这些元素的新的操作。

访问者模式包含以下主要角色:
抽象访问者（Visitor）角色：定义了对每一个元素 （Element） 访问的行为，它的参数就是可以访问的元素，它的方法个数理论上来讲与元素类个数（Element的实现类个数）是一样的，从这点不难看出，访问者模式要求元素类的个数不能改变。
具体访问者（ConcreteVisitor）角色：给出对每一个元素类访问时所产生的具体行为。
抽象元素（Element）角色：定义了一个接受访问者的方法（ accept ），其意义是指，每一个元素都要可以被访问者访问。
具体元素（ConcreteElement）角色： 提供接受访问方法的具体实现，而这个具体的实现，通常情况下是使用访问者提供的访问该元素类的方法。
对象结构（Object Structure）角色：定义当中所提到的对象结构，对象结构是一个抽象表述，具体点可以理解为一个具有容器性质或者复合对象特性的类，它会含有一组元素（ Element ），并且可以迭代这些元素，供访问者访问。


// 访问者接口
public interface IVisitor {
    void visit(Engineer engineer);
    void visit(Pm pm);
}
// 具体的访问者类,访问者角色(CEO)
public class CeoVisitor implements IVisitor {
    @Override
    public void visit(Engineer engineer) {
        System.out.println(engineer.getName() + "KPI为:" + engineer.getKpi());
    }
    @Override
    public void visit(Pm pm) {
        System.out.println(pm.getName() + "KPI为:" + pm.getKpi());
    }
}
// 具体的访问者类,访问者角色(CTO)
public class CtoVisitor implements IVisitor {
    @Override
    public void visit(Engineer engineer) {
        System.out.println(engineer.getName() + "工作内容:" + engineer.getCodeLine() + "行代码");
    }
    @Override
    public void visit(Pm pm) {
        System.out.println(pm.getName() + "工作内容:" + pm.getProject() + "个项目");
    }
}
@Data
// 抽象元素(员工)
public abstract class Employee {
    private String name;
    private Integer kpi;
    public Employee(String name) {
        this.name = name;
        this.kpi = new Random().nextInt(10);
    }
    public abstract void accept(IVisitor visitor);
}
// 具体元素(程序员)
public class Engineer extends Employee {
    public Engineer(String name) {
        super(name);
    }
    @Override
    public void accept(IVisitor visitor) {
        visitor.visit(this);
    }
    public Integer getCodeLine() {
        return new Random().nextInt(10000);
    }
}
// 具体元素(项目经理)
public class Pm extends Employee {
    public Pm(String name) {
        super(name);
    }
    @Override
    public void accept(IVisitor visitor) {
        visitor.visit(this);
    }
    public Integer getProject() {
        return new Random().nextInt(10);
    }
}
@AllArgsConstructor
public class Report {
    private List<Employee> employeeList;
    public void showReport(IVisitor visitor) {
        for (Employee employee : employeeList) {
            employee.accept(visitor);
        }
    }
}
    // 测试
    public static void main(String[] args){
      List<Employee> employeeList = new ArrayList<>();
      employeeList.add(new Engineer("工程师A"));
      employeeList.add(new Engineer("工程师B"));
      employeeList.add(new Engineer("项目经理A"));
      employeeList.add(new Engineer("工程师C"));
      employeeList.add(new Engineer("工程师D"));
      employeeList.add(new Engineer("项目经理B"));
      Report report = new Report(employeeList);
      System.out.println("=============CEO==============");
      report.showReport(new CeoVisitor());
      System.out.println("=============CTO==============");
      report.showReport(new CtoVisitor());
      // =============CEO==============
      // 工程师AKPI为:2
      // 工程师BKPI为:4
      // 项目经理AKPI为:4
      // 工程师CKPI为:2
      // 工程师DKPI为:0
      // 项目经理BKPI为:0
      // =============CTO==============
      // 工程师A工作内容:5811行代码
      // 工程师B工作内容:9930行代码
      // 项目经理A工作内容:2163行代码
      // 工程师C工作内容:4591行代码
      // 工程师D工作内容:333行代码
      // 项目经理B工作内容:3940行代码
    }



在访问者模式使用了双分派技术，所谓双分派技术就是在选择方法的时候，不仅仅要根据消息接收者的运行时区别，还要根据参数的运行时区别。
在访问者模式中，客户端将具体状态当做参数传递给具体访问者，这里完成第一次分派，然后具体访问者作为参数的“具体状态”中的方法，同时也将自己this作为参数传递进去，
这里就完成了第二次分派。双分派意味着得到的执行操作决定于请求的种类和接受者的类型。

适用场景：

数据结构稳定，作用于数据结构的操作经常变化的场景。
需要数据结构与数据操作分离的场景。
需要对不同数据类型（元素）进行操作，而不使用分支判断具体类型的场景。
优点：

解耦了数据结构与数据操作，使得操作集合可以独立变化。
扩展性好：可以通过扩展访问者角色，实现对数据集的不同操作。
元素具体类型并非单一，访问者均可操作。
各角色职责分离，符合单一职责原则。
缺点：

无法增加元素类型：若系统数据结构对象易于变化，经常有新的数据对象增加进来，则访问者类必须增加对应元素类型的操作，违背了开闭原则。
具体元素变更困难：具体元素增加属性，删除属性等操作会导致对应的访问者类需要进行相应的修改，尤其当有大量访问者类时，修改访问太大。
违背依赖倒置原则：为了达到“区别对待”，访问者依赖的是具体元素类型，而不是抽象。


18、行为型-中介者模式：
中介者模式又称为调解者模式或调停者模式。用一个中介对象封装一系列的对象交互，中介者使各对象不需要显示地相互作用，从而使其耦合松散，而且可以独立地改变它们之间的交互。

核心：通过中介者解耦系统各层次对象的直接耦合，层次对象的对外依赖通信统统交由中介者转发。

中介者模式包含以下主要角色：

抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。
具体中介者（ConcreteMediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。
抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。
具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。

// 抽象同事类
@AllArgsConstructor
public class Person {
    protected String name;
    protected MediatorCompany mediatorCompany;
}
// 房主
public class HouseOwner extends Person {
    public HouseOwner(String name, MediatorCompany mediatorCompany) {
        super(name, mediatorCompany);
    }
    // 联络方法
    public void connection(String message) {
        mediatorCompany.connection(this, message);
    }
    // 获取消息
    public void getMessage(String message) {
        System.out.println("房主" + name + "获取到的信息:" + message);
    }
}
// 租客
public class Tenant extends Person {
    public Tenant(String name, MediatorCompany mediatorCompany) {
        super(name, mediatorCompany);
    }
    public void connection(String message) {
        mediatorCompany.connection(this, message);
    }
    public void getMessage(String message) {
        System.out.println("租客" + name + "获取到的信息:" + message);
    }
}
// 中介公司(中介者)
@Data
public class MediatorCompany {
    private HouseOwner houseOwner;
    private Tenant tenant;
    public void connection(Person person, String message) {
        // 房主需要通过中介获取租客信息
        if (person.equals(houseOwner)) {
            this.tenant.getMessage(message);
        } else { // 反之租客通过中介获取房主信息
            this.houseOwner.getMessage(message);
        }
    }
}
    // 测试
    public static void main(String[] args){
        // 先创建三个角色，中介公司，房主，租客
        MediatorCompany mediatorCompany = new MediatorCompany();
        // 房主和租客都在同一家中介公司
        HouseOwner houseOwner = new HouseOwner("张三", mediatorCompany);
        Tenant tenant = new Tenant("李四", mediatorCompany);
        // 中介公司获取房主和租客的信息
        mediatorCompany.setHouseOwner(houseOwner);
        mediatorCompany.setTenant(tenant);
        // 房主和租客都在这家中介公司发布消息，获取到对应的消息
        tenant.connection(tenant.name + "想租一房一厅!");
        houseOwner.connection(houseOwner.name + "这里有!来看看呗!");
        // 测试结果
        // 房主张三获取到的信息:李四想租一房一厅!
        // 租客李四获取到的信息:张三这里有!来看看呗!
    }


但是，中介者对象封装了对象之间的关联关系，导致中介者对象变得比较庞大复杂，所承担的责任也比较多，维护起来也比较困难，
它需要知道每个对象和他们之间的交互细节，如果它出问题，将会导致整个系统都会出问题。

适用场景：
系统中对象之间存在复杂的引用关系，产生的我相互依赖关系结构混乱且难以理解。
交互的公共行为，如果需要改变行为则可以增加新的中介者类。

优点：
减少类间的依赖，将多对多依赖转化成了一对多，降低了类间耦合。
类间各司其职，符合迪米特法则。

缺点：
中介者模式中将原本多个对象直接的相互依赖变成了中介者和多个同事类的依赖关系。当同事类越多时，中介者就会越臃肿，变得复杂且难以维护。


19、行为型-命令模式：
命令模式的本质是将请求封装成对象，将发出命令与执行命令的责任分开，命令的发送者和接收者完全解耦，发送者只需知道如何发送命令，不需要关心命令是如何实现的，甚至是否执行成功都不需要理会。
命令模式的关键在于引入了抽象命令接口，发送者针对抽象命令接口编程，只有实现了抽象命令接口的具体命令才能与接收者相关联。
使用命令模式的优势在于降低了系统的耦合度，而且新命令可以很方便添加到系统中，也容易设计一个组合命令。但缺点在于会导致某些系统有过多的具体命令类，因为针对每一个命令都需要设计一个具体命令类。

在命令模式中，通常包含以下几个角色：
命令接口（Command）：这是一个抽象类或接口，声明了执行命令的接口。
具体命令（Concrete Command）：实现了命令接口的类，具体执行命令。它持有一个接收者（Receiver）对象的引用，这个引用是在创建具体命令时传入的。
调用者（Invoker）：要求命令对象执行请求。调用者会持有一个或多个命令对象的引用，并可以调用命令对象的执行方法。
接收者（Receiver）：执行命令的对象。它知道如何执行命令相关的操作。接收者可以是一个类，也可以是其接口或抽象类的实例。
客户端（Client）：创建命令对象，并设置命令对象的接收者。

命令模式的工作流程
客户端创建具体的命令对象，并设置它的接收者。
客户端将命令对象传给调用者。
调用者通过命令对象来调用接收者的相关操作。

命令模式的优点
降低耦合度：命令模式将调用操作的对象与知道如何执行该操作的对象解耦。
增加灵活性：由于命令对象可以在运行时动态地创建、组合和执行，因此可以很容易地添加新的命令，修改现有命令，以及将命令组合成复合命令。
支持撤销操作：通过实现一个可撤销的命令，可以在执行命令后撤销它，恢复到执行前的状态。
支持日志记录：命令对象可以将它们的执行记录到日志中，这有助于实现事务处理、系统恢复等高级功能。

示例代码：
当然，下面是一个简单的命令模式示例代码，用于模拟一个遥控器（Invoker）控制电视（Receiver）的开关操作。

首先，我们定义一个命令接口（Command），它有一个执行方法（execute）。

// 命令接口  
public interface Command {  
    void execute();  
}


然后，我们定义电视类（TV），它是接收者（Receiver），具有打开和关闭的方法。
// 接收者 - 电视  
public class TV {  
    public void on() {  
        System.out.println("电视已打开");  
    }  
  
    public void off() {  
        System.out.println("电视已关闭");  
    }  
}

接下来，我们定义两个具体命令类（TVOnCommand 和 TVOffCommand），它们实现了命令接口，并持有电视对象的引用。
// 具体命令 - 打开电视  
public class TVOnCommand implements Command {  
    private TV tv;  
  
    public TVOnCommand(TV tv) {  
        this.tv = tv;  
    }  
  
    @Override  
    public void execute() {  
        tv.on();  
    }  
}  
  
// 具体命令 - 关闭电视  
public class TVOffCommand implements Command {  
    private TV tv;  
  
    public TVOffCommand(TV tv) {  
        this.tv = tv;  
    }  
  
    @Override  
    public void execute() {  
        tv.off();  
    }  
}

最后，我们定义遥控器类（RemoteControl），它是调用者（Invoker），可以存储命令对象，并调用它们的执行方法。
// 调用者 - 遥控器  
public class RemoteControl {  
    private Command command;  
  
    // 设置命令  
    public void setCommand(Command command) {  
        this.command = command;  
    }  
  
    // 执行命令  
    public void pressButton() {  
        if (command != null) {  
            command.execute();  
        }  
    }  
}

现在，我们可以编写一个客户端（Client）来测试这个命令模式。
public class CommandPatternDemo {  
    public static void main(String[] args) {  
        // 创建接收者  
        TV tv = new TV();  
  
        // 创建具体命令  
        Command tvOnCommand = new TVOnCommand(tv);  
        Command tvOffCommand = new TVOffCommand(tv);  
  
        // 创建调用者  
        RemoteControl remote = new RemoteControl();  
  
        // 设置命令并执行  
        remote.setCommand(tvOnCommand);  
        remote.pressButton(); // 输出：电视已打开  
  
        remote.setCommand(tvOffCommand);  
        remote.pressButton(); // 输出：电视已关闭  
    }  
}

在这个示例中，遥控器（RemoteControl）是调用者，它持有一个命令对象（Command）的引用。通过调用setCommand方法，我们可以将不同的命令对象（如TVOnCommand或TVOffCommand）传递给遥控器。
然后，通过调用pressButton方法，遥控器会执行当前设置的命令对象的execute方法，从而控制电视的开关。这样，遥控器与电视之间的耦合度就降低了，因为遥控器不需要知道电视的具体实现细节，只需要知道如何执行命令即可。


20、行为型-状态模式：
命令模式是对命令的封装，每一个命令都是一个操作：请求的一方发出请求要求执行一个操作；接收的一方收到请求，并执行操作。命令模式解耦了请求方和接收方，请求方只需请求执行命令，不用关心命令是怎样被接收，怎样被操作以及是否被执行等。本质：解耦命令的请求与处理。

命令模式包含以下主要角色：

抽象命令类（Command）角色： 定义命令的接口，声明执行的方法。
具体命令（Concrete Command）角色：具体的命令，实现命令接口；通常会持有接收者，并调用接收者的功能来完成命令要执行的操作。
实现者/接收者（Receiver）角色： 接收者，真正执行命令的对象。任何类都可能成为一个接收者，只要它能够实现命令要求实现的相应功能。
调用者/请求者（Invoker）角色： 要求命令对象执行请求，通常会持有命令对象，可以持有很多的命令对象。这个是客户端真正触发命令并要求命令执行相应操作的地方，也就是说相当于使用命令对象的入口。

// 播放器类
public class Player {
    public void play() {
        System.out.println("正常播放");
    }
    public void pause() {
        System.out.println("暂停播放");
    }
    public void stop() {
        System.out.println("停止播放");
    }
}
// 命令接口
public interface IAction {
    void excuse();
}
// 播放命令类
@AllArgsConstructor
public class PlayAction implements IAction {
    private Player player;
    @Override
    public void excuse() {
        this.player.play();
    }
}
// 暂停命令类
@AllArgsConstructor
public class PauseAction implements IAction {
    private Player player;
    @Override
    public void excuse() {
        this.player.pause();
    }
}
// 停止命令类
@AllArgsConstructor
public class StopAction implements IAction{
    private Player player;
    @Override
    public void excuse() {
        this.player.stop();
    }
}
// 控制器
public class Controller {
    public void excuse(IAction action) {
        action.excuse();
    }
}
   // 测试方法
   public static void main(String[] args) {
        // 正常播放
        new Controller().excuse(new PlayAction(new Player()));
        // 暂停播放
        new Controller().excuse(new PauseAction(new Player()));
        // 停止播放
        new Controller().excuse(new StopAction(new Player()));
    }


适用场景：
现实语义中具备“命令”的操作（如命令菜单，shell命令...）。
请求调用者和请求接收者需要解耦，使得调用者和接收者不直接交互。
需要抽象出等待执行的行为，比如撤销操作和恢复操作等。
需要支持命令宏（即命令组合操作）。

优点：
通过引入中间件（抽象接口），解耦了命令的请求与实现。
扩展性良好，可以很容易地增加新命令。
支持组合命令，支持命令队列。
可以在现有的命令的基础上，增加额外功能。

缺点：
具体命令类可能过多。
增加 了程序的复杂度，理解更加困难。


21、行为型-备忘录模式：
备忘录模式又称为快照模式（Snapshot Pattern）或令牌模式（Token Pattern），是指在不破坏封装的前提下，
捕获一个对象的内部状态，并在对象之外保存这个状态，这样以后就可将该对象恢复到原先保存的状态。
特征：“后悔药”

备忘录模式的主要角色如下：
发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。
备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。
管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。
备忘录有两个等效的接口：

窄接口：管理者(Caretaker)对象（和其他发起人对象之外的任何对象）看到的是备忘录的窄接口(narror Interface)，这个窄接口只允许他把备忘录对象传给其他的对象。
宽接口：与管理者看到的窄接口相反，发起人对象可以看到一个宽接口(wide Interface)，这个宽接口允许它读取所有的数据，以便根据这些数据恢复这个发起人对象的内部状态。

“白箱”备忘录模式：
下面就以游戏打怪为简单的例子进行代码实现（下面“黑箱”同这个例子）：
备忘录角色对任何对象都提供一个宽接口，备忘录角色的内部所存储的状态就对所有对象公开。


// 游戏角色类
@Data
public class GameRole {
    private Integer vit; // 生命力
    private Integer atk; // 攻击力
    private Integer def; // 防御力
    // 初始化状态
    public void init() {
        this.vit = 100;
        this.atk = 100;
        this.def = 100;
    }
    // 战斗到0
    public void fight() {
        this.vit = 0;
        this.atk = 0;
        this.def = 0;
    }
    // 保存角色状态
    public RoleStateMemento saveState() {
        return new RoleStateMemento(this.vit, this.atk, this.def);
    }
    // 回复角色状态
    public void recoverState(RoleStateMemento roleStateMemento) {
        this.vit = roleStateMemento.getVit();
        this.atk = roleStateMemento.getAtk();
        this.def = roleStateMemento.getDef();
    }
    // 展示状态
    public void showState() {
        System.out.println("角色生命力:" + this.vit);
        System.out.println("角色攻击力:" + this.atk);
        System.out.println("角色防御力:" + this.def);
    }
}
// 游戏状态存储类(备忘录类)
@Data
@AllArgsConstructor
public class RoleStateMemento {
    private Integer vit; // 生命力
    private Integer atk; // 攻击力
    private Integer def; // 防御力
}
// 角色状态管理者类
@Data
public class RoleStateCaretaker {
    private RoleStateMemento roleStateMemento;
}
    // 测试结果
    public static void main(String[] args){
      System.out.println("===========打boss前状态===========");
      GameRole gameRole = new GameRole();
      gameRole.init();
      gameRole.showState();
      // 保存进度
      RoleStateCaretaker roleStateCaretaker = new RoleStateCaretaker();
      roleStateCaretaker.setRoleStateMemento(gameRole.saveState());
      System.out.println("===========打boss后状态===========");
      gameRole.fight();
      gameRole.showState();
      System.out.println("===========恢复状态===========");
      gameRole.recoverState(roleStateCaretaker.getRoleStateMemento());
      gameRole.showState();
      // ===========打boss前状态===========
      // 角色生命力:100
      // 角色攻击力:100
      // 角色防御力:100
      // ===========打boss后状态===========
      // 角色生命力:0
      // 角色攻击力:0
      // 角色防御力:0
      // ===========恢复状态===========
      // 角色生命力:100
      // 角色攻击力:100
      // 角色防御力:100
    }
 
"白箱"备忘录模式是破坏封装性的，但是通过程序员自律，同样可以在一定程度上实现大部分的用意。

“黑箱”备忘录模式:
备忘录角色对发起人对象提供了一个宽接口，而为其他对象提供一个窄接口，
在Java语言中，实现双重接口的办法就是将备忘录类设计成发起人类的内部成员类。
将RoleStateMemento设为GameRole的内部类，从而将RoleStateMemento对象封装在GameRole 里面；
在外面提供一个标识接口Memento给RoleStateCaretaker及其他对象使用。
这样GameRole类看到的是RoleStateMemento所有的接口，
而RoleStateCaretaker及其他对象看到的仅仅是标识接口Memento所暴露出来的接口，从而维护了封装型。


// 窄接口,标识接口
public interface Memento {
}
// 角色状态管理者类
@Data
public class RoleStateCaretaker {
    private Memento memento;
}
// 游戏角色类
@Data
public class GameRole {
    private Integer vit; // 生命力
    private Integer atk; // 攻击力
    private Integer def; // 防御力
    // 初始化状态
    public void init() {
        this.vit = 100;
        this.atk = 100;
        this.def = 100;
    }
    // 战斗到0
    public void fight() {
        this.vit = 0;
        this.atk = 0;
        this.def = 0;
    }
    // 保存角色状态
    public RoleStateMemento saveState() {
        return new RoleStateMemento(this.vit, this.atk, this.def);
    }
    // 回复角色状态
    public void recoverState(Memento memento) {
        RoleStateMemento roleStateMemento = (RoleStateMemento) memento;
        this.vit = roleStateMemento.getVit();
        this.atk = roleStateMemento.getAtk();
        this.def = roleStateMemento.getDef();
    }
    // 展示状态
    public void showState() {
        System.out.println("角色生命力:" + this.vit);
        System.out.println("角色攻击力:" + this.atk);
        System.out.println("角色防御力:" + this.def);
    }
    // 备忘录内部类
    @Data
    @AllArgsConstructor
    private class RoleStateMemento implements Memento {
        private Integer vit; // 生命力
        private Integer atk; // 攻击力
        private Integer def; // 防御力
    }
}
    // 测试结果
    public static void main(String[] args){
      System.out.println("===========打boss前状态===========");
      GameRole gameRole = new GameRole();
      gameRole.init();
      gameRole.showState();
      // 保存进度
      RoleStateCaretaker roleStateCaretaker = new RoleStateCaretaker();
      roleStateCaretaker.setMemento(gameRole.saveState());
      System.out.println("===========打boss后状态===========");
      gameRole.fight();
      gameRole.showState();
      System.out.println("===========恢复状态===========");
      gameRole.recoverState(roleStateCaretaker.getMemento());
      gameRole.showState();
      // ===========打boss前状态===========
      // 角色生命力:100
      // 角色攻击力:100
      // 角色防御力:100
      // ===========打boss后状态===========
      // 角色生命力:0
      // 角色攻击力:0
      // 角色防御力:0
      // ===========恢复状态===========
      // 角色生命力:100
      // 角色攻击力:100
      // 角色防御力:100
    }

适用场景：
需要保存历史快照的场景。
希望在对象之外保存状态，且除了自己其他类对象无法访问状态保存具体内容。

优点：
简化发起人实体类职责，隔离状态存储与获取，实现了信息的封装，客户端无需关心状态的保存细节。
提供状态回滚功能。

缺点：
消耗资源：如果需要保存的状态过多时，每一次保存都会消耗很多内存。

	
22、行为型-迭代器模式：
迭代器模式又称为游标模式（Cursor Pattern），它提供一种顺序访问集合/容器对象元素的方法，而又无须暴露结合内部表示。
本质：抽离集合对象迭代行为到迭代器中，提供一致访问接口。

迭代器模式主要包含以下角色：
抽象聚合（Aggregate）角色：定义存储、添加、删除聚合元素以及创建迭代器对象的接口。
具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。
抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、next() 等方法。
具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。

// 迭代器接口
public interface Iterator<T> {
    Boolean hasNext();
    T next();
}
// 迭代器接口实现类
public class IteratorImpl<T> implements Iterator<T> {
    private List<T> list;
    private Integer cursor;
    private T element;
 
    public IteratorImpl(List<T> list) {
        this.list = list;
    }
    @Override
    public Boolean hasNext() {
        return cursor < list.size();
    }
    @Override
    public T next() {
        element = list.get(cursor);
        cursor++;
        return element;
    }
}
// 容器接口
public interface Aggregate<T> {
    void add(T t);
    void remove(T t);
    Iterator<T> iterator();
}
// 容器接口实现类
public class AggregateImpl<T> implements Aggregate<T> {
    private List<T> list = new ArrayList<>();
    @Override
    public void add(T t) {
        list.add(t);
    }
    @Override
    public void remove(T t) {
        list.remove(t);
    }
    @Override
    public Iterator<T> iterator() {
        return new IteratorImpl<>(list);
    }
}

适用场景：
访问一个集合对象的内容而无需暴露它的内部表示。
为遍历不同的集合结构提供一个统一的访问接口。

优点：
多态迭代：为不同的聚合结构提供一致的遍历接口，即一个迭代接口可以访问不同的聚集对象。
简化集合对象接口：迭代器模式将集合对象本身应该提供的元素迭代接口抽取到了迭代器中，使集合对象无须关心具体迭代行为。
元素迭代功能多样化：每个集合对象都可以提供一个或多个不同的迭代器，使的同种元素聚合结构可以有不同的迭代行为。
解耦迭代与集合：迭代器模式封装了具体的迭代算法，迭代算法的变化，不会影响到集合对象的架构。

缺点：
对于比较简单的遍历（像数组或者有序列表），使用迭代器方式遍历较为繁琐。
增加了类的个数，在一定程度上增加了系统的复杂性。


23、行为型-解释器模式：
解释器模式给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。
特征：为了解释一种语言，而为语言创建的解释器。

解释器模式包含以下主要角色：
抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。
终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。
非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。
环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。
客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。


// 抽象角色 定义解释器
public interface Expression {
    int interpret();
}
@AllArgsConstructor
public class NumberTerminal implements Expression {
    private int number;
    @Override
    public int interpret() {
        return this.number;
    }
}
// 非终结表达式（抽象类）
@AllArgsConstructor
public abstract class NonTerminal implements Expression {
    protected Expression left;
    protected Expression right;
}
// 非终结表达式（加法）
public class PlusNonTerminal extends NonTerminal implements Expression {
    public PlusNonTerminal(Expression left, Expression right) {
        super(left, right);
    }
    @Override
    public int interpret() {
        return left.interpret() + right.interpret();
    }
}
// 非终结表达式（减法）
public class MinusNonTerminal extends NonTerminal implements Expression {
    public MinusNonTerminal(Expression left, Expression right) {
        super(left, right);
    }
    @Override
    public int interpret() {
        return left.interpret() - right.interpret();
    }
}
// 非终结表达式（乘法）
public class MclNonTerminal extends NonTerminal implements Expression {
    public MclNonTerminal(Expression left, Expression right) {
        super(left, right);
    }
    @Override
    public int interpret() {
        return left.interpret() * right.interpret();
    }
}
// 非终结表达式（除法）
public class DivisionNonTerminal extends NonTerminal implements Expression {
    public DivisionNonTerminal(Expression left, Expression right) {
        super(left, right);
    }
    @Override
    public int interpret() {
        return left.interpret() / right.interpret();
    }
}
// 计算器类（实现运算逻辑）
public class Cal {
    private Expression left;
    private Expression right;
    private Integer result;
    public Cal(String expression) {
        this.parse(expression);
    }
    private Integer parse(String expression) {
        // 获取表达式元素
        String [] elements = expression.split(" ");
        for (int i = 0; i < elements.length; i++) {
            String element = elements[i];
            // 判断是否是运算符号
            if (OperatorUtils.isOperator(element)) {
                // 运算符号的右边就是右终结符
                right = new NumberTerminal(Integer.valueOf(elements[++i]));
                //计算结果
                result = OperatorUtils.getNonTerminal(left, right, element).interpret();
                // 计算结果重新成为左终结符
                left = new NumberTerminal(result);
            } else {
                left = new NumberTerminal(Integer.valueOf(element));
            }
        }
        return result;
    }
    public Integer cal() {
        return result;
    }
 
}
// 操作工具类
public class OperatorUtils {
    // 判断是不是非终结符
    public static boolean isOperator(String symbol) {
        return symbol.equals("+") || symbol.equals("-") || symbol.equals("*")|| symbol.equals("/");
    }
    // 简单工厂
    public static NonTerminal getNonTerminal(Expression left, Expression right, String symbol) {
        if (symbol.equals("+")) {
            return new PlusNonTerminal(left, right);
        } else if (symbol.equals("-")) {
            return new MinusNonTerminal(left, right);
        } else if (symbol.equals("*")) {
            return new MclNonTerminal(left, right);
        } else if (symbol.equals("/")) {
            return new DivisionNonTerminal(left, right);
        }
        return null;
    }
}
    // 测试
    // PS：此处进行的逻辑仅仅实现从左到右运算，并没有先乘除后加减的逻辑
    public static void main(String[] args) {
        System.out.println(new Cal("10 + 20 - 40 * 60").cal()); // -600
        System.out.println(new Cal("20 + 50 - 60 * 2").cal()); // 20
    }
	
Spring中的解释器模式：

public static void main(String[] args) {
        ExpressionParser expressionParser = new SpelExpressionParser();
        org.springframework.expression.Expression expression = expressionParser.parseExpression("10 + 20 + 30 * 4");
        Integer value = expression.getValue(Integer.class);
        System.out.println(value); // 150
        expression = expressionParser.parseExpression("(10+20+30)*4");
        value = expression.getValue(Integer.class);
        System.out.println(value); // 240
    }

可以看到Spring中解释器写的是比较完善的，不仅有先乘除后加减和先括号进行运算的日常计算规则，
而且对于空格也并没有要求，仅需要写出完整的表达式即可运算出来。

适用场景：
一些重复出现的问题可以用一种简单的语言来进行表述。
一个简单语法需要解释的场景。

优点：
扩展性强：在解释器模式中由于语法是由很多类表示的，当语法规则更改时，只需修改相应的非终结符表达式即可；若扩展语法时，只需添加相应非终结符类即可。
增加了新的解释表达式的方式。
易于实现文法：解释器模式对应的文法应当是比较简单且易于实现的，过于复杂的语法并不适合使用解释器模式。

缺点：
语法规则较复杂时，会引起类膨胀。
执行效率比较低


数据结构与算法：
数据结构与算法相辅相成，不会孤立存在；数据结构是为算法服务的，
算法是作用在特定的数据结构之上（如数组具有随机访问的特点，二分查找算法需要用数组来存储数据）。
数据结构是静态的，只是组织数据的一种形式，若不在它的基础上操作、构建算法、孤立存在的数据结构是没有意义的。

数据结构与算法之数组：
数组是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。

无序数组：
public class MyArray {
	//声明一个数组
	private long[] arr;
	
	//有效数据的长度
	private int elements;
	
	//无参构造函数，默认长度为50
	public MyArray(){
		arr = new long[50];
	}
	
	public MyArray(int maxsize){
		arr = new long[maxsize];
	}
	
	
	//添加数据
	public void insert(long value){
		arr[elements] = value;
		elements++;
	}
	
	//显示数据
	public void display(){
		System.out.print("[");
		for(int i = 0;i < elements;i++){
			System.out.print(arr[i] + " ");
		}
		System.out.println("]");
	}
	
	//根据下标查找数据
	public long get(int index){
		if(index >= elements || index < 0){
			throw new ArrayIndexOutOfBoundsException();
		}else{
			return arr[index];
		}
	}
	
	/**
	 * 根据值查询
	 * @param value 需要被查询的值
	 * @return 被查询值的下标
	 */
	public int search(int value){
		//声明一个变量i用来记录该数据的下标值
		int i ;
		for(i = 0;i < elements;i++){
			if(value == arr[i]){
				break;
			}
		}
		//如果查询到最后一个元素依然没有找到
		if(i == elements){
			return -1;
		}else{
			return i;
		}
	}
	
	//根据下标删除数据
	public void delete(int index){
		if(index >= elements || index < 0){
			throw new ArrayIndexOutOfBoundsException();
		}else{
			//删除该元素后，后面所有元素前移一位
			for(int i = index; i < elements;i++){
				arr[i] = arr[i+1];
			}
			elements--;
		}
		
	}
	/**
	 * 替换数据
	 * @param index 被替换的下标
	 * @param newvalue 新的数据
	 */
	public void change(int index,int newvalue){
		if(index >= elements || index < 0){
			throw new ArrayIndexOutOfBoundsException();
		}else{
			arr[index] = newvalue;
		}
	} 
}

有序数组：
public class MyOrderArray { 
	private long[] arr;
	
	private int elements;
	
	public MyOrderArray(){
		arr = new long[50];
	}
	
	public MyOrderArray(int maxsize){
		arr = new long[maxsize];
	}
	
	//添加数据
	public void insert(int value){
		int i;
		for(i = 0;i < elements;i++){
			if(arr[i] > value){
				break;
			}
		}
		for(int j = elements;j > i;j--){
			arr[j] = arr[j -1];
		}
		arr[i] = value;
		elements++;
	}
	
	
	//删除数据
	public void delete(int index){
		if(index >=elements || index <0){
			throw new ArrayIndexOutOfBoundsException();
		}else{
			for(int i = index;i < elements; i++){
				arr[i] = arr[i+1];
			}
			elements--;
		}
	}
	
	//修改数据
	public void change(int index,int value){
		if(index >= elements || index < 0){
			throw new IndexOutOfBoundsException();
		}else{
			arr[index] = value;
		}
	}
	
	//根据下标查询数据
	public long get(int index){
		if(index >= elements || index < 0){
			throw new IndexOutOfBoundsException();
		}else{
			return arr[index];
		}
	}
	
	//展示数据
	public void display(){
		System.out.print("[");
		for(int i = 0; i < elements;i++){
			System.out.print(arr[i] + " ");
		}
		System.out.println("]");
	}
	
	
	//二分法查找数据
	public int binarySearch(long value){
			//声明三个指针分别指向数组的头，尾，中间
			int low = 0;
			int pow = elements;
			int middle = 0;
			
			while(true){
				middle = (low + pow) / 2;
				//如果中指针所指的值等于带查询数
				if(arr[middle] == value){
					return middle;
				}else if(low > pow){
					return -1;
				}else{
					if(arr[middle] > value){
						//待查询的数在左边,右指针重新改变指向
						pow = middle-1;
					}else{
						//带查询的数在右边，左指针重新改变指向
						low = middle +1;
					}
				}
			}
	}
}


数据结构与算法之栈：
栈（stack）是一种容器，可存入数据元素、访问元素、删除元素，它的特点在于只能允许在容器的一端（称为栈顶端指标，英语：top）
进行加入数据（英语：push）和输出数据（英语：pop）的运算。

1.由于栈数据结构只允许在一端进行操作，因而按照后进先出的原理运作。
2.栈可以用顺序表实现，也可以用链表实现。

package mystack;

public class MyStack {

    //栈的底层使用数组来存储数据
    //private int[] elements;
    int[] elements; //测试时使用

    public MyStack() {
        elements = new int[0];
    }

    //添加元素
    public void push(int element) {
        //创建一个新的数组
        int[] newArr = new int[elements.length + 1];
        //把原数组中的元素复制到新数组中
        for (int i = 0; i < elements.length; i++) {
            newArr[i] = elements[i];
        }
        //把添加的元素放入新数组中
        newArr[elements.length] = element;
        //使用新数组替换旧数组
        elements = newArr;
    }

    //取出栈顶元素
    public int pop() {
        //当栈中没有元素
        if (is_empty()) {
            throw new RuntimeException("栈空");
        }
        //取出数组的最后一个元素
        int element = elements[elements.length - 1];
        //创建一个新数组
        int[] newArr = new int[elements.length - 1];
        //原数组中除了最后一个元素其他元素放入新数组
        for (int i = 0; i < elements.length - 1; i++) {
            newArr[i] = elements[i];
        }
        elements = newArr;
        return element;
    }

    //查看栈顶元素
    public int peek() {
        return elements[elements.length - 1];
    }

    //判断栈是否为空
    public boolean is_empty() {
        return elements.length == 0;
    }

    //查看栈的元素个数
    public int size() {
        return elements.length;
    }
}
	

数据结构与算法之队列：
队列（Queue）是只允许在一端进行插入操作，而在另一端进行删除操作的线性表。
队列是一种先进先出的（First In First Out）的线性表，简称FIFO。允许插入的一端为队尾，允许删除的一端为队头。
1.队列不允许在中间部位进行操作！
2.同栈一样，队列也可以用顺序表或者链表实现。

public class MyQueue {

    int[] elements;

    public MyQueue() {
        elements = new int[0];
    }

    //入队
    public void enqueue(int element) {
        //创建一个新的数组
        int[] newArr = new int[elements.length + 1];
        //把原数组中的元素复制到新数组中
        for (int i = 0; i < elements.length; i++) {
            newArr[i] = elements[i];
        }
        //把添加的元素放入新数组中
        newArr[elements.length] = element;
        //使用新数组替换旧数组
        elements = newArr;
    }

    //出队
    public int dequeue() {
        if (isEmpty()) {
            throw new RuntimeException("队空，无数据");
        }
        //把数组中第1个元素取出来
        int element = elements[0];
        //创建一个新数组
        int[] newArr = new int[elements.length - 1];
        //把原数组除了第一个数据，其他存入新数组
        for (int i = 0; i < newArr.length; i++) {
            newArr[i] = elements[i + 1];
        }
        //新数组替换旧数组
        elements = newArr;

        return element;
    }

    //判断是否队空
    public boolean isEmpty() {
        return elements.length==0;
    }

    //获取队列长度
    public int size() {
        return elements.length;
    }
}

数据结构与算法之链表：
单链表：
单链表也叫单向链表，是链表中最简单的一种形式，它的每个节点包含两个域，一个信息域（元素域）和一个链接域。
这个链接指向链表中的下一个节点，而最后一个节点的链接域则指向一个空值。	
1.表元素域data用来存放具体的数据。
2.链接域next用来存放下一个节点的位置。

//一个节点
public class Node {

    //节点内容
    int data;
    //下一个节点
    Node next;

    public Node(int data) {
        this.data = data;
    }

    //为节点追加节点
    public Node append(Node node) {
        //当前节点
        Node currentNode = this;
        //循环向后找
        while (true) {
            //取出下一个节点
            Node nextNode = currentNode.next();
            //如果下一个节点为null，当前节点已经是最后一个节点
            if (nextNode == null) {
                break;
            }
            //赋给当前节点，无线向后找
            currentNode = nextNode;
        }
        //把需要追加的节点，追加为找到的当前节点（最后一个节点）的下一个节点
        currentNode.next = node;
        return this;
    }

    //显示所有节点信息
    public void show() {
        Node currentNode = this;
        while (true) {
            System.out.print(currentNode.data + " ");
            //取出下一个节点
            currentNode = currentNode.next;
            //如果是最后一个节点
            if (currentNode == null) {
                break;
            }
        }
        System.out.println();
    }

    //插入一个节点作为当前节点的下一个节点
    public void after(Node node) {
        //取出下一个节点，作为下下一个节点
        Node nextNext = next;
        //把新节点作为当前节点的下一个节点
        this.next = node;
        //把下下一个节点设置为新节点的下一个节点
        node.next = nextNext;

    }

    //删除下一个节点
    public void removeNode() {
        //取出下下一个节点
        Node newNext = next.next;
        //把下下一个节点设置为当前节点的下一个节点
        this.next = newNext;
    }

    //获取下一个节点
    public Node next() {
        return this.next;
    }

    //获取节点中的数据
    public int getData() {
        return this.data;
    }

    //判断节点是否为最后一个节点
    public boolean isLast() {
        return next == null;
    }

}

双向链表：
在双向链表中有两个指针域，一个是指向前驱结点的prev，一个是指向后继结点的next指针。

public class DoubleNode {
    //上一个节点
    DoubleNode pre = this;
    //下一个节点
    DoubleNode next = this;
    //节点数据
    int data;

    public DoubleNode(int data) {
        this.data = data;
    }

    //增加节点
    public void after(DoubleNode node) {
        //原来的下一个节点
        DoubleNode nextNext = next;
        //新节点作为当前节点的下一个节点
        this.next = node;
        //当前节点作为新节点的前一个节点
        node.pre = this;
        //原来的下一个节点作为新节点的下一个节点
        node.next = nextNext;
        //原来的下一个节点的上一个节点为新节点
        nextNext.pre = node;
    }

    //获取下一个节点
    public DoubleNode getNext() {
        return this.next;
    }

    //获取上一个节点
    public DoubleNode getPre() {
        return this.pre;
    }

    //获取数据
    public int getData() {
        return this.data;
    }
}

数据结构与算法之树：
树是一种非线性数据结构，它是由一个或多个结点组成的。

树的结构：
1、根节点：最顶上的唯一的一个；
2、双亲节点：子节点的父节点就叫做双亲节点；
3、子节点：双亲节点所产生的节点就是子节点
4、路径：从根节点到目标节点所走的路程叫做路径；
5、节点的度：有多少个子节点就有多少的度（最下面的度一定为0，所以是叶子节点）
6、节点的权：在节点中所存的数字
7、叶子节点：没有子节点的节点，就是没有下一代的节点；
8、子树：在整棵树中将一部分看成也是一棵树，即使只有一个节点也是一棵树，不过这个树是在整个大树职中的，包含的关系
9、层：就是族谱中有多少代的人；
10、树的高度：树的最大的层数：就是层数中的最大值
11、森林：多个树组成的集合

树结构的优点：
1.空间利用率比较高
2.可以非常快速地查找到最大值和最小值

树的种类：
无序树：树中任意节点的子节点之间没有顺序关系，这种树称为无序树，也称为自由树；

有序树：树中任意节点的子节点之间有顺序关系，这种树称为有序树；

二叉树：每个节点最多含有两个子树的树称为二叉树；
完全二叉树：对于一颗二叉树，假设其深度为d(d>1)。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树，其中满二叉树的定义是所有叶节点都在最底层的完全二叉树;
平衡二叉树（AVL树）：当且仅当任何节点的两棵子树的高度差不大于1的二叉树；
排序二叉树（二叉查找树（英语：Binary Search Tree），也称二叉搜索树、有序二叉树）；
霍夫曼树（用于信息编码）：带权路径最短的二叉树称为哈夫曼树或最优二叉树；
B树：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多余两个子树。

数据结构与算法之排序算法：

直接插入排序：

算法思想：
直接插入排序的核心思想就是：将数组中的所有元素依次跟前面已经排好的元素相比较，
如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过。
因此，从上面的描述中我们可以发现，直接插入排序可以用两个循环完成：

第一层循环：遍历待比较的所有数组元素
第二层循环：将本轮选择的元素(selected)与已经排好序的元素(ordered)相比较。
如果：selected > ordered，那么将二者交换

代码实现：
#直接插入排序
def insert_sort(L):
    #遍历数组中的所有元素，其中0号索引元素默认已排序，因此从1开始
    for x in range(1,len(L)):
    #将该元素与已排序好的前序数组依次比较，如果该元素小，则交换
    #range(x-1,-1,-1):从x-1倒序循环到0
        for i in range(x-1,-1,-1):
    #判断：如果符合条件则交换
            if L[i] > L[i+1]:
                temp = L[i+1]
                L[i+1] = L[i]
                L[i] = temp

希尔排序
算法思想：
希尔排序的算法思想：将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；
每次将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。
同样的：从上面的描述中我们可以发现：希尔排序的总体实现应该由三个循环完成：

第一层循环：将gap依次折半，对序列进行分组，直到gap=1
第二、三层循环：也即直接插入排序所需要的两次循环。具体描述见上。

代码实现：
#希尔排序
def insert_shell(L):
    #初始化gap值，此处利用序列长度的一般为其赋值
    gap = (int)(len(L)/2)
    #第一层循环：依次改变gap值对列表进行分组
    while (gap >= 1):
    #下面：利用直接插入排序的思想对分组数据进行排序
    #range(gap,len(L)):从gap开始
        for x in range(gap,len(L)):
    #range(x-gap,-1,-gap):从x-gap开始与选定元素开始倒序比较，每个比较元素之间间隔gap
            for i in range(x-gap,-1,-gap):
    #如果该组当中两个元素满足交换条件，则进行交换
                if L[i] > L[i+gap]:
                    temp = L[i+gap]
                    L[i+gap] = L[i]
                    L[i] =temp
    #while循环条件折半
        gap = (int)(gap/2)


简单选择排序
算法思想：
简单选择排序的基本思想：比较+交换。

从待排序序列中，找到关键字最小的元素；
如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换；
从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。
因此我们可以发现，简单选择排序也是通过两层循环实现。
第一层循环：依次遍历序列当中的每一个元素
第二层循环：将遍历得到的当前元素依次与余下的元素进行比较，符合最小元素的条件，则交换。

代码实现：
# 简单选择排序
def select_sort(L):
#依次遍历序列中的每一个元素
    for x in range(0,len(L)):
#将当前位置的元素定义此轮循环当中的最小值
        minimum = L[x]
#将该元素与剩下的元素依次比较寻找最小元素
        for i in range(x+1,len(L)):
            if L[i] < minimum:
                temp = L[i];
                L[i] = minimum;
                minimum = temp
#将比较后得到的真正的最小值赋值给当前位置
        L[x] = minimum	

堆排序
堆的概念
堆：本质是一种数组对象。特别重要的一点性质：<b>任意的叶子节点小于（或大于）它所有的父节点</b>。
对此，又分为大顶堆和小顶堆，大顶堆要求节点的元素都要大于其孩子，
小顶堆要求节点元素都小于其左右孩子，两者对左右孩子的大小关系不做任何要求。
利用堆排序，就是基于大顶堆或者小顶堆的一种排序方法。下面，我们通过大顶堆来实现。

基本思想：
堆排序可以按照以下步骤来完成：

1.首先将序列构建称为大顶堆；
（这样满足了大顶堆那条性质：位于根节点的元素一定是当前序列的最大值）

2.取出当前大顶堆的根节点，将其与序列末尾元素进行交换；
（此时：序列末尾的元素为已排序的最大值；由于交换了元素，当前位于根节点的堆并不一定满足大顶堆的性质）
对交换后的n-1个序列元素进行调整，使其满足大顶堆的性质；

3.重复2.3步骤，直至堆中只有1个元素为止
	
#-------------------------堆排序--------------------------------
#**********获取左右叶子节点**********
def LEFT(i):
    return 2*i + 1
def RIGHT(i):
    return 2*i + 2
#********** 调整大顶堆 **********
#L:待调整序列 length: 序列长度 i:需要调整的结点
def adjust_max_heap(L,length,i):
#定义一个int值保存当前序列最大值的下标
    largest = i
#执行循环操作：两个任务：1 寻找最大值的下标；2.最大值与父节点交换
    while (1):
#获得序列左右叶子节点的下标
        left,right = LEFT(i),RIGHT(i)
#当左叶子节点的下标小于序列长度 并且 左叶子节点的值大于父节点时，将左叶子节点的下标赋值给largest
        if (left < length) and (L[left] > L[i]):
            largest = left
            print('左叶子节点')
        else:
            largest = i
#当右叶子节点的下标小于序列长度 并且 右叶子节点的值大于父节点时，将右叶子节点的下标值赋值给largest
        if (right < length) and (L[right] > L[largest]):
            largest = right
            print('右叶子节点')
#如果largest不等于i 说明当前的父节点不是最大值，需要交换值
        if (largest != i):
            temp = L[i]
            L[i] = L[largest]
            L[largest] = temp
            i = largest
            print(largest)
            continue
        else:
            break
#********** 建立大顶堆 **********
def build_max_heap(L):
    length = len(L)
    for x in range((int)((length-1)/2),-1,-1):
        adjust_max_heap(L,length,x)
#********** 堆排序 **********
def heap_sort(L):
#先建立大顶堆，保证最大值位于根节点；并且父节点的值大于叶子结点
    build_max_heap(L)
#i：当前堆中序列的长度.初始化为序列的长度
    i = len(L)
#执行循环：1. 每次取出堆顶元素置于序列的最后(len-1,len-2,len-3...)
#         2. 调整堆，使其继续满足大顶堆的性质，注意实时修改堆中序列的长度
    while (i > 0):
        temp = L[i-1]
        L[i-1] = L[0]
        L[0] = temp
#堆中序列长度减1
        i = i-1
#调整大顶堆
        adjust_max_heap(L,i,0)

冒泡排序
基本思想:
冒泡排序思路比较简单：

将序列当中的左右元素，依次比较，保证右边的元素始终大于左边的元素；
（ 第一轮结束后，序列最后一个元素一定是当前序列的最大值；）
对序列当中剩下的n-1个元素再次执行步骤1。
对于长度为n的序列，一共需要执行n-1轮比较
（利用while循环可以减少执行次数）

#冒泡排序
def bubble_sort(L):
    length = len(L)
#序列长度为length，需要执行length-1轮交换
    for x in range(1,length):
#对于每一轮交换，都将序列当中的左右元素进行比较
#每轮交换当中，由于序列最后的元素一定是最大的，因此每轮循环到序列未排序的位置即可
        for i in range(0,length-x):
            if L[i] > L[i+1]:
                temp = L[i]
                L[i] = L[i+1]
                L[i+1] = temp

快速排序
算法思想：
快速排序的基本思想：挖坑填数+分治法
从序列当中选择一个基准数(pivot)
在这里我们选择序列当中第一个数最为基准数
将序列当中的所有数依次遍历，比基准数大的位于其右侧，比基准数小的位于其左侧
重复步骤1.2，直到所有子集当中只有一个元素为止。
用伪代码描述如下：
1．i =L; j = R; 将基准数挖出形成第一个坑a[i]。
2．j--由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。
3．i++由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。
4．再重复执行2，3二步，直到i==j，将基准数填入a[i]中

代码实现：
#快速排序
#L：待排序的序列；start排序的开始index,end序列末尾的index
#对于长度为length的序列：start = 0;end = length-1
def quick_sort(L,start,end):
    if start < end:
        i , j , pivot = start , end , L[start]
        while i < j:
#从右开始向左寻找第一个小于pivot的值
            while (i < j) and (L[j] >= pivot):
                j = j-1
#将小于pivot的值移到左边
            if (i < j):
                L[i] = L[j]
                i = i+1 
#从左开始向右寻找第一个大于pivot的值
            while (i < j) and (L[i] < pivot):
                i = i+1
#将大于pivot的值移到右边
            if (i < j):
                L[j] = L[i]
                j = j-1
#循环结束后，说明 i=j，此时左边的值全都小于pivot,右边的值全都大于pivot
#pivot的位置移动正确，那么此时只需对左右两侧的序列调用此函数进一步排序即可
#递归调用函数：依次对左侧序列：从0 ~ i-1//右侧序列：从i+1 ~ end
        L[i] = pivot
#左侧序列继续排序
        quick_sort(L,start,i-1)
#右侧序列继续排序
        quick_sort(L,i+1,end)

归并排序
算法思想：
1.归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用分治法的一个典型的应用。
它的基本操作是：将已有的子序列合并，达到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。
2.归并排序其实要做两件事：
分解----将序列每次折半拆分
合并----将划分后的序列段两两排序合并
因此，归并排序实际上就是两个操作，拆分+合并
3.如何合并？
L[first...mid]为第一段，L[mid+1...last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first...last]并且也有序。
首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[]
重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[]
此时将temp[]中的元素复制给L[]，则得到的L[first...last]有序
4.如何分解？
在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列分组；
直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。

代码实现:

# 归并排序
#这是合并的函数
# 将序列L[first...mid]与序列L[mid+1...last]进行合并
def mergearray(L,first,mid,last,temp):
#对i,j,k分别进行赋值
    i,j,k = first,mid+1,0
#当左右两边都有数时进行比较，取较小的数
    while (i <= mid) and (j <= last):
        if L[i] <= L[j]:
            temp[k] = L[i]
            i = i+1
            k = k+1
        else:
            temp[k] = L[j]
            j = j+1
            k = k+1
#如果左边序列还有数
    while (i <= mid):
        temp[k] = L[i]
        i = i+1
        k = k+1
#如果右边序列还有数
    while (j <= last):
        temp[k] = L[j]
        j = j+1
        k = k+1
#将temp当中该段有序元素赋值给L待排序列使之部分有序
    for x in range(0,k):
        L[first+x] = temp[x]
# 这是分组的函数
def merge_sort(L,first,last,temp):
    if first < last:
        mid = (int)((first + last) / 2)
#使左边序列有序
        merge_sort(L,first,mid,temp)
#使右边序列有序
        merge_sort(L,mid+1,last,temp)
#将两个有序序列合并
        mergearray(L,first,mid,last,temp)
# 归并排序的函数
def merge_sort_array(L):
#声明一个长度为len(L)的空列表
    temp = len(L)*[None]
#调用归并排序
    merge_sort(L,0,len(L)-1,temp)	


基数排序
算法思想:
基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。
分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中
收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[ ]
对新形成的序列L[]重复执行分配和收集元素中的十位、百位...直到分配完该序列中的最高位，则排序结束
根据上述“基数排序”的展示，我们可以清楚的看到整个实现的过程

代码实现:
#************************基数排序****************************
#确定排序的次数
#排序的顺序跟序列中最大数的位数相关
def radix_sort_nums(L):
    maxNum = L[0]
#寻找序列中的最大数
    for x in L:
        if maxNum < x:
            maxNum = x
#确定序列中的最大元素的位数
    times = 0
    while (maxNum > 0):
        maxNum = (int)(maxNum/10)
        times = times+1
    return times
#找到num从低到高第pos位的数据
def get_num_pos(num,pos):
    return ((int)(num/(10**(pos-1))))%10
#基数排序
def radix_sort(L):
    count = 10*[None]       #存放各个桶的数据统计个数
    bucket = len(L)*[None]  #暂时存放排序结果
#从低位到高位依次执行循环
    for pos in range(1,radix_sort_nums(L)+1):
        #置空各个桶的数据统计
        for x in range(0,10):
            count[x] = 0
        #统计当前该位(个位，十位，百位....)的元素数目
        for x in range(0,len(L)):
            #统计各个桶将要装进去的元素个数
            j = get_num_pos(int(L[x]),pos)
            count[j] = count[j]+1
        #count[i]表示第i个桶的右边界索引
        for x in range(1,10):
            count[x] = count[x] + count[x-1]
        #将数据依次装入桶中
        for x in range(len(L)-1,-1,-1):
            #求出元素第K位的数字
            j = get_num_pos(L[x],pos)
            #放入对应的桶中，count[j]-1是第j个桶的右边界索引
            bucket[count[j]-1] = L[x]
            #对应桶的装入数据索引-1
            count[j] = count[j]-1
        # 将已分配好的桶中数据再倒出来，此时已是对应当前位数有序的表
        for x in range(0,len(L)):
            L[x] = bucket[x]


Java数据库及持久层技术以及多种数据库的应用：

MYSQL：MySQL 是一个流行的开源关系型数据库管理系统（RDBMS），它使用 SQL（结构化查询语言）来管理数据。

数据定义语言 (DDL)
CREATE DATABASE: 创建一个新的数据库。
ALTER DATABASE: 修改数据库的属性。
DROP DATABASE: 删除一个数据库。
CREATE TABLE: 创建一个新的数据表。
ALTER TABLE: 修改数据表的结构（如添加、删除或修改列）。
DROP TABLE: 删除一个数据表。
CREATE INDEX: 创建一个索引来提高查询效率。
DROP INDEX: 删除一个索引。
TRANCATE TABLE: 删除一个表中的所有行，并重置任何自增列的计数器

数据操纵语言 (DML)
INSERT INTO: 在数据表中插入新的行。
UPDATE: 修改数据表中的现有行。
DELETE: 从数据表中删除行。

数据查询语言 (DQL)
SELECT: 从数据表中检索数据。这是 SQL 中最常用的语句之一，可以与其他子句（如 WHERE、ORDER BY、GROUP BY、HAVING 等）结合使用，以实现复杂的查询。

数据控制语言 (DCL)
GRANT: 赋予用户或角色访问数据库的权限。
REVOKE: 撤销用户或角色的权限。

其他常用语句
USE: 选择要使用的数据库。
SHOW DATABASES: 显示所有数据库。
SHOW TABLES: 显示所选数据库中的所有数据表。
DESCRIBE 或 DESC: 显示数据表的结构。
EXPLAIN: 提供查询的执行计划，用于分析和优化查询性能。
LIMIT: 限制查询结果返回的行数。
OFFSET: 与 LIMIT 结合使用，指定从哪一行开始返回结果。
JOIN: 用于将两个或多个数据表的行组合起来，基于这些表之间的某些相关列之间的关系。
UNION: 用于合并两个或多个 SELECT 语句的结果集，但不返回任何重复的行。
UNION ALL: 与 UNION 类似，但会返回所有行，包括重复的行。

Mysql的调优方法有哪些？
MySQL的调优是一个复杂但重要的过程，它涉及到多个方面，包括硬件、软件配置、查询优化、索引优化等。以下是一些MySQL调优的主要方法，按照不同的类别进行归纳：

一、硬件与软件配置
服务器硬件：
增加内存：提高缓存的效果，例如增加InnoDB缓冲池的大小。
使用固态硬盘（SSD）：加快数据读取和写入速度。
根据需要配置多核CPU和高速网络。
MySQL配置：
调整innodb_buffer_pool_size：这是InnoDB存储引擎用于缓存数据和索引的内存区域，通常设置为总内存的50%到80%。
调整max_connections：根据并发连接数进行调整，以避免连接数耗尽。
调整query_cache_size和query_cache_type：开启查询缓存并调整其大小，但请注意，在某些情况下，查询缓存可能会成为性能瓶颈。
调整table_open_cache：增加打开表的数量缓存，以加速表访问。

二、查询优化
优化查询语句：
避免使用SELECT *，只选择需要的字段。
使用JOIN语句连接多个表，而非多次单表查询。
避免在WHERE子句中对字段进行函数操作，以免影响索引的使用。
使用合适的WHERE条件，减少返回的数据行数。
优化排序和分组操作，如使用索引进行排序。
索引优化：
使用适当的索引来加快查询速度，但避免过多的索引，因为索引也会占用存储空间并降低写操作的性能。
定期对索引进行优化和重建，以保持其性能。
使用EXPLAIN语句分析查询的执行计划，找出可能的性能瓶颈。

三、表结构与设计
选择合适的数据类型：在创建表时，选择合适的数据类型可以节省存储空间并提高查询性能。例如，对于小数字，使用TINYINT比INT更节省空间。
规范化与反规范化：规范化可以减少数据冗余和提高数据完整性，但可能会导致查询性能下降。在需要时，可以考虑使用反规范化来提高查询性能。

四、其他调优方法
分区和分表：对于数据量巨大的表，可以考虑使用分区或分表来提高查询效率。分区将一个表物理上分割成多个较小的、更易于管理的片段，而分表则是将一个大表拆分成多个结构相同的小表。
使用缓存：在应用程序中使用缓存可以减少对数据库的访问次数，提高读取速度。可以考虑使用如Redis或Memcached等内存数据库作为缓存层。
监控和调优：定期监控数据库的性能指标，如查询响应时间、连接数、磁盘I/O等，并根据需要进行调优。可以使用如Percona Monitoring and Management (PMM)或MySQL Enterprise Monitor等工具进行监控和调优。
应用负载均衡：如果单个MySQL服务器无法满足性能需求，可以考虑使用负载均衡技术将请求分发到多个MySQL服务器上。这可以提高系统的吞吐量和可用性。


MySQL的底层工作原理：
逻辑架构：
客户层：处理连接处理、授权认证、安全等功能。

Server层：
连接器：负责建立客户端与MySQL服务器的连接，验证用户身份并分配线程。
连接池：复用线程、管理线程以及限制最大连接数。
查询缓存（注意：在较新的MySQL版本中，查询缓存可能已被移除或默认关闭，因为它在某些场景下可能不是最优选择）。
解析器：将SQL语句进行解析，并生成解析树。
预处理器：进一步检查解析树是否合法。
优化器：根据解析树生成执行计划，并选择最优的执行策略。
执行器：根据执行计划执行SQL语句，并返回结果。
内置函数和跨存储引擎功能：如日期、时间、数学和加密函数，以及存储过程、触发器、视图等。

存储引擎层：
负责数据的存储和提取，通过API与Server层通信。
支持多种存储引擎，如InnoDB、MyISAM、Memory等。

查询处理：
逻辑查询处理：确定执行查询应该产生什么样的结果，如JOIN、WHERE、GROUP BY等操作的顺序。
物理查询处理：代表MySQL数据库是如何得到逻辑查询处理所确定的结果的，可能涉及索引的选择、数据的读取方式等。


存储引擎：
MySQL存储引擎是一种数据库存储数据的机制、索引的技巧以及锁定水平，即存储的方式和存储的格式。它决定了MySQL数据库如何存储数据、如何为存储的数据建立索引以及如何锁定数据等。MySQL支持多种存储引擎，每种引擎都有其特定的设计目的和适用场景。

常见MySQL存储引擎及其特性
InnoDB
特性：
支持事务处理（ACID兼容）：提供具有提交、回滚和崩溃恢复能力的事务安全，保证数据的一致性和完整性。
支持行级锁：能够高效地处理并发访问，减少锁冲突。
支持外键约束：可以维护表之间的数据一致性。
支持MVCC（多版本并发控制）：提高并发性能。
使用B+树索引：加速数据的查找和排序。
应用场景：适用于需要高并发写入操作、数据完整性要求高的应用程序，如银行系统、电子商务网站等。

MyISAM
特性：
查询速度快：特别是读操作，因为MyISAM使用表级锁，读操作可以并发进行。
不支持事务处理：表级锁也导致其在处理并发写入时性能较差。
支持全文索引：是MySQL唯一支持全文索引的存储引擎。
数据紧凑存储：可获得更小的索引和更快的全表扫描性能。
应用场景：适合读多写少的场景，如只读数据报表、静态内容存储等。

Memory
特性：
数据存储在RAM中：访问速度非常快，但重启服务后数据会丢失。
使用哈希索引：快速定位和检索数据，但不适合范围查询或排序查询。
支持大多数基本的MySQL数据类型，但不支持TEXT、BLOB等大型对象数据类型。
应用场景：适用于需要快速读写操作而对数据持久化要求不高的场景，如缓存、会话管理和临时数据处理。

CSV
特性：
数据文件以.csv格式保存：与Excel表格兼容，保存的文件是一个普通的文本文件。
不支持索引：查询性能较低。
应用场景：适用于需要导出数据到Excel或进行简单数据交换的场景。

Archive
特性：
高度压缩：适合存储大量历史数据。
仅支持INSERT和SELECT操作：不支持UPDATE和DELETE操作。
应用场景：日志存储、审计记录等不需要更新的历史数据。

存储引擎的应用
在选择MySQL存储引擎时，需要根据具体的应用场景和需求来决定。

例如：
如果应用需要高并发写入操作和数据完整性保证，可以选择InnoDB存储引擎。
如果应用主要是读操作且对事务要求不高，可以选择MyISAM存储引擎。
如果需要快速读写操作且对数据持久化要求不高，可以选择Memory存储引擎。
如果需要导出数据到Excel或进行简单数据交换，可以选择CSV存储引擎。
如果需要存储大量历史数据且不需要频繁更新，可以选择Archive存储引擎。

页：
关于MySQL的数据页（Page）结构，特别是InnoDB存储引擎下的，它是InnoDB存储数据的基本单位。一个InnoDB页通常大小为16KB（尽管这个大小可以配置，但16KB是最常见的设置）。InnoDB页用于存储表中的数据、索引以及其他系统信息。

InnoDB的数据页主要包含以下几个部分：
File Header：文件头部分包含了页的基本信息，如页的编号、页的类型（是数据页、索引页还是其他类型的页）、前一个页和后一个页的指针（用于双向链表连接）、页被最后修改的时间戳、日志序列号（LSN）等。
Page Header：页头部分包含了页的状态信息、页中记录的数量、空闲空间的大小、最小和最大的事务ID（用于MVCC）等。
Infimum 和 Supremum Records （重要）：这两个是伪记录，分别表示页中记录的最小和最大值。它们并不存储用户数据，但用于定义记录的边界。
User Records：用户记录部分存储了实际的行数据或索引项。这些记录是变长的，并且按照主键（或索引键）的顺序存储。
Free Space：空闲空间部分是在页被初始化时预留的，用于后续插入记录时分配空间。当记录被删除或页分裂时，空闲空间会增加。
Page Directory：页目录是一个槽数组，用于快速定位页中的记录。每个槽指向一条记录的头部。由于InnoDB采用页内记录的紧凑存储和页分裂策略，页目录可以帮助快速定位记录，而无需遍历整个页。
File Trailer：文件尾部是一个校验和（checksum），用于验证页的完整性。它存储在页的末尾，不占用File Header中的空间。

InnoDB存储引擎的数据页结构支持了高效的数据存储、检索和更新操作。以下是几个主要应用场景：
数据插入：当新记录被插入时，InnoDB会首先检查Free Space是否有足够的空间。如果有，记录会被添加到适当的位置，并更新Page Directory和Page Header中的相关信息。如果没有足够的空间，页可能会进行分裂。
数据检索：根据主键或索引键查找记录时，InnoDB会使用B+树索引结构来定位到正确的页，然后利用Page Directory快速定位到具体的记录。
数据更新和删除：记录更新时，如果更新后的记录大小没有变化或变化很小，记录可能会被原地修改（in-place update）。如果记录变大或需要移动到不同的页，则会发生记录的移动或页的分裂。记录删除时，相应的空间会被标记为空闲空间，等待后续使用。
事务和并发控制：InnoDB通过MVCC（多版本并发控制）机制支持高并发的事务处理。每个页头中保存的最小和最大事务ID用于确定哪些事务可以看到该页中的记录。
MySQL的InnoDB存储引擎的数据页结构是其高性能和可靠性的基石之一。通过精心设计的页结构，InnoDB能够高效地处理大量数据的存储和访问需求。


在innodb引擎的数据页中，是如何保证数据的有序性的？

在InnoDB引擎的数据页中，数据的有序性主要通过以下几个机制来保证：
1. 主键索引与聚簇索引
聚簇索引：InnoDB使用聚簇索引来存储数据，即将表中的数据按主键的顺序存储。这意味着数据本身就是索引的一部分，索引的顺序与数据的物理存储顺序相同。如果表定义了主键，那么主键索引就是聚簇索引；如果没有显式定义主键，InnoDB会选择第一个唯一索引作为聚簇索引；如果都没有，InnoDB会隐式地生成一个唯一的ROWID作为聚簇索引。
数据排序：由于数据是按照主键索引的顺序存储的，因此数据在物理存储上自然就是有序的。

2. 页内数据的组织
数据页结构：InnoDB的数据页是存储和读取数据的基本单位，每个页的大小默认为16KB。数据页内部会存储一行一行的数据，并且每一行数据都会按照主键大小进行排序存储。
记录堆：数据页内的记录存储区称为记录堆，分为有效记录和已删除记录两种。有效记录按照主键顺序排列，而已删除的记录会组成一个垃圾链表，等待后续被覆盖或回收。
页分裂：当数据页中的空闲空间不足以插入新记录时，会发生页分裂。新记录会被插入到新的数据页中，并且保证新页中的记录也是有序的。页分裂过程会确保索引的连续性和有序性。

3. 索引页与B+树结构
索引页：索引页是存储索引信息的数据页，它们组成了一个B+树结构。B+树的每个节点都是一个索引页，非叶子节点存储的是索引键和指向子节点的指针，而叶子节点存储的是实际的记录或主键值。
有序性保证：B+树通过维护索引键的有序性来保证数据的有序性。在B+树中，每个节点的索引键都是按照顺序排列的，这使得在查找、插入和删除操作时能够保持数据的有序性。

4. 查找与定位
二分查找：在数据页和索引页中，都使用了二分查找来提高查找效率。通过二分查找，可以快速定位到所需的数据或索引页。
数据页目录：在数据页内部，还使用了一个数据页目录来存储主键ID和行位置的映射关系。这使得在数据页内部查找特定记录时也能够快速定位。

综上所述，InnoDB引擎通过聚簇索引、页内数据的组织、索引页与B+树结构以及高效的查找与定位机制来保证数据的有序性。这种有序性不仅提高了查询效率，还保证了数据的完整性和一致性。


索引：
MySQL使用索引来加速数据的查找。常见的索引数据结构包括B树和B+树。
索引的构建本质上是控制树的高度，以降低查找的复杂度。

事务处理：
MySQL中只有使用了InnoDB存储引擎的数据库或表才支持事务处理。
事务用于维护数据库的完整性，确保多个操作要么全部成功，要么全部失败。
事务遵循ACID原则：原子性、一致性、隔离性和持久性。

复制：
MySQL支持数据复制功能，可以将一个数据库服务器的数据同步到另一个或多个服务器。
复制功能有助于实现数据备份、负载均衡、故障恢复等目的。

总结来说，MySQL的底层工作原理涉及逻辑架构的分层处理、查询的逻辑和物理处理、存储引擎的数据存储和提取、索引的加速查找、事务的完整性维护以及数据的复制等多个方面。
这些组件和机制共同协作，使得MySQL能够高效地处理各种数据库操作。


MYSQL体系结构：
Client Connectors
接入方。支持很多协议(JDBC、ODBC、.NET、PHP、Python、PERL、C 等)

Management Serveices & Utilities
系统管理和控制工具，mysqldump、 mysql复制集群、分区管理等

Connection Pool
连接池：管理缓冲用户连接、用户名、密码、权限校验、线程处理等需要缓存的需求

SQL Interface
SQL接口：接受用户的SQL命令，并且返回用户需要查询的结果

Parser
解析器，SQL命令传递到解析器的时候会被解析器验证和解析。解析器是由Lex和YACC实现的

Optimizer
查询优化器，SQL语句在查询之前会使用查询优化器对查询进行优化

Cache和Buffer（高速缓存区）
查询缓存，如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据

pluggable storage Engines
插件式存储引擎。存储引擎是MySql中具体的与文件打交道的子系统

File System
文件系统，数据、日志（redo，undo）、索引、错误日志、查询记录、慢查询等

MYSQL架构：
连接层：最上层是一些客户端和连接服务。主要完成一些类似于连接处理、授权认证、及相关的安全方案。
在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。
服务器也会为安全接入的每个客户端验证它所具有的操作权限。

服务层：第二层服务层，主要完成大部分的核心服务功能， 包括查询解析、分析、优化、缓存、以及所有的内置函数，所有跨存储引擎的功能也都在这一层实现，
包括触发器、存储过程、视图等

引擎层：第三层存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。
不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。

存储层：第四层为数据存储层，主要是将数据存储在运行于该设备的文件系统之上，并完成与存储引擎的交互

关于SQL语句
SQL语言包括以下几个部分：
1.数据定义语言（Data-Definition Language，DDL）：创建、删除、修改关系模式的命令
2.数据操纵语言（Data-Manipulation Language，DML）：对数据库其中的对象和数据运行访问工作的编程语句，包括查询、修改、插入、添加
3.完整性（integrity）：SQL DDL包括定义完整性约束的命令
4.视图定义（view definition）：SQL DDL包括定义视图的命令
5.事务控制（transaction control）：SQL 包括定义事务的开始点和结束点的命令
6.嵌入式SQL（embedded SQL）和动态SQL（dynamic SQL）：定义如何嵌入诸如C、C++和Java这样的编程语言中
7.授权（authorization）：SQL DCL用来定义访问权限和安全级别。

SQL的基本数据类型：
SQL的基本数据类型由13个基本数据类型组成，它们是由 Microsoft Jet 数据库引擎和几个验证过的有效同义字定义的。常见的有：

整形（Integer）。
单精度（Single）。
双精度（Double）。
可变长度字符（VarChar）。
固定长度字符（Char）。
长型（Long）。
日期（Date）。

此外，还有货币、文本、时间、位串、UI格式化类型等。

关于SQL语句在MySQL中执行过程详解
一、SQL语句执行原理：
第一步:客户端把语句发给服务器端执行
当我们在客户端执行 select 语句时,客户端会把这条 SQL 语句发送给服务器端,让服务器端的
进程来处理这语句。也就是说,Oracle 客户端是不会做任何的操作,他的主要任务就是把客户端产生
的一些 SQL 语句发送给服务器端。虽然在客户端也有一个数据库进程,但是,这个进程的作用跟服务器
上的进程作用是不相同的。服务器上的数据库进程才会对SQL语句进行相关的处理。不过,有个问题需
要说明,就是客户端的进程跟服务器的进程是一一对应的。也就是说,在客户端连接上服务器后,在客户
端与服务器端都会形成一个进程,客户端上的我们叫做客户端进程;而服务器上的我们叫做服务器进程。

第二步:语句解析
当客户端把 SQL 语句传送到服务器后,服务器进程会对该语句进行解析。同理,这个解析的工作,
也是在服务器端所进行的。虽然这只是一个解析的动作,但是,其会做很多“小动作”。

1. 查询高速缓存(library cache)。服务器进程在接到客户端传送过来的 SQL 语句时,不
会直接去数据库查询。而是会先在数据库的高速缓存中去查找,是否存在相同语句的执行计划。如果在
数据高速缓存中,则服务器进程就会直接执行这个 SQL 语句,省去后续的工作。所以,采用高速数据缓
存的话,可以提高 SQL 语句的查询效率。一方面是从内存中读取数据要比从硬盘中的数据文件中读取
数据效率要高,另一方面,也是因为这个语句解析的原因。
大多数情况下查询缓存不管用。因为①只有相同的查询操作才会命中查询缓存②如果该表使用了DML、DCL
语句，那么该表的所有高速缓存查询就会失效。③使用了每次查询都返回不一样结果的函数（例如NOW（）），
不会命中查询缓存
不过这里要注意一点,这个数据缓存跟有些客户端软件的数据缓存是两码事。有些客户端软件为了
提高查询效率,会在应用软件的客户端设置数据缓存。由于这些数据缓存的存在,可以提高客户端应用软
件的查询效率。但是,若其他人在服务器进行了相关的修改,由于应用软件数据缓存的存在,导致修改的
数据不能及时反映到客户端上。从这也可以看出,应用软件的数据缓存跟数据库服务器的高速数据缓存
不是一码事。

2. 语句合法性检查(data dict cache)。当在高速缓存中找不到对应的 SQL 语句时,则服
务器进程就会开始检查这条语句的合法性。这里主要是对 SQL 语句的语法进行检查,看看其是否合乎
语法规则。如果服务器进程认为这条 SQL 语句不符合语法规则的时候,就会把这个错误信息,反馈给客
户端。在这个语法检查的过程中,不会对 SQL 语句中所包含的表名、列名等等进行 SQL 他只是语法
上的检查。

3. 语言含义检查(data dict cache)。若 SQL 语句符合语法上的定义的话,则服务器进程
接下去会对语句中的字段、表等内容进行检查。看看这些字段、表是否在数据库中。如果表名与列名不
准确的话,则数据库会就会反馈错误信息给客户端。所以,有时候我们写 select 语句的时候,若语法
与表名或者列名同时写错的话,则系统是先提示说语法错误,等到语法完全正确后,再提示说列名或表名
错误。

4. 获得对象解析锁(control structer)。当语法、语义都正确后,系统就会对我们需要查询
的对象加锁。这主要是为了保障数据的一致性,防止我们在查询的过程中,其他用户对这个对象的结构发
生改变。

5. 数据访问权限的核对(data dict cache)。当语法、语义通过检查之后,客户端还不一定
能够取得数据。服务器进程还会检查,你所连接的用户是否有这个数据访问的权限。若你连接上服务器
的用户不具有数据访问权限的话,则客户端就不能够取得这些数据。有时候我们查询数据的时候,辛辛苦
苦地把 SQL 语句写好、编译通过,但是,最后系统返回个 “没有权限访问数据”的错误信息,让我们气
半死。这在前端应用软件开发调试的过程中,可能会碰到。所以,要注意这个问题,数据库服务器进程先
检查语法与语义,然后才会检查访问权限。

6. 确定最佳执行计划 。当语句与语法都没有问题,权限也匹配的话,服务器进程还是不会直接对
数据库文件进行查询。服务器进程会根据一定的规则,对这条语句进行优化。不过要注意,这个优化是有
限的。一般在应用软件开发的过程中,需要对数据库的 sql 语言进行优化,这个优化的作用要大大地大
于服务器进程的自我优化。所以,一般在应用软件开发的时候,数据库的优化是少不了的。当服务器进程
的优化器确定这条查询语句的最佳执行计划后,就会将这条 SQL 语句与执行计划保存到数据高速缓存
(library cache)。如此的话,等以后还有这个查询时,就会省略以上的语法、语义与权限检查的步骤,
而直接执行 SQL 语句,提高 SQL 语句处理效率。

第三步:语句执行
语句解析只是对 SQL 语句的语法进行解析,以确保服务器能够知道这条语句到底表达的是什么意
思。等到语句解析完成之后,数据库服务器进程才会真正的执行这条 SQL 语句。这个语句执行也分两
种情况。
一是若被选择行所在的数据块已经被读取到数据缓冲区的话,则服务器进程会直接把这个数据传递
给客户端,而不是从数据库文件中去查询数据。
若数据不在缓冲区中,则服务器进程将从数据库文件中查询相关数据,并把这些数据放入到数据缓冲
区中(buffer cache)。

第四步:提取数据
当语句执行完成之后,查询到的数据还是在服务器进程中,还没有被传送到客户端的用户进程。所以,
在服务器端的进程中,有一个专门负责数据提取的一段代码。他的作用就是把查询到的数据结果返回给
用户端进程,从而完成整个查询动作。从这整个查询处理过程中,我们在数据库开发或者应用软件开发过
程中,需要注意以下几点:
一是要了解数据库缓存跟应用软件缓存是两码事情。数据库缓存只有在数据库服务器端才存在,在
客户端是不存在的。只有如此,才能够保证数据库缓存中的内容跟数据库文件的内容一致。才能够根据
相关的规则,防止数据脏读、错读的发生。而应用软件所涉及的数据缓存,由于跟数据库缓存不是一码事
情,所以,应用软件的数据缓存虽然可以提高数据的查询效率,但是,却打破了数据一致性的要求,有时候
会发生脏读、错读等情况的发生。所以,有时候,在应用软件上有专门一个功能,用来在必要的时候清除
数据缓存。不过,这个数据缓存的清除,也只是清除本机上的数据缓存,或者说,只是清除这个应用程序
的数据缓存,而不会清除数据库的数据缓存。
二是绝大部分 SQL 语句都是按照这个处理过程处理的。我们 DBA 或者基于 Oracle 数据库的
开发人员了解这些语句的处理过程,对于我们进行涉及到 SQL 语句的开发与调试,是非常有帮助的。有
时候,掌握这些处理原则,可以减少我们排错的时间。特别要注意,数据库是把数据查询权限的审查放在
语法语义的后面进行检查的。所以,有时会若光用数据库的权限控制原则,可能还不能满足应用软件权限
控制的需要。此时,就需要应用软件的前台设置,实现权限管理的要求。而且,有时应用数据库的权限管
理,也有点显得繁琐,会增加服务器处理的工作量。因此,对于记录、字段等的查询权限控制,大部分程
序涉及人员喜欢在应用程序中实现,而不是在数据库上实现。

SQL语句中的函数、关键字、排序等执行顺序:
1. FROM 子句返回初始结果集。
2. WHERE 子句排除不满足搜索条件的行。
3. GROUP BY 子句将选定的行收集到 GROUP BY 子句中各个唯一值的组中。
4. 选择列表中指定的聚合函数可以计算各组的汇总值。
5. 此外,HAVING 子句排除不满足搜索条件的行。
6. 计算所有的表达式;
7. 使用 order by 对结果集进行排序。
8. 查找你要搜索的字段。

sql语句中有case when的情况时，sql的执行顺序：
1.FROM 和 JOIN
首先，确定要从哪些表中检索数据，并执行JOIN操作。

2.WHERE
过滤不满足WHERE条件的记录。

3.SELECT（部分）
评估SELECT列表中的表达式和函数，包括CASE WHEN。但请注意，此时的评估通常是基于每行的，并且尚未进行分组或聚合。
为每行生成一个或多个列的值，这些值可能包括计算字段、聚合函数的初始值（如果使用了聚合函数）等。

4.GROUP BY
将结果集按照GROUP BY子句中指定的列或表达式进行分组。
如果在SELECT列表中使用了聚合函数（如SUM、COUNT、AVG等），则在此阶段计算这些聚合值。

5.HAVING
过滤不满足HAVING条件的分组。

6.SELECT（完成）
如果在SELECT列表中有非聚合列且未包含在GROUP BY中，则可能需要进行额外的处理（这取决于具体的DBMS和SQL模式）。
返回最终的查询结果集。

7.ORDER BY
对结果集进行排序。

8.LIMIT / OFFSET（或其他类似子句）
限制返回的记录数或跳过一定数量的记录。



MYSQL视图：
MySQL视图是一种虚拟表，它是存储在数据库中的查询语句，并不实际存在于数据库中。视图的结构和数据是建立在对表的查询基础上的，视图包括几个被定义的数据列和多个数据行，这些数据列和数据行来源于其所引用的表。
视图所对应的数据并不实际地以视图结构存储在数据库中，而是存储在视图所引用的表中，通过视图看到的数据只是存放在基本表中的数据。对视图的操作与对表的操作一样，可以对其进行查询、修改（有一定的限制）、删除。 

当对视图中的数据进行修改时，相应的基本表的数据也要发生变化，同时，若基本表的数据发生变化，则这种变化也可以自动地反映到视图中。视图能够对机密数据提供安全保证，可以在设计数据库的时候，对不同的用户定义不同的视图，使机密数据不出现在不应该看到这些数据的用户所使用的视图中。

视图创建语句：
CREATE VIEW employee_view AS
SELECT employee_id, first_name, last_name, salary
FROM employees
WHERE department_id = 10;

常见的使用MySQL视图的场景：
1. 简化复杂查询
当一个查询非常复杂，包含多次连接、多层嵌套或计算时，可以将其封装在一个视图中。这样，用户只需查询视图，而不必每次都编写复杂的SQL语句。

CREATE VIEW simple_view AS
SELECT a.column1, b.column2
FROM table1 a
JOIN table2 b ON a.id = b.id
WHERE a.status = 'active';

2. 提高可读性和可维护性
通过使用视图，可以将复杂的逻辑隐藏在视图背后，使得查询代码更加简洁、易读和易于维护。

3. 数据抽象和安全
视图可以用于数据抽象，只展示给用户需要的数据，同时隐藏底层表结构。还可以用来限制用户访问特定的数据，提高数据的安全性。

CREATE VIEW limited_view AS
SELECT column1, column2
FROM table1
WHERE column3 = 'some_value';

4. 重用SQL代码
将常用的查询封装成视图，方便在多个地方重用，避免重复编写相同的SQL代码。

5. 兼容性和迁移
在数据库结构变化时，通过修改视图定义而不是修改大量的应用程序代码，可以实现数据库结构的迁移和兼容性。

6. 权限管理
通过视图，可以为不同的用户分配不同的访问权限。例如，只允许某些用户访问特定视图，而不允许他们访问底层表。

GRANT SELECT ON simple_view TO 'user'@'localhost';

7. 聚合和计算
视图可以用于存储聚合和计算结果，以便在需要时快速访问这些结果，而不必每次都执行复杂的计算。


CREATE VIEW sales_summary AS
SELECT product_id, SUM(sales_amount) AS total_sales
FROM sales
GROUP BY product_id;

8. 联合多个表的数据
视图可以将多个表的数据联合起来，提供一个统一的接口，以便于查询和分析。

CREATE VIEW combined_view AS
SELECT a.column1, b.column2, c.column3
FROM table1 a
JOIN table2 b ON a.id = b.id
JOIN table3 c ON b.id = c.id;



MySQL索引：
MySQL索引（Index）是帮助MySQL高效获取数据的数据结构。它可以被看作是一本书的目录，通过索引，数据库系统可以快速地定位到表中的某一行数据，而不需要扫描整个表。索引可以大大提高数据库查询的速度，特别是在处理大量数据时。

索引的作用：
加快数据检索速度：索引可以极大地减少数据库引擎需要扫描的数据量，从而加快查询速度。
加速表与表之间的连接：对于需要连接多个表的查询，索引可以帮助减少连接的行数，从而提高查询速度。
降低数据库的IO成本：索引可以减少磁盘I/O操作，因为通过索引可以快速定位到数据，减少了需要读取的数据量。
帮助服务器避免排序和临时表：在某些情况下，索引可以帮助数据库服务器避免对数据进行排序，也可以减少创建临时表的需要。

索引的类型：
B-Tree索引：最常见的索引类型，适用于全键值、键值范围或键值前缀查找。
哈希索引：基于哈希表的索引，它只支持等值比较查询，且查询速度非常快，但不支持排序操作。
全文索引：用于在文本字段上进行全文搜索。
空间索引：用于地理空间数据类型。

在MySQL中，可以使用CREATE INDEX语句或ALTER TABLE语句来创建索引。

CREATE INDEX idx_name ON table_name(column_name);  
  
ALTER TABLE table_name ADD INDEX idx_name(column_name);
虽然索引可以极大地提高查询性能，但索引也不是越多越好。每个索引都需要占用物理空间，并且在插入、删除和更新数据时，索引也需要被更新，这可能会降低这些操作的性能。因此，在设计数据库和索引时，需要权衡查询性能和数据修改性能的需求。


索引的维护：
随着数据量的增加和表结构的变更，索引的性能可能会下降。因此，定期检查和优化索引是数据库维护的重要工作之一。这包括重新组织索引、删除不再需要的索引以及根据查询模式调整索引策略等。


使用索引的注意事项：
1.索引列的数据长度能少则少；较短的索引列可以节省存储空间，并且可能提高索引的效率，尤其是在进行全表扫描或索引重建时。

2.索引一定不是越多越好，越全越好，一定是建合适的；过多的索引会占用额外的存储空间，并且在插入、删除和更新数据时会导致额外的开销。因此，应该根据实际的查询需求和数据特点来创建合适的索引。

3.匹配列前缀可用到索引 like 9999%，like %9999%、like %9999用不到索引；
注意： like 9999% 并不会一定用到索引（离散型太差，就不会用到索引），like %9999%、like %9999 是绝对不会用到索引的
然而，like '9999%' 确实可以使用索引，因为索引可以用于查找以'9999'开头的所有行。然而，like '%9999%' 和 like '%9999' 确实无法有效利用索引，因为通配符%在模式的最前面。
如果前缀值在表中非常普遍（即选择性低），那么即使使用了索引，性能提升也可能不明显。


4.Where 条件中 NOT IN 和 <> 操作在某些情况下仍然可以使用索引，但这取决于优化器的选择和数据分布。如果优化器认为使用索引的成本低于全表扫描，那么它会选择使用索引。

5.匹配范围值，order by 也可用到索引；当使用范围查询（如BETWEEN、<、>等）时，MySQL可以使用索引来加速查询。同样，如果ORDER BY子句中的列是索引的一部分，MySQL也可能使用索引来优化排序操作。

6.多用指定列查询，只返回自己想到的数据列，少用select *；通过只查询需要的列，可以减少数据传输量，提高查询效率。

7.联合索引中如果不是按照索引最左列开始查找，无法使用索引；但需要注意的是，在某些情况下（如索引跳跃扫描或索引下推），即使不是从最左列开始，MySQL也可能利用索引的一部分。然而，这通常不是最优的查询计划。

8.联合索引中精确匹配最左前列并范围匹配另外一列可以用到索引；
比如说：有个[name,phone]联合索引
    where name=‘abc’ and phone>‘136xxxxxxxx’ 这种是可以用到索引的

9.联合索引中，如果查询中有某个列的范围查询，则其右边的所有列都无法使用索引；
比如说：有个[age,name]联合索引
    where age>18 and name='abc'这是用不到索引的
它只会age使用索引，name不会使用索引。还是基于最左匹配原则

在大多数情况下，一旦索引列上进行了范围查询，那么该列右边的列就无法再通过索引进行过滤了，因为索引是按照顺序排列的。但是，在某些特殊情况下（如索引覆盖扫描），即使存在范围查询，索引也可能被部分使用。



索引在哪些情况下会失效：
1. 查询条件中使用函数或表达式
当在查询条件中对列使用函数或表达式时，MySQL无法直接利用索引进行快速定位，因为索引是基于列的原始值建立的。例如，使用YEAR(create_date) = 2023这样的查询条件时，由于YEAR函数的作用，索引会失效。
正确的做法是避免在查询条件中使用函数，而是通过其他方式（如范围查询）来替代。

2. 数据类型不匹配
如果查询条件中的数据类型与列的数据类型不匹配，MySQL可能会进行隐式类型转换，这会导致索引失效。例如，如果列是字符类型（VARCHAR），
而查询条件中使用了数字类型（INT），MySQL会尝试将字符类型转换为数字类型进行比较，这个过程中索引可能无法被有效利用。

3. 未遵循最左前缀原则（针对复合索引）
复合索引是基于多个列建立的索引。在查询时，如果没有从复合索引的最左列开始使用索引列，那么索引可能会失效。例如，如果有一个复合索引(first_name, last_name)，而查询条件只使用了last_name列，那么索引就不会被有效利用。

4. LIKE查询中的通配符使用不当
在使用LIKE查询时，如果通配符%出现在模式的开头，MySQL将无法进行索引优化，因为无法预测哪些行可能包含匹配的值。例如，LIKE '%AA'这样的查询会导致索引失效。如果通配符只出现在末尾或中间，那么索引仍然可以被有效利用。

LIKE '%AA'索引失效的原因：
通配符在开头：
在LIKE '%AA'的查询中，通配符%位于模式的开头。这意味着查询将匹配任何以AA结尾的字符串，而不仅仅是索引列中确切的AA值。
由于通配符在开头，MySQL无法利用索引来预测哪些行可能包含匹配的值，因此它必须扫描整个索引或表来查找匹配项。这导致索引无法有效减少需要搜索的数据量，从而失去其加速效果。

索引前缀无法确定：
索引的有效性在很大程度上依赖于它能够利用索引列的前缀来过滤数据。然而，在LIKE '%AA'的情况下，由于通配符在开头，索引列的前缀是不确定的，因此索引无法被有效利用。

查询优化器的选择：
MySQL的查询优化器会分析查询并决定是否使用索引。在LIKE '%AA'的情况下，由于上述原因，优化器可能判断使用索引的成本高于全表扫描的成本，因此选择不进行索引查找。

5. 使用OR连接不同的索引列
当查询条件使用OR连接不同的索引列时，如果OR两边的列没有同时包含索引，或者MySQL评估认为使用索引的成本高于全表扫描的成本，那么索引可能会失效。为了优化这类查询，可以考虑将查询拆分成多个查询，并使用UNION来合并结果。

6. 使用负向条件
使用负向条件（如NOT IN、NOT LIKE、!=等）时，索引可能会失效。这是因为负向条件通常要求数据库检查每一行数据来确定是否满足条件，这导致了全表扫描。在某些情况下，可以通过子查询或其他查询优化技术来避免索引失效。

MySQL不等于（!= 或者<>）操作为什么会导致索引失效？
MySQL中不等于（!= 或者<>）操作会导致索引失效的原因主要有以下几点：

全表扫描的必要性：当MySQL使用不等于操作符时，它通常需要扫描整个表来查找不满足条件的记录。这是因为不等于操作不像等于操作那样具有明确的匹配值，MySQL无法直接定位到具体的索引位置。
因此，为了找到所有不满足条件的记录，MySQL需要逐行扫描整个表，这就导致了索引失效。

索引结构的限制：索引在MySQL中通常是以B+树等数据结构实现的，这些结构对于范围查询和精确匹配查询非常高效。
然而，对于不等于操作，索引结构无法有效地定位到不满足条件的记录，因为索引是按照某种顺序排列的，而不等于操作可能跨越索引中的多个位置。

优化器的选择：MySQL的查询优化器会根据查询条件和表的结构选择最合适的查询策略。当优化器认为全表扫描比使用索引更快时，它会选择不使用索引。
对于不等于操作，由于需要扫描大量的记录，优化器可能会认为全表扫描更为高效，从而选择不使用索引。

需要注意的是，虽然不等于操作通常会导致索引失效，但在某些特定情况下，如果索引列的选择性很高（即不同值的比例很高），或者查询中涉及到其他可以使用索引的条件，优化器可能会选择使用索引。
然而，这种情况相对较少见，因此在编写查询语句时，应该尽量避免使用不等于操作符，尤其是在涉及到大量数据的查询中。
此外，对于需要使用不等于操作的场景，可以考虑使用其他查询策略或优化方法，如使用NOT EXISTS、LEFT JOIN等替代不等于操作，或者使用覆盖索引来优化查询性能。

7. 索引列包含大量空值
如果索引列包含大量空值（NULL），那么这些空值在索引中不会被特别处理，这可能导致索引在某些查询中的效率降低。在设计数据库和索引时，应尽量避免索引列包含大量空值。

8. 索引列参与计算或函数操作
除了前面提到的在查询条件中使用函数会导致索引失效外，如果索引列在查询过程中参与了计算或函数操作（如+、-、*、/等运算），那么索引也可能无法被有效利用。

9. 索引统计信息过时
MySQL会维护索引的统计信息来帮助查询优化器选择最佳的查询计划。如果索引统计信息过时或不准确，那么查询优化器可能会做出错误的决策，导致索引失效。因此，定期更新索引统计信息是提高查询性能的重要手段之一。

10. 使用了不合适的索引类型
不同的索引类型（如B-Tree索引、哈希索引等）适用于不同的查询场景。如果选择了不合适的索引类型，那么索引可能无法发挥应有的作用。因此，在设计索引时需要根据实际的查询需求和数据特点来选择合适的索引类型。


MYSQL索引的数据结构:
MySQL索引的数据结构主要包括以下几种，这些结构是在存储引擎层实现的，不同的存储引擎可能会采用不同的索引结构：

1. B+Tree索引
定义：B+Tree索引是MySQL中最常见的索引类型，它是对经典的B+Tree进行了优化，增加了一个指向相邻叶子节点的链表指针，以提高区间访问的性能。
特点：
相对于二叉树，B+Tree的层级更少，搜索效率更高。
支持范围匹配及排序操作。
叶子节点包含了全部的数据信息（对于聚簇索引）或主键值（对于非聚簇索引）。
叶子节点之间通过链表相连，便于范围查询。

2. Hash索引
定义：Hash索引是通过哈希表实现的，只有精确匹配索引所有列的查询才有效。
特点：
查询效率非常高，时间复杂度为O(1)。
不支持范围查询，只能用于等值查询。
在出现Hash冲突时，性能会下降。
在MySQL中，只有Memory存储引擎支持Hash索引，而InnoDB存储引擎具有自适应Hash功能，但这不是用户创建的Hash索引。

3. R-tree空间索引
定义：R-tree索引是MyISAM存储引擎的一个特殊索引，主要用于地理空间数据类型。
特点：
适用于多维空间数据的索引。
底层通过R树实现，能够有效管理空间数据。

4. Full-text全文索引
定义：全文索引通过建立倒排索引，快速匹配文档，类似于搜索引擎的索引机制。
特点：
适用于对文本内容进行搜索。
只能在文本类型（如CHAR、VARCHAR、TEXT）字段上创建。
Memory存储引擎不支持该索引。


为什么MYSQL索引使用B+树是更合适的选择：
1. 磁盘I/O效率
数据库索引通常存储在磁盘上，而不是内存中。由于磁盘I/O操作的速度远慢于内存访问，因此减少磁盘I/O次数是提高数据库查询性能的关键。B+树相比二叉查找树，在保持数据有序的同时，通过增加每个节点的子节点数（即分支因子），大大降低了树的高度，从而减少了查询时所需的磁盘I/O次数。

2. 节点存储效率
B+树的每个节点可以存储多个关键字（或键值对）和子节点指针，这使得树更加“矮胖”。相比之下，二叉查找树的一个节点通常只存储一个关键字和两个子节点指针（对于非叶子节点）。在相同数量的数据下，B+树的高度远低于二叉查找树，从而减少了查询过程中的磁盘I/O次数。

3. 叶子节点之间的链表
B+树的一个关键特性是所有数据都存储在叶子节点中，并且叶子节点之间通过链表相连。这种结构非常适合进行范围查询，因为可以直接在叶子节点链表中遍历，而无需回溯到父节点。此外，这种结构还使得B+树在进行顺序访问时（如全表扫描或范围查询）更加高效。

4. 内部节点不存储数据
B+树的内部节点（非叶子节点）仅存储键值（用于指导搜索）和子节点指针，不存储实际的数据记录。这减少了内部节点的空间占用，使得B+树能够拥有更多的分支因子，进一步降低树的高度。相比之下，二叉查找树的每个节点都需要存储数据，限制了节点的分支因子。

5. 缓存友好性
由于B+树的结构更加紧凑，且叶子节点之间通过链表相连，这使得B+树在缓存中的表现更加出色。当进行范围查询或顺序访问时，已经加载到缓存中的叶子节点可能包含多个相邻的数据记录，这些记录可能都会被后续操作使用，从而提高了缓存的利用率。


什么是B-树？
B-树（B类树）的特点就是每层节点数目非常多，层数很少，目的就是为了减少磁盘IO次数。
B-树是一种多路平衡查找树，它的每一个节点最多包含K个孩子，k称为B树的阶。k的大小取决于磁盘页的大小。
一个m阶的B树具有如下几个特征：

1.根结点至少有两个子女。
2.每个中间节点都包含k-1个元素和k个孩子，其中 m/2 <= k <= m。
3.每一个叶子节点都包含k-1个元素，其中 m/2 <= k <= m。
4.所有的叶子结点都位于同一层。
5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。
一个B树结构的具体例子。

B-树用作索引：
在B-树中进行查询时，在查询的中的比较次数其实不比二叉查找树少，尤其当单一节点中的元素数量很多时。可是相比磁盘IO的速度，内存中的耗时几乎可以忽略，所以只要树的高度足够低，IO次数足够少，就可以提高查询性能。
相比之下节点内部元素多一点也没有关系，仅仅是多了几次内存交互，只要不超过磁盘页的大小即可。这就是B-树的优势之一。B树在插入和删除节点的时候如果导致树不平衡，就通过自动调整节点的位置来保持树的自平衡。
非关系型数据库MongoDB使用B树作为数据库索引。大部分关系型数据库，比如Mysql，则使用B+树作为索引。
B+树是基于B-树的一种变体，有着比B-树更高的查询性能。
B+树和B-树有一些共同点，但是B+树也具备一些新的特征。


什么是B+树？
B+树是B-树的一种变体，它针对数据库索引的需求进行了优化：

更高的分支因子：由于B+树的非叶子节点只存储键值（索引）而不存储实际数据，这使得每个节点能够有更多的子节点，从而在相同大小的磁盘页中存储更多的键值，进一步降低了树的高度。
更稳定的查询性能：在B+树中，所有的查询都必须到达叶子节点才能找到数据。这种特性使得查询性能更加稳定，因为不论查询的数据在何处，所需的磁盘I/O次数都是相同的（取决于树的高度）。
优化的范围查询：B+树的叶子节点通过链表相连，这极大地优化了范围查询的性能。在进行范围查询时，一旦找到范围的起始点，就可以沿着链表顺序遍历，而无需回到根节点或中间节点。
更适合磁盘存储：由于B+树的非叶子节点不存储数据，它更适合用于磁盘存储。在数据库系统中，索引通常存储在磁盘上，而B+树的结构使得磁盘I/O更加高效。

一个m阶的B+树具有如下几个特征：
1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。
2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。
3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。


B+树中的节点之间不但含有重复元素，而且叶子节点还用指针连在一起。在B+树中，每一个父节点的中的元素都出现在子节点中，是子节点的最大（或最小元素）。
由于父节点的所有元素都出现在子节点，因此所有叶子节点包含了全量元素信息。并且每个叶子节点都有指向下一个叶子节点的指针，形成了有序链表。
在B+树中，只有叶子节点才真正存储数据，非叶子节点不存储数据。使用B+树做索引的好处主要体现在查询性能上。在单元素查询的时候，B+树会自顶向下逐层查找节点，最终找到匹配的叶子节点。
对于范围查询，比如查询范围为3~11的元素，B-树只能依靠繁琐的中序遍历，首先自顶向下查找范围的下限，然后中序遍历找到上限。
B+树的范围查询则要简单的多，首先自顶向下查找范围的下限，然后只需要在叶子节点所在的链表上做遍历即可。

B+树和B-树的区别： 
B+树中间节点没有存储数据，只有叶节点存放数据，其余节点用来索引，所以同样大小的磁盘页可以容纳更多的节点元素，而B-树是每个索引节点都会有Data域。
这就意味着，数据量相同的情况下，B+树的结构比B-树更加“矮胖”，因此查询是IO次数也更少。这就决定了B+树更适合用来存储外部数据，也就是所谓的磁盘数据。
其次，B+树的查询必须最终查询到叶子节点，而B-树只要找到匹配元素即可，无论匹配元素处于中间节点还是叶子节点。
因此，B-树的查询性能并不稳定（最好情况是只查根节点，最坏情况是查到叶子节点）。而B+树每一次查找都是稳定的。

综合起来，B+树相比B-树的优势有三个：
1.单一节点存储更多的元素，使得查询的IO次数更少。
2.所有查询都要查找到叶子节点，查询性能稳定。
3.所有叶子节点形成有序链表，便于范围查询。

最后总结一下：
数据库索引为什么不用AVL（平衡二叉树）？
数据库索引是存储在磁盘上，磁盘IO操作比较耗时，为了提高查询效率就需要减少磁盘IO的次数。而磁盘IO次数和树的高度有关，所以为了减少磁盘IO就需要降低树的高度，这时查找的结构就可以把二叉树变成B类的树。

数据库索引为什么用B+树而不用B-树？
数据库索引采用B+树的主要原因是B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。
而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低）。


那么MongoDB为什么使用B-树而不是B+树？
至于MongoDB为什么使用B-树而不是B+树，可以从它的设计角度来考虑，它并不是传统的关系性数据库，而是以Json格式作为存储的nosql，目的就是高性能，高可用，易扩展。首先它摆脱了关系模型，上面所述的范围查询的优点就没那么强烈了，其次Mysql由于使用B+树，数据都在叶节点上，每次查询都需要访问到叶节点，而MongoDB使用B-树，所有节点都有Data域，只要找到指定索引就可以进行访问，无疑单次查询平均快于Mysql（但侧面来看Mysql至少平均查询耗时差不多）。
总体来说，Mysql选用B+树和MongoDB选用B-树还是以自己的需求来选择的。

数据库索引为什么不用红黑树？
首先看一下红黑树的特点：
红黑树是一种自平衡的二叉查找树，除了符合二叉查找树的基本特征外，它还具有下列的附加特征。
1.节点是红色或黑色。
2.根节点是黑色。
3.每个叶子节点都是黑色的空节点（NIL节点）。
4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)
5.从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。


这是因为这些规则限制，才保证了红黑树的自平衡。红黑树从根到叶子的最长路径不会超过最短路径的2倍。当插入或删除节点的时候，红黑树的规则可能被打破。这时候就需要做出一些调整， 来继续维持我们的规则。
那么在大规模数据存储的时候，红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况。磁盘查找存取的次数往往由树的高度所决定，所以，只要我们通过某种较好的树结构尽量减少树的高度，就可以提高查询性能。B树可以有多个子女，从几十到上千，可以降低树的高度。
归结起来，使用B+树而不是用其他结构的原因就是为了减少磁盘IO的次数，减少数的高度，而B+树就很好的做到了这一点。

那么什么情况下使用红黑树?红黑树和B树使用场合有什么不同？两者都是有序的数据结构，可用作数据容器。红黑树多用在内部排序，即全放在内存中的，微软STL的map和set的内部实现就是红黑树。
B树多用在内存里放不下，大部分数据存储在外存上时。因为B树层数少，因此可以确保每次操作，读取磁盘的次数尽可能的少。
在数据较小，可以完全放到内存中时，红黑树的时间复杂度比B树低。反之，数据量较大，外存中占主要部分时，B树因其读磁盘次数少，而具有更快的速度。



MYSQL事务机制：
MySQL事务机制是数据库管理系统（DBMS）提供的一组功能，用于确保一组SQL操作能够以原子方式执行，确保数据一致性、隔离性和持久性。

事务机制通过四个重要特性（ACID特性）来实现这些目标：
1.原子性 。原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，
如果操作失败则不能对数据库有任何影响。
2.一致性 。一致性是指一个事务中，事务前后数据的完整性必须保持一致。
3.隔离性 。隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，
多个并发事务之间要相互隔离。
4.持久性 。持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。

事务并发带来的问题：
1.脏读 ：一个事务对数据进行了增删改，但未提交，另一事务可以读取到未提交的数据。如果第一个事务这时候回滚了，那么第二个事务就读到了脏数据。
2.不可重复读：一个事务中发生了两次读操作，第一次读操作和第二次读操作之间，另外一个事务对数据进行了修改，这就会导致两次读取的数据是不一致的。
3.幻读：第一个事务对一定范围的数据进行批量查询，第二个事务在这个范围增加一条数据，这时候第一个事务就会丢失对新增数据的内容。

事务的基本操作：
BEGIN / START TRANSACTION：开始一个事务。
COMMIT：提交事务，保存所有更改。
ROLLBACK：回滚事务，撤销所有更改。

事务的隔离级别：
MySQL提供了四种隔离级别，每种隔离级别都对事务的可见性和并发行为有不同的控制：

1.读未提交 (Read Uncommitted)：一个事务可以读取其他未提交事务的更改，可能导致“脏读”。
2.读已提交 (Read Committed)：一个事务只能读取其他已提交事务的更改，防止“脏读”，但可能导致“不可重复读”。
3.可重复读 (Repeatable Read)：一个事务在开始时读取的数据在整个事务过程中保持一致，防止“脏读”和“不可重复读”，但可能导致“幻读”。InnoDB存储引擎在该隔离级别下通过间隙锁定避免了幻读。
4.可序列化 (Serializable)：最高的隔离级别，通过强制事务顺序执行，完全防止“脏读”、“不可重复读”和“幻读”，但并发性最低。


MySQL锁：
锁的出现，就是用于解决不同事务对共享资源并发访问所引起的脏读、不可重复读、幻读问题。
MySQL InnoDB 锁类型，分别是：共享锁(Shard Lock)、排它锁(Exclusive Lock)、意向共享锁(Intention Shared Lock)、意向排它锁(Intention Exclusive Lock)、自增锁(AUTO-INC Lock)、临键锁(Next-key Lock)、间隙锁(Gap Lock)、记录锁(Record Lock) 八种。
(记录锁、间隙锁、临键锁这三种锁，都是行锁的具体实现算法。我们可以将其理解为锁的类型，也可以理解为行锁的具体实现算法)

MySQL行级锁：行级锁，每次操作锁住对应的行数据。锁定粒度最小，发生锁冲突的概率最低，并发度最高。
应用在InnoDB存储引擎中,MyISAM不支持行级锁。

MySQL表级锁：表级锁，每次操作锁住整张表。锁定粒度大，发生锁冲突的概率最高，并发度最低。
应用在MyISAM、InnoDB、BDB等存储引擎中。
对于表级锁，主要分为以下三类：
表锁：通过LOCK TABLES语句手动锁定表，可以是读锁或写锁。
    读锁（READ LOCK）：其他事务可以读取表，但不能写入。
    LOCK TABLES my_table READ;
    -- 执行读操作
    UNLOCK TABLES;

    写锁（WRITE LOCK）：其他事务既不能读取也不能写入。
    LOCK TABLES my_table WRITE;
    -- 执行写操作
    UNLOCK TABLES;

元数据锁（Meta Data Lock, MDL）：自动添加，用于保护表的结构和元数据的并发访问。MDL在执行诸如ALTER TABLE等操作时自动加上，不需要显式使用。
意向锁（Intention Lock）
意向锁用于表示事务计划在表中某些行上获取共享或排他锁。意向锁不会阻塞其他锁请求，但有助于提高锁管理的效率。
意向共享锁（IS 锁）：事务计划在某些行上获取共享锁。
意向排他锁（IX 锁）：事务计划在某些行上获取排他锁。

共享锁（S锁）：
添加共享锁之后，如果查询未命中索引，则不会使用行锁，会退变为表锁。
所以在没有创建name索引时，修改徐勇数据时，会被阻塞；
>当创建name索引后，由于查询命中索引，所以行锁将曹茜这条数据锁住，并不会影响表中其他数据的操作。此时修改徐勇这条数据能够成功。
>由于行锁、表锁的原因，其他事务会处于等待状态。锁超时等待，默认为 50 s。
我们可通过命令：show variables like 'innodb_lock_wait_timeout';
锁超时时间也分会话/全局级别，我们可通过set session innodb_lock_wait_timeout = 20; 
或 set global innodb_lock_wait_timeout = 20; 来进行修改设置。

允许事务读取一行数据，但不允许修改。
SELECT * FROM my_table WHERE id = 1 LOCK IN SHARE MODE;

排他锁（X锁）：
添加排他锁之后，如果 name 字段查询未命中索引，则不会使用行锁，会退变为表锁。
所以在没有创建name索引时，修改徐勇数据时，会被阻塞；
>当创建name索引后，由于 name 字段查询命中索引，所以行锁会将曹茜这条数据锁住（其他会话无法获取该条数据的共享锁、排它锁），
并不会影响表中其他数据的操作。此时修改徐勇这条数据能够成功。
>此时查询数据是可以正常进行的。是因为 InnoDB 还有一个"一致性的非锁定读"的概念，
就是说行锁未释放之前，其他事务读取该行数据读取的是它的快照数据，并不会与行锁冲突
>由于行锁、表锁的原因，其他事务会处于等待状态。锁超时等待，默认为 50 s。

允许事务读取和修改一行数据，其他事务不能读取和修改该行。
SELECT * FROM my_table WHERE id = 1 FOR UPDATE;

意向锁：
1 意向共享锁（IS锁）
表锁。表示事务准备给数据行加入共享锁时，即一个数据行加共享锁前必须先取得该表的 IS 锁， 意向共享锁之间是可以相互兼容的

2 意向排他锁（IX锁）
表锁。表示事务准备给数据行加入排他锁时，即一个数据行加排他锁前必须先取得该表的 IX 锁， 意向排它锁之间是可以相互兼容的

意向锁(IS锁、IX锁)是 InnoDB 数据操作之前自动加的，不需要用户干预。所以此处不做介绍，有个概念即可。当事务想去进行锁表时，可以先判断意向锁是否存在，存在时则可快速返回该表不能启用表锁。

自增锁：
当我们插入数据时，自增锁会 +1，但是此时如果事务执行了 rollback 等其他操作，导致数据并没有插入成功，
此时自增锁 id 并不会随之回退，会永久丢失，从而导致的自增 id 列不连续。

临键锁：
临键锁（Next-key Lock），作为 InnoDB 引擎默认行锁算法。就是解决 幻读 问题。
当 SQL 执行按照索引进行数据查询时，查询条件为范围查找(between and 、< 、> 等)并有数据命中时，
此时 SQL 语句加上的锁为 Next-key Lock，锁住的是索引的记录+下一个区间（左开右闭）。

临键锁，为什么要锁住下一个区间？
这和索引底层 B+Tree 有关系。MySQL 索引底层选择 B+Tree。B+Tree 数据都保存在叶子节点，并且具有顺序性，
从左到右依次增大。临键锁锁住相邻区间，此时 insert 插入数据时，我们并不能够将数据成功的插入到该区间，
就能够满足每次查询的数据一致，从而解决幻读问题。

示例解释：还是select * from orders where id > 5 and id < 9 for update;这个查询，
事务A 第一次查询出来一条 id = 7 的数据。如果不锁住相邻区间，此时事务B 插入了一条 id = 8 的数据。
那么事务A 在接下来就会查询出 2 条数据。如果锁住相邻区间，那么只要在事务 A 没结束期间，
查询到的都会只有 id = 7 这1条数据。这就解决了幻读问题。

间隙锁：
间隙锁（Gap Lock）。当 SQL 执行按照索引进行数据查询时，查询条件的数据不存在，
此时 SQL 语句加上的锁即为 Gap Lock，锁住的是数据不存在的区间（左开右开）。

因为只有在 RR 事务隔离级别下，InnoDB 引擎才能够解决数据幻读的问题。
临键锁(Next-key Lock) = 间隙锁(Gap Lock) + 记录锁(Record Lock)，所以 Gap 锁只有在 RR 级别下存在。

记录锁：
记录锁（Record Lock）。当 SQL 执行按照唯一性（Primary key、Unique key）索引进行数据查询时，查询条件等值匹配且查询的数据存在，
此时 SQL 语句加上的锁即为 Record Lock，锁住的是具体的索引项。

MySQL MVCC：
MVCC 多版本并发控制
MySQL的大多数事务型存储引擎实现都不是简单的行级锁。基于提升并发性考虑，一般都同时实现了多版本并发控制（MVCC），包括Oracle、PostgreSQL。只是实现机制各不相同。
可以认为 MVCC 是行级锁的一个变种，但它在很多情况下避免了加锁操作，因此开销更低。
虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只是锁定必要的行。
MVCC 的实现是通过保存数据在某个时间点的快照来实现的。也就是说不管需要执行多长时间，每个事物看到的数据都是一致的。
典型的MVCC实现方式，分为乐观（optimistic）并发控制和悲观（pressimistic）并发控制。
下边通过 InnoDB的简化版行为来说明 MVCC 是如何工作的。
InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现。这两个列，一个保存了行的创建时间，一个保存行的过期时间（删除时间）。
当然存储的并不是真实的时间，而是系统版本号（system version number）。
每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。

REPEATABLE READ（可重读）隔离级别下MVCC如何工作：

SELECT：
InnoDB会根据以下两个条件检查每行记录：
①InnoDB只查找版本早于当前事务版本的数据行，这样可以确保事务读取的行，要么是在开始事务之前已经存在要么是事务自身插入或者修改过的
②行的删除版本号要么未定义，要么大于当前事务版本号，这样可以确保事务读取到的行在事务开始之前未被删除

只有符合上述两个条件的才会被查询出来

INSERT：InnoDB为新插入的每一行保存当前系统版本号作为行版本号
DELETE：InnoDB为删除的每一行保存当前系统版本号作为行删除标识
UPDATE：InnoDB为插入的一行新纪录保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为删除标识

保存这两个额外系统版本号，使大多数操作都不用加锁。使数据操作简单，性能很好，并且也能保证只会读取到符合要求的行。
不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作和一些额外的维护工作。
MVCC 只在 COMMITTED READ（读提交）和REPEATABLE READ（可重复读）两种隔离级别下工作。


MYSQL存储引擎：
MySQL 提供了多个不同的存储引擎，包括处理事务安全表的引擎和处理非事务安全表的引擎。在 MySQL 中，不需要在整个服务器中使用同一种存储引擎，针对具体的要求，可以对每一个表使用不同的存储引擎。
MySQL 5.7 支持的存储引擎有 InnoDB、MyISAM、Memory、Merge、Archive、CSV、BLACKHOLE 等。可以使用SHOW ENGINES;语句查看系统所支持的引擎类型

InnoDB：
在 MySQL 5.5 及以后版本后，MySQL 选择使用 InnoDB为默认存储引擎。
在创建数据库表时，不指定存储引擎时，使用的就是 InnoDB。如需使用其他存储引擎，可以手动来指定。
 
特点：
InnoDB 支持事务操作；（每一条SQL都默认封装成事务，自动提交，会影响速度）
InnoDB 支持外键；
InnoDB 是聚集索引（聚簇索引）；
InnoDB 不保存表的总条数；
InnoDB 5.7版本之前不支持全文检索；
InnoDB 支持表级锁、行级锁，默认为行级锁；
InnoDB 表必须有主键（如果我们没有明确去指定创建主键索引。它会帮我们隐藏的生成一个 6 byte 的 int 型的索引作为主键索引）；
InnoDB 文件存储方式为.frm文件存储表结构，ibd文件存储数据内容。

名词解释：
什么是聚集(簇)索引？
聚簇索引的特点是叶子节点包含了完整的记录行，而非聚簇索引的叶子节点只有所以字段和主键ID。
MySQL 索引采用 B+Tree（不熟悉？来跳转链接了解：MySQL 索引底层为什么选择B+Tree）。
InnoDB 和 MyISAM 作为 MySQL 中 B+Tree 索引的两种重要体现形式。 
InnoDB 推荐以主键作为索引来组织数据进行存储，它认为主键是一个非常重要的属性。
InnoDB 表数据文件本身就是按B+Tree组织的一个索引结构，聚簇索引就是按照每张表的主键来构造一个B+树，
叶子节点中存放的就是整张表的数据。
 
一般建表会用一个自增主键做聚簇索引，没有的话MySQL会默认创建，
但是这个主键如果更改代价较高，故建表时要考虑自增ID不能频繁 update 这一点。
MySQL一张表，比如有id(主键)，name，age等字段。我们可以建很多个索引，MySQL除了主键索引外，都是非聚簇索引。
即只有我们创建的主键id索引，我们可以叫他id索引，它别名又叫做聚簇索引，
（因为只有id索引，叶子节点包含了完整的记录行）理解成一个别名的即可。
如果我们再创建一个name索引，它就叫做非聚簇索引，或者辅助索引。）
 
我们日常工作中，根据实际情况自行添加的索引都是辅助索引，辅助索引就是一个为了需找主键索引的二级索引，
现在找到主键索引再通过主键索引找数据；

MyISAM存储引擎：
MyISAM 作为 MySQL 中 B+Tree 索引的另一种重要体现形式。
 
特点：
MyISAM 是非聚集索引；
MyISAM 有一个变量专门来保存整个表的行数，查询count很快(注意不能加任何 where 条件)
MyISAM 支持全文索引；
MyISAM 可以被压缩后进行查询操作，节省空间容量；
MyISAM 支持表级锁，不支持行级锁；
MyISAM 中主键不是必须的；
MyISAM 文件存储方式为.frm文件存储表结构，.MYD文件存储数据内容，.MYI文件存储索引文件。

MySQL主从复制（数据库的主数据库复制到从数据库）
读写分离：主（master）实现写请求，从（slave）实现读请求，从而减轻数据库的压力。
redis可以实现读写分离，elasticsearch没有主从同步，kafka用的是主读主写

MySQL分库分表：
分库分表的方式有四种，它们分别是：垂直分表、垂直分库、水平分库和水平分表。

垂直分表：可以把一个宽表的字段按照访问频率、是否是大字段的原则拆分为多个表，这样既能使业务清晰，还能提高部分性能。
拆分后，尽量从业务角度避免联查，否则性能方面将得不偿失。

垂直分库：可以把多个表按照业务的耦合性来进行分类，分别存放在不同的数据库中，
这些库可以分布在不同的服务器，从而使访问压力被分摊在多个服务器，大大提高性能，同时能提高整体架构的业务清晰度，
不同的业务库可根据自身情况定制优化方案。但是它需要解决跨库带来的所有复杂问题。

水平分库：可以把一个表的数据(按数据行)分到多个不同的库，每个库只有这个表的部分数据，
这些库可以分布在不同的服务器，从而使访问压力被多服务器负载，提升性能。
它不仅需要解决跨库带来的问题，还需要解决数据路由的问题。

水平分表：可以把一个表的数据(按数据行)分到多个同一个数据库的多张表中，每个表的数据只有这个表的部分数据，
这样做能小幅提升性能，它仅仅作为水平分库的一个补充优化。

最后，一般来说，在系统设计阶段就应该根据业务耦合程度来确定用哪种分库分表的方式(方案)，
在数据量及访问压力不是特别大的情况，首先考虑缓存、读写分离、索引技术等方案。
若数据量极大，且连续增长，再考虑水平分库水平分表的方案。


Oracle:
SQL语句在Oracle中执行过程详解
Oracle采用了共享池来判断SQL语句是否存在缓存和执行计划。
1.语法检查
2.语义检查
3.权限检查
4.共享池检查：最主要的作用是缓存SQL语句和该语句的执行计划→ 软解析
5.优化器→硬解析


数据库连接技术：
JDBC：传统连接数据库的方法，容易产生硬编码问题，难以保证SQL注入安全性
//步骤一:定义连接数据库的相关信息
static String driver = "com.mysql.jdbc.Driver";
连接数据库
    static String url = "jdbc:mysql://localhost:3306/school?useSSL=false";
    static String username = "root";
    static String password = "root";

public static void main(String[] args) {
//步骤二:加载jdbc驱动
 Class.forName(driver);

//步骤三:通过驱动管理器获得和数据的连接
Connection conn = DriverManager.getConnection(url,username,password);
System.out.println("数据库的连接:"+conn);

//步骤四:定义查询的sql语句

//步骤五:创建一个PreparedStatement对象(可以用来执行sql,来操作数据库)
PreparedStatement pstmt = conn.prepareStatement(sql);

//步骤六:执行sql,得到结果集
ResultSet rs = pstmt.executeQuery();

//步骤七:关闭资源
}

数据库连接池Druid：
1.数据库连接池是个容器，负责分配、管理数据库连接(Connection)；
2.它允许应用程序重复使用一个现有的数据库连接，而不是再重新建立一个；
3.释放空闲时间超过最大空闲时间的数据库连接来避免因为没有释放数据库连接而引起的数据库连接遗漏；
public class Druid_ {
    @Test
    public void testDruid() throws Exception {
        //1.加入Druid jar包
        //2.加入配置文件，将该文件拷贝到项目的src目录下
        //3.创建Properties对象，读取配置文件
        Properties properties = new Properties();
        properties.load(new FileInputStream("src\\druid.properties"));

        //4.创建一个指定参数的数据库连接池
        DataSource dataSource = DruidDataSourceFactory.createDataSource(properties);
        long start =System.currentTimeMillis();
        for (int i = 0; i < 5000; i++) {
            Connection connection = dataSource.getConnection();
            connection.close();
        }
        long end =System.currentTimeMillis();
        System.out.println("Druid 5000次连接mysql耗时：" + (end - start));//Druid 5000次连接mysql耗时：3608
    }
}

Mybatis：
MyBatis是一个优秀的持久层框架，它对jdbc的操作数据库的过程进行封装，使开发者只需要关注 SQL 本身，而不需要花费精力去处理例如注册驱动、创建connection、创建statement、手动设置参数、结果集检索等jdbc繁杂的过程代码。
Mybatis通过xml或注解的方式将要执行的各种statement（statement、preparedStatemnt、CallableStatement）配置起来，并通过java对象和statement中的sql进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射成java对象并返回。
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper PUBLIC "-//mybatis.org/DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.mybitis.mapper.UserMapper">
    <resultMap id="result" type="com.mybitis.entity.User">
        <result column="userid" jdbcType="INTEGER" property="userid" />
        <result column="username" jdbcType="VARCHAR" property="username" />
        <result column="password" jdbcType="VARCHAR" property="password" />
    </resultMap>

    <select id="findAllUser" resultType="com.mybitis.entity.User">
        select  * from user;
    </select>

    <select id="findUserByUserId" resultType="com.mybitis.entity.User">
        select * from user where userid=#{userid};
    </select>
    <select id="findUserByUsername" resultType="com.mybitis.entity.User">
        select * from user where username=#{username};
    </select>

    <insert id="insertUser" parameterType="com.mybitis.entity.User" keyProperty="userid" useGeneratedKeys="true">
        insert into user(userid,username,password) values (#{userid},#{username},#{password});
    </insert>

    <update id="updateUser" parameterType="com.mybitis.entity.User">
        update user set username=#{username},password=#{password} where userid=#{userid};
    </update>

    <delete id="deleteUser" parameterType="com.mybitis.entity.User">
        delete from user where userid=#{userid};
    </delete>
</mapper>

MyBatis获取参数值的两种方式：${}和#{}
${}的本质就是字符串拼接，#{}的本质就是占位符赋值
${}使用字符串拼接的方式拼接sql，若为字符串类型或日期类型的字段进行赋值时，需要手动加单引号；
但是#{}使用占位符赋值的方式拼接sql，此时为字符串类型或日期类型的字段进行赋值时，可以自动添加单引号使用#{}可以防止SQL注入

1、单个字面量类型的参数
若mapper接口中的方法参数为单个的字面量类型
此时可以使用${}和#{}以任意的名称获取参数的值，注意${}需要手动加单引号

2、多个字面量类型的参数
若mapper接口中的方法参数为多个时
此时MyBatis会自动将这些参数放在一个map集合中，以arg0,arg1...为键，以参数为值；
以param1,param2...为键，以参数为值；因此只需要通过${}和#{}访问map集合的键就可以获取相对应的值，注意${}需要手动加单引号

3、map集合类型的参数
若mapper接口中的方法需要的参数为多个时，此时可以手动创建map集合，将这些数据放在map中
只需要通过${}和#{}访问map集合的键就可以获取相对应的值，注意${}需要手动加单引号

4、实体类类型的参数
若mapper接口中的方法参数为实体类对象时
此时可以使用${}和#{}，通过访问实体类对象中的属性名获取属性值，注意${}需要手动加单引号

5、使用@Param标识参数
可以通过@Param注解标识mapper接口中的方法参数
此时，会将这些参数放在map集合中，以@Param注解的value属性值为键，以参数为值；
以param1,param2...为键，以参数为值；只需要通过${}和#{}访问map集合的键就可以获取相对应的值，注意${}需要手动加单引号


MyBatis的缓存：
1、MyBatis的一级缓存
一级缓存是SqlSession级别的，通过同一个SqlSession查询的数据会被缓存，下次查询相同的数据，就会从缓存中直接获取，不会从数据库重新访问

使一级缓存失效的四种情况：
1) 不同的SqlSession对应不同的一级缓存
2) 同一个SqlSession但是查询条件不同
3) 同一个SqlSession两次查询期间执行了任何一次增删改操作
4) 同一个SqlSession两次查询期间手动清空了缓存

2、MyBatis的二级缓存
二级缓存是SqlSessionFactory级别，通过同一个SqlSessionFactory创建的SqlSession查询的结果会被缓存；此后若再次执行相同的查询语句，结果就会从缓存中获取。

二级缓存开启的条件：
a>在核心配置文件中，设置全局配置属性cacheEnabled="true"，默认为true，不需要设置
<settings>
    <setting name="cacheEnabled" value="true"/>
</settings>
b>在映射文件中设置标签<cache />
<mapper namespace="com.example.MyMapper">
    <cache/>
    <!-- 其他映射配置 -->
</mapper>
c>二级缓存必须在SqlSession关闭或提交之后有效
d>查询的数据所转换的实体类类型必须实现序列化的接口
使二级缓存失效的情况：
两次查询之间执行了任意的增删改，会使一级和二级缓存同时失效

Mybatis的源码解析：

SqlSessionFactory
作用：SqlSessionFactory 是 MyBatis 的核心接口，用于创建 SqlSession 对象。SqlSession 对象完全包含了执行 SQL 命令所需的所有方法。
实现：通常通过 SqlSessionFactoryBuilder 来构建 SqlSessionFactory 实例。

SqlSession
作用：SqlSession 是 MyBatis 的核心接口，用于执行持久化操作。
方法：提供了执行 SQL 命令、获取映射器实例和管理事务的方法。

Mapper
作用：Mapper 接口是 MyBatis 中的一个重要概念，它是开发者定义的接口，MyBatis 会自动生成这个接口的实现，并通过这个实现来执行 SQL 语句。
实现：Mapper 接口的实现是通过动态代理来完成的，MyBatis 会在运行时动态地创建 Mapper 接口的代理对象。


获得Mapper动态代理之后，执行SQL的顺序：
1.获得Mapper动态代理
2.获得MapperMethod对象
3.根据SQL指令跳转执行语句
4.查询前的缓存处理
5.执行数据库查询操作
6.针对ResultSet结果集转换为实体类

MyBatis 的工作流程大致可以分为以下几个步骤：
1.读取配置文件：MyBatis 在启动时会读取全局配置文件和映射器文件，并将配置信息解析成 Java 对象存储在内存中。
2.创建 SqlSessionFactory：通过 SqlSessionFactoryBuilder 根据配置信息创建 SqlSessionFactory 实例。
3.创建 SqlSession：通过 SqlSessionFactory 创建 SqlSession 实例，SqlSession 提供了执行 SQL 命令的所有方法。
4.执行 SQL 命令：
使用 SqlSession 的方法执行 SQL 命令。
如果执行的是 Mapper 接口中的方法，MyBatis 会通过动态代理机制调用对应的 Mapper SQL 语句。
5.处理结果：将 SQL 执行的结果映射成 Java 对象并返回。


Mybatis的逆向工程：
MyBatis Generator是一个用于生成MyBatis的Java数据访问层代码和SQL映射文件的逆向工程工具。你可以通过配置MyBatis Generator来自动生成数据库表的实体类、Mapper接口以及SQL映射文件。

逆向工程的配置文件：
<!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN"
    "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd">

<generatorConfiguration>
    <classPathEntry location="path/to/your/database/driver.jar" />

    <context id="MyBatisGenerator" targetRuntime="MyBatis3">
        <property name="javaFileEncoding" value="UTF-8" />
        <property name="beginningDelimiter" value=" " />
        <property name="endingDelimiter" value=" " />

        <!-- 数据库连接配置 -->
        <jdbcConnection driverClass="com.mysql.cj.jdbc.Driver"
            connectionURL="jdbc:mysql://localhost:3306/your_database"
            userId="your_username"
            password="your_password">
        </jdbcConnection>

        <!-- 生成实体类的目标包 -->
        <javaModelGenerator targetPackage="com.example.model" targetProject="src/main/java">
            <property name="enableSubPackages" value="true" />
            <property name="trimStrings" value="true" />
        </javaModelGenerator>

        <!-- 生成Mapper接口的目标包 -->
        <sqlMapGenerator targetPackage="com.example.mapper" targetProject="src/main/resources">
            <property name="enableSubPackages" value="true" />
        </sqlMapGenerator>

        <!-- 生成Mapper XML 文件的目标包 -->
        <javaClientGenerator type="XMLMAPPER" targetPackage="com.example.mapper" targetProject="src/main/resources">
            <property name="enableSubPackages" value="true" />
        </javaClientGenerator>

        <!-- 配置要生成的表和生成规则 -->
        <table tableName="your_table_name" domainObjectName="YourTable" enableCountByExample="false" enableUpdateByExample="false"
            enableDeleteByExample="false" enableSelectByExample="false" selectByExampleQueryId="false" />
    </context>
</generatorConfiguration>

MybatisPlus：
MyBatis-Plus 是一个 MyBatis 的增强工具，旨在简化 MyBatis 的开发，提供常见的 CRUD 操作，并且支持丰富的插件扩展能力。以下是 MyBatis-Plus 的详细介绍：

MyBatis-Plus 的特点
无侵入：MyBatis-Plus 在 MyBatis 的基础上进行增强，不会影响已有的 MyBatis 配置和使用方式。
CRUD 操作封装：提供了丰富的 CRUD 操作封装，减少了编写 SQL 的时间和工作量。
插件支持：支持多种插件，包括分页插件、性能分析插件、SQL 注入器等，扩展性强。
代码生成器：提供代码生成器，可以快速生成实体类、Mapper 接口、Service 接口及实现类、Controller 等。
支持 Lambda 表达式：提供 Lambda 风格的条件构造器，使得查询条件的构建更加优雅。
多种数据库支持：支持 MySQL、Oracle、PostgreSQL、SQL Server、SQLite 等多种数据库。
自动填充：支持字段的自动填充功能，例如创建时间、更新时间等字段。

核心功能
1. 简化的 CRUD 操作
MyBatis-Plus 提供了一系列简化的 CRUD 操作方法，通过继承 BaseMapper 接口，可以快速实现基本的 CRUD 操作。


public interface UserMapper extends BaseMapper<User> {
}
通过继承 BaseMapper，可以直接使用以下方法：

insert：插入数据
deleteById：根据 ID 删除数据
updateById：根据 ID 更新数据
selectById：根据 ID 查询数据
selectList：查询所有数据

2. 条件构造器
MyBatis-Plus 提供了 QueryWrapper 和 UpdateWrapper 用于构建查询和更新条件。


// QueryWrapper 示例
QueryWrapper<User> queryWrapper = new QueryWrapper<>();
queryWrapper.eq("name", "John");
List<User> users = userMapper.selectList(queryWrapper);

// LambdaQueryWrapper 示例
LambdaQueryWrapper<User> lambdaQueryWrapper = new LambdaQueryWrapper<>();
lambdaQueryWrapper.eq(User::getName, "John");
List<User> users = userMapper.selectList(lambdaQueryWrapper);

3. 分页插件
MyBatis-Plus 提供了分页插件，方便进行分页查询。


IPage<User> page = new Page<>(1, 10);
IPage<User> userPage = userMapper.selectPage(page, null);

4. 代码生成器
通过代码生成器可以快速生成实体类、Mapper 接口、Service 接口及实现类、Controller 等。


AutoGenerator mpg = new AutoGenerator();

// 全局配置
GlobalConfig gc = new GlobalConfig();
gc.setOutputDir("D://mybatis-plus");
gc.setAuthor("Author");
mpg.setGlobalConfig(gc);

// 数据源配置
DataSourceConfig dsc = new DataSourceConfig();
dsc.setUrl("jdbc:mysql://localhost:3306/testdb");
dsc.setDriverName("com.mysql.cj.jdbc.Driver");
dsc.setUsername("root");
dsc.setPassword("password");
mpg.setDataSource(dsc);

// 包配置
PackageConfig pc = new PackageConfig();
pc.setModuleName("module");
pc.setParent("com.example");
mpg.setPackageInfo(pc);

// 执行生成
mpg.execute();


列式存储数据库：
列存储不同于传统的关系型数据库，其数据在表中是按行存储的，列方式所带来的重要好处之一就是，由于查询中的选择规则是通过列来定义的，因此整个数据库是自动索引化的。
列式数据库是以列相关存储架构进行数据存储的数据库，主要适合于批量数据处理和即时查询。相对应的是行式数据库，数据以行相关的存储体系架构进行空间分配，主要适合于大批量的数据处理，常用于联机事务型数据处理。
列存储数据库使用一个称为 keyspace 的概念。keyspace 有点像关系模型中的模式。keyspace 包含所有列族(有点像关系模型中的表)，其中包含行，包含列。
列式存储多用于OLAP(Online analytical processing)联机分析处理系统

>列族由多行组成。
>每一行可以包含与其他行不同数量的列。而且这些列不必与其他行的列匹配(例如，它们可以有不同的列名、数据类型、数量等)。
>每行包含一列。它不像关系数据库那样跨所有行。每个列包含一个名称/值对，以及一个时间戳。

		  Column 		Column 		   Column
		  Name 			Name		   Name 
Row Key   Value 		Value 		   Value
		  Timestamp   	Timestamp	   Timestamp

每一行的结构：
Row Key：每一行都有一个惟一的键，这是该行的惟一标识符。
Column：每个列包含名称、值和时间戳。
Name：KV 对的 K
Value：KV 对的 V
Timestamp：这提供了插入数据的日期和时间。这可以用来确定数据的最新版本。

特点/优点
高效的压缩效率，节省磁盘空间和计算CPU和内存
基于 CPU L2 缓存高效的数据迭代
压缩算法：列式数据库由于其每一列都是分开储存的。所以很容易针对每一列的特征运用不同的压缩算法。常见的列式数据库压缩算法有Run Length Encoding ， Data Dictionary ， Delta Compression ， BitMap Index ， LZO ， Null Compression 等等。根据不同的特征进行的压缩效率从10W:1 到10:1 不等。而且数据越大其压缩效率的提升越为明显。
延迟物化：列式数据库由于其特殊的执行引擎，在数据中间过程运算的时候一般不需要解压数据而是以指针代替运算，直到最后需要输出完整的数据时。
聚合查询：由于它们的结构，柱状数据库在聚合查询(如SUM、COUNT、AVG等)方面表现得特别好。
可扩展性：列式存储数据库是可伸缩的。它们非常适合大规模并行处理(MPP)，这涉及到将数据分散到一个大的机器集群中——通常是数千台机器。
快速查询和写入：可以非常快地加载。可以在几秒钟内加载十亿行表。几乎可以立即开始查询和分析。

列式存储数据库有：1.HBase；2.ClickHouse；3.Druid；4.HP Vertica（也支持行式存储）。

非关系型数据库(nosql)：
redis：
Redis 是一个开源(BSD许可)的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。
它支持多种类型的数据结构，如 字符串 (strings) ，散列 (hashes) ， 列表(lists)集合 (sets)，有集合 (sorted sets) 与范围查询，bitmaps， hyperloglogs和地理空间 (geospatial) 索引半径查询。 
Redis 内置了 复制 (replication) ，LUA脚本(Luascripting)，LRU驱动事件(LRU eviction)，事务 transactions) 
和不同级别的 磁盘持久化(persistence) ，并通过 Redis哨兵 (Sentinel) 和自动 分区(Cluster) 提供高可用性 (highavailability)

redis五大数据类型：
1.String字符串
特性：
最基本的数据类型，一个键对应一个字符串值。
值可以是字符串、整数或浮点数。

使用场景：
缓存单个值，如缓存用户信息、页面内容。
计数器，如网站访问量计数。

常用命令：
SET key value：设置指定键的值。
GET key：获取指定键的值。
INCR key：将指定键的值加1。
DECR key：将指定键的值减1。
APPEND key value：将指定值追加到原值末尾。

2.List
特性：
有序列表，可以在头部或尾部插入和删除元素。
适合实现消息队列等功能。

使用场景：
消息队列，任务调度系统。
实现最新消息排行，如微博时间线。

常用命令：
LPUSH key value：在列表头部插入一个值。
RPUSH key value：在列表尾部插入一个值。
LPOP key：移除并返回列表头部的值。
RPOP key：移除并返回列表尾部的值。
LRANGE key start stop：获取列表指定范围内的元素。

3.Set
特性：
无序集合，集合中的元素唯一且不重复。
提供集合的交集、并集和差集操作。

使用场景：
标签系统，如用户标签、文章标签。
去重操作，如抽奖活动中的唯一用户。

常用命令：
SADD key member：向集合添加一个元素。
SREM key member：移除集合中的一个元素。
SMEMBERS key：获取集合中的所有元素。
SISMEMBER key member：判断元素是否在集合中。
SUNION key1 key2 ...：返回多个集合的并集。

4.Hash
特性：
一种键值对集合，即一个键对应一个字典。
适合存储对象，字段和值可以灵活操作。

使用场景：
存储用户信息等对象数据，每个字段对应一个属性。
实现轻量级的数据库表结构。

常用命令：
HSET key field value：设置哈希表指定字段的值。
HGET key field：获取哈希表指定字段的值。
HGETALL key：获取哈希表中的所有字段和值。
HMSET key field1 value1 field2 value2 ...：批量设置多个字段的值。
HDEL key field：删除哈希表中的一个或多个字段。

5.ZSet
特性：
有序集合，每个元素关联一个评分（score），集合中的元素按评分排序。
提供按评分范围或排名范围操作。

使用场景：
排行榜系统，如游戏积分榜。
延迟队列，任务调度系统。

常用命令：
ZADD key score member：向有序集合添加一个元素及其评分。
ZRANGE key start stop [WITHSCORES]：按排名范围获取有序集合中的元素。
ZRANK key member：获取元素在有序集合中的排名。
ZREM key member：移除有序集合中的一个元素。
ZSCORE key member：获取元素的评分。

Redis数据类型对应的编码：

1. 字符串（String）
编码方式：
int：当字符串值只包含整数，且可以用64位有符号整数表示时，Redis会将其编码为int类型，以节省内存空间。
raw（或称为embstr和SDS）：如果字符串值无法被int编码表示，Redis会将其编码为raw类型，即普通的字符串编码方式。
raw编码方式中，Redis使用简单动态字符串（Simple Dynamic String, SDS）来存储字符串，SDS是一种动态字符串结构，由长度、空闲空间和字节数组三部分组成。
在Redis 7.0之前，对于小于等于44字节的字符串，Redis使用embstr编码，该编码方式会分配一个固定大小的空间（如64字节），并将数据存储在内部。对于大于44字节的字符串，Redis则使用raw编码。

注意：从Redis 7.0开始，引入了新的数据结构ListPack来替代ZipList，但这一变化主要影响列表（List）和哈希（Hash）等类型的底层实现，对字符串类型的编码方式影响不大。

2. 列表（List）
编码方式：
ziplist：当列表的元素都是小整数或较短的字符串时，Redis会将列表编码为ziplist类型。ziplist是一种紧凑的编码方式，可以节省内存空间。
linkedlist（或称为quicklist）：在Redis 3.2之前，当列表的元素包含较长的字符串时，Redis会将列表编码为linkedlist类型，这是一种双向链表的编码方式。但从Redis 3.2开始，引入了quicklist，它是linkedlist和ziplist的结合体，用于优化列表的存储效率和内存使用。

3. 集合（Set）
编码方式：
intset：当集合的元素都是小整数时，Redis会将集合编码为intset类型，这是一种紧凑的编码方式，可以节省内存空间。
hashtable：当集合的元素包含较长的字符串时，Redis会将集合编码为hashtable类型，通过哈希函数来快速定位元素。

4. 哈希（Hash）
编码方式：
ziplist（Redis 7.0之前）或listpack（Redis 7.0及以后）：当哈希的字段和值都可以存储为小整数或较短的字符串时，Redis会将哈希编码为ziplist（或listpack）类型。这是一种紧凑的编码方式，可以节省内存空间。
hashtable：当哈希的字段和值包含较长的字符串时，Redis会将哈希编码为hashtable类型，通过哈希函数来快速定位字段和值。

5. 有序集合（Zset）
编码方式：
ziplist：当有序集合的元素数量较少，且每个元素的成员和分数都比较小时，Redis会使用ziplist编码。
skiplist：当有序集合的元素数量较多，或元素较大时，Redis会使用skiplist编码。Skiplist是一种跳跃表数据结构，它可以在O(log N)时间复杂度内完成元素的查找、插入和删除操作。

总结
Redis的数据类型及其编码方式设计得非常灵活，旨在根据不同的数据特性和使用场景来优化内存使用和访问效率。了解这些编码方式有助于更好地理解Redis的内部工作机制，并优化Redis的使用。

SpringBoot整合Redis：
SpringBoot配置Redis工具类
@Configuration
public class RedisConfig {
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory factory) {
        RedisTemplate<String, Object> template = new RedisTemplate<String, Object>();
        template.setConnectionFactory(factory);
        // key采用String的序列化方式
        template.setKeySerializer(new StringRedisSerializer());
        // hash的key也采用String的序列化方式
        template.setHashKeySerializer(new StringRedisSerializer());
        // value序列化方式采用jackson
        template.setValueSerializer(new GenericJackson2JsonRedisSerializer());
        // hash的value序列化方式采用jackson
        template.setHashValueSerializer(new GenericJackson2JsonRedisSerializer());
        template.afterPropertiesSet();
        return template;
    }

}

redis缓存穿透：
Redis缓存穿透是指查询一个根本不存在的数据，由于缓存层和数据库层都没有命中，导致每次请求都会直接到达数据库。这种情况如果发生频繁，会对数据库造成很大的压力。

解决Redis缓存穿透的方法有几种，下面介绍几种常见的方法：

1.缓存空结果：
当查询一个不存在的key时，将这个key对应的空结果（如null或者特定的标志）也缓存起来，并设置一个较短的过期时间，防止恶意请求一直打到数据库。

String key = "user::" + userId;
String value = redis.get(key);
if (value == null) {
    value = db.queryUser(userId);
    if (value == null) {
        redis.set(key, "", 60);  // 设置空结果缓存1分钟
    } else {
        redis.set(key, value, 3600);  // 缓存正常结果1小时
    }
}
return value;

2.布隆过滤器：
在请求到达缓存层之前，使用布隆过滤器判断key是否存在。如果布隆过滤器判断key不存在，就直接返回空结果，避免对数据库的查询。

BloomFilter bloomFilter = ... // 初始化布隆过滤器并填充已有数据

String key = "user::" + userId;
if (!bloomFilter.mightContain(key)) {
    return null;  // 布隆过滤器认为key不存在，直接返回空
}

String value = redis.get(key);
if (value == null) {
    value = db.queryUser(userId);
    if (value == null) {
        redis.set(key, "", 60);  // 设置空结果缓存1分钟
    } else {
        redis.set(key, value, 3600);  // 缓存正常结果1小时
    }
}
return value;

3.参数校验：
对请求参数进行严格的校验和过滤，避免非法请求直接打到缓存和数据库层。

4.限流：
对于同一请求源（如IP地址）进行限流，防止恶意请求高频访问导致缓存穿透。


redis缓存雪崩：
Redis缓存雪崩问题是指大量缓存同时过期或Redis服务器宕机，导致大量请求直接打到数据库，造成数据库瞬时压力剧增，甚至导致系统崩溃。这种情况通常是因为缓存设置不当或服务器故障引起的。

为了解决Redis缓存雪崩问题，可以采取以下几种措施：

缓存过期时间设置随机化：
为不同的缓存设置不同的过期时间，避免大批量缓存同时过期。

int expirationTime = 3600 + new Random().nextInt(600); // 3600秒到4200秒之间随机
redis.set(key, value, expirationTime);

缓存预热：
在系统启动时，提前将热点数据加载到缓存中，避免在流量高峰期缓存未命中。

public void cachePreheat() {
    List<String> hotKeys = getHotKeysFromDatabase();
    for (String key : hotKeys) {
        String value = db.query(key);
        redis.set(key, value, 3600);
    }
}

双重缓存机制：
使用多级缓存机制，第一层使用本地缓存，第二层使用分布式缓存。在Redis宕机时，本地缓存可以暂时承担部分请求。

String key = "user::" + userId;
String value = localCache.get(key);
if (value == null) {
    value = redis.get(key);
    if (value == null) {
        value = db.queryUser(userId);
        if (value != null) {
            redis.set(key, value, 3600);
        }
    }
    localCache.put(key, value);
}
return value;

请求限流与降级：
对请求进行限流，设置最大并发数或请求速率；在缓存失效时，可以降级处理，返回部分默认数据或提示服务不可用。

RateLimiter rateLimiter = RateLimiter.create(100); // 每秒最多处理100个请求
if (!rateLimiter.tryAcquire()) {
    return "System busy, please try again later";
}

String key = "user::" + userId;
String value = redis.get(key);
if (value == null) {
    value = db.queryUser(userId);
    if (value != null) {
        redis.set(key, value, 3600);
    } else {
        return "Data not available";
    }
}
return value;

监控与自动恢复：
监控Redis的运行状态和缓存命中率，及时发现和处理问题。可以通过脚本或运维工具实现自动恢复。

# 监控脚本示例
redis-cli ping
if [ $? -ne 0 ]; then
    # 重启Redis服务或切换到备用服务
    systemctl restart redis
fi
通过这些措施，可以有效缓解Redis缓存雪崩问题，提升系统的稳定性和可靠性。根据具体情况，可以组合使用这些方法，达到最佳效果。


Redis缓存击穿：
Redis缓存击穿问题是指某个热点数据在缓存失效的瞬间，有大量并发请求同时查询这个数据，导致请求直接打到数据库，可能引起数据库压力骤增，甚至宕机。解决缓存击穿问题的常用方法有以下几种：

使用互斥锁：
在缓存失效时，通过加锁的方式，确保只有一个线程能查询数据库并更新缓存，其他线程等待。

String key = "user::" + userId;
String value = redis.get(key);
if (value == null) {
    synchronized (this) { // 锁定
        value = redis.get(key); // 再次检查缓存，防止重复加载
        if (value == null) {
            value = db.queryUser(userId);
            redis.set(key, value, 3600);
        }
    }
}
return value;

使用分布式锁：
在分布式环境下，可以使用Redis自身的分布式锁机制，如SETNX命令，确保只有一个进程查询数据库。

String key = "user::" + userId;
String lockKey = "lock::" + key;
String value = redis.get(key);
if (value == null) {
    if (redis.setnx(lockKey, "1")) { // 尝试获取锁
        try {
            value = db.queryUser(userId);
            redis.set(key, value, 3600);
        } finally {
            redis.del(lockKey); // 释放锁
        }
    } else {
        // 其他线程等待一段时间后重试
        Thread.sleep(100);
        value = redis.get(key);
    }
}
return value;

缓存降级：
在缓存失效且数据库压力较大时，返回默认数据或友好提示，避免直接查询数据库。

String key = "user::" + userId;
String value = redis.get(key);
if (value == null) {
    if (shouldQueryDatabase()) { // 根据数据库负载情况决定是否查询
        value = db.queryUser(userId);
        if (value != null) {
            redis.set(key, value, 3600);
        } else {
            value = "Default Data"; // 返回默认数据
        }
    } else {
        value = "Service busy, please try again later";
    }
}
return value;

预加载和续期：
对于热点数据，定期刷新缓存的过期时间，或者在即将过期时提前加载新数据，确保缓存不过期。

public void preloadCache() {
    List<String> hotKeys = getHotKeys();
    for (String key : hotKeys) {
        String value = db.query(key);
        redis.set(key, value, 3600);
    }
}

// 在缓存即将过期时续期
String key = "user::" + userId;
String value = redis.get(key);
if (value != null && redis.ttl(key) < 600) { // 距离过期时间小于10分钟
    redis.expire(key, 3600); // 续期1小时
}

使用多级缓存：
利用本地缓存和Redis缓存相结合的方式，减少对Redis和数据库的直接访问。

String key = "user::" + userId;
String value = localCache.get(key);
if (value == null) {
    value = redis.get(key);
    if (value == null) {
        value = db.queryUser(userId);
        redis.set(key, value, 3600);
    }
    localCache.put(key, value);
}
return value;

Redis如何解决数据倾斜问题？
数据倾斜是指数据在Redis节点之间分布不均匀，导致某些节点承载了过多的数据或流量，影响整体性能和可靠性。Redis在分布式环境中常采用分片（sharding）来解决数据倾斜问题。
以下是几种常见的解决方法：

1. 合理设计分片策略
哈希分片：使用一致性哈希算法（Consistent Hashing）将数据分布到不同的节点上。这样可以减少因节点增加或减少导致的数据重分布问题。

// 计算key的哈希值并映射到相应的节点
int hash = hashFunction(key);
int nodeIndex = hash % numberOfNodes;
RedisNode node = nodes.get(nodeIndex);
node.set(key, value);
预分片：在系统设计初期就预先分配好较多的虚拟节点，通过这些虚拟节点将数据均匀分布到实际节点上。当添加或删除节点时，只需少量的数据迁移。

int numberOfVirtualNodes = 1000; // 大量虚拟节点
int virtualNodeIndex = hash % numberOfVirtualNodes;
int realNodeIndex = virtualNodeIndex % numberOfRealNodes;
RedisNode node = nodes.get(realNodeIndex);
node.set(key, value);

2. 动态调整分片
数据重分布：监控每个节点的负载情况，定期或在负载不均衡时，重新分配数据，确保各个节点负载均衡。

public void rebalance() {
    // 计算每个节点的负载
    for (RedisNode node : nodes) {
        int load = node.getLoad();
        if (load > threshold) {
            redistribute(node);
        }
    }
}

private void redistribute(RedisNode overloadedNode) {
    // 将部分数据迁移到负载较轻的节点
    List<Data> dataToMove = overloadedNode.getHeavyData();
    for (Data data : dataToMove) {
        RedisNode targetNode = findTargetNode(data);
        targetNode.set(data.getKey(), data.getValue());
        overloadedNode.delete(data.getKey());
    }
}

3. 使用Redis Cluster
Redis Cluster：Redis官方提供的解决方案，可以自动进行数据分片和负载均衡。它将数据分为16384个插槽（slots），并将这些插槽分布在不同的节点上。Redis Cluster能够在节点增加或减少时自动调整数据分布。

# 创建Redis Cluster节点
redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --cluster-announce-ip 192.168.0.1 --port 7000

# 添加节点到集群
redis-cli --cluster add-node 192.168.0.2:7001 192.168.0.1:7000

4. 数据预热与缓存策略
数据预热：在系统启动或高峰期之前，将热点数据预先加载到各个节点，确保负载均衡。

public void preheatCache() {
    List<String> hotKeys = getHotKeysFromDatabase();
    for (String key : hotKeys) {
        int hash = hashFunction(key);
        int nodeIndex = hash % numberOfNodes;
        RedisNode node = nodes.get(nodeIndex);
        node.set(key, db.query(key));
    }
}
缓存层次化：使用多级缓存，如本地缓存、Redis缓存和数据库，减少对单个Redis节点的压力。

String key = "user::" + userId;
String value = localCache.get(key);
if (value == null) {
    value = redis.get(key);
    if (value == null) {
        value = db.queryUser(userId);
        redis.set(key, value, 3600);
    }
    localCache.put(key, value);
}
return value;

5. 监控与告警
监控系统：部署监控系统，实时监控Redis集群的负载情况和数据分布，及时发现和处理数据倾斜问题。

# 使用Redis自带的INFO命令获取节点信息
redis-cli -h 192.168.0.1 -p 7000 INFO

# 使用第三方监控工具，如Prometheus和Grafana
通过以上方法，可以有效解决Redis的数据倾斜问题，确保系统在高并发和大数据量场景下的稳定性和性能。根据具体需求和场景，可以选择一种或多种方法组合使用。


Redis分布式锁：
Redis分布式锁是一种基于Redis实现的锁机制，用于在分布式系统中控制资源的并发访问。Redis分布式锁的实现需要考虑锁的获取和释放，以及一些特殊情况下的处理。常用的实现方式是使用Redis的SET命令，配合NX（仅在键不存在时设置）和EX（过期时间）选项来实现。

分布式锁的实现
获取锁
使用SET命令获取锁，确保原子性操作：
SET lock_key unique_identifier NX EX expire_time

lock_key：锁的键。
unique_identifier：唯一标识符，用于识别持有锁的客户端。
NX：仅在键不存在时设置。
EX：过期时间，防止死锁。

示例代码：
String lockKey = "my_lock";
String uniqueId = UUID.randomUUID().toString();
int expireTime = 30; // 30 seconds
String result = jedis.set(lockKey, uniqueId, SetParams.setParams().nx().ex(expireTime));
if ("OK".equals(result)) {
    // Lock acquired
} else {
    // Failed to acquire lock
}


释放锁
使用Lua脚本原子性地释放锁，确保只释放自己持有的锁：
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end

示例代码：
String script = "if redis.call('get', KEYS[0]) == ARGV[0] then " +
                "return redis.call('del', KEYS[0]) " +
                "else return 0 end";
Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(uniqueId));
if (result.equals(1L)) {
    // Lock released
} else {
    // Failed to release lock
}

注意事项
1.唯一标识符：每个客户端获取锁时应生成一个唯一标识符，确保只有锁的持有者能释放锁。
2.设置过期时间：锁的过期时间应合理设置，防止因客户端崩溃或网络分区导致的死锁。过期时间应略大于业务操作的最长时间。
3.锁续期：对于长时间的业务操作，可以使用锁续期机制（如Redisson的可重入锁）来防止锁因过期被误释放。
4.避免单点故障：在单个Redis实例上实现的分布式锁存在单点故障风险，推荐使用Redlock算法。

Redlock算法
Redlock是由Redis创始人Antirez提出的一种实现分布式锁的算法，适用于高可靠性要求的场景。它在多个独立的Redis节点上获取锁，从而提高容错性。

示例代码：
public boolean tryGetDistributedLock(String lockKey, String requestId, int expireTime) {
    long startTime = System.currentTimeMillis();
    int quorum = redisNodes.size() / 2 + 1;
    int successCount = 0;

    for (Jedis jedis : redisNodes) {
        String result = jedis.set(lockKey, requestId, SetParams.setParams().nx().px(expireTime));
        if ("OK".equals(result)) {
            successCount++;
        }
    }

    long elapsedTime = System.currentTimeMillis() - startTime;
    if (successCount >= quorum && elapsedTime < expireTime) {
        return true; // Lock acquired
    } else {
        // Lock failed, release any acquired locks
        for (Jedis jedis : redisNodes) {
            jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));
        }
        return false;
    }
}

Redis分布式锁的弊端：
Redis分布式锁虽然提供了一种简便的分布式并发控制机制，但在实际应用中需要注意其潜在的弊端和挑战。通过合理设计和优化，可以有效减少这些问题的影响，确保系统的稳定性和一致性。

以下是一些关键建议：
1.使用Redis集群或Redlock算法，提高系统的可靠性和容错性。
2.合理设置锁的过期时间，并实现锁续期机制，防止锁失效和死锁问题。
3.确保系统时钟同步，避免时钟漂移引发的锁机制失效。
4.优化锁的粒度和锁操作，减少锁竞争，提高系统性能。
5.通过综合考虑这些因素，可以在分布式系统中更好地应用Redis分布式锁，实现高效的并发控制。


redis分布式锁和主从复制有哪些场景上的区别
Redis分布式锁和Redis主从复制（Replication）在使用场景和目的上有很大的不同。以下是两者在场景上的区别：

Redis分布式锁场景和目的

1. 并发控制：
   - 用于在分布式系统中控制多个节点对共享资源的并发访问，确保数据的一致性和避免数据竞争。
   - 例如：在电商系统中，防止多个用户同时购买同一件商品导致超卖。

2. 任务调度：
   - 确保在分布式环境中某个任务在某一时刻只由一个实例执行，避免任务重复执行。
   - 例如：分布式任务调度系统中，同一任务在多个实例中只执行一次。

3. 限流和排队：
   - 控制并发请求的数量，避免系统过载。
   - 例如：API限流，限制某个接口的访问频率。

4. 分布式事务：
   - 在跨多个服务或数据库的事务中使用锁来确保操作的一致性。
   - 例如：订单系统中，确保库存扣减和订单创建的原子性操作。

实现方式
- 使用Redis的SET命令结合NX（仅在键不存在时设置）和EX（过期时间）选项实现。
- 使用Redlock算法在多个Redis实例上获取锁，提高容错性和一致性。
- 常用命令：`SET key value NX EX seconds`，`DEL key`，Lua脚本实现原子性操作。


Redis分布式锁的看门狗机制：
Redis分布式锁的看门狗（Watch Dog）机制是Redisson客户端在使用Redis实现分布式锁时提供的一种重要功能。
该机制主要用于确保在客户端持有锁期间，锁不会因为超时而被意外释放，从而保证了操作的安全性和原子性。

看门狗机制的好处包括：
避免死锁：即使在客户端操作异常长的情况下，也可以保证锁最终被释放，避免死锁的发生。
操作安全性：通过保证在操作期间锁一直有效，确保了操作的安全性和数据的一致性。


Redis主从复制：Redis主从复制通过数据的单向复制，实现了数据的备份和负载均衡。通过全量复制和部分复制的机制，确保了数据的完整性和同步效率。
场景和目的
1. 高可用性：
   - 通过主从复制实现数据的冗余备份，防止单点故障。
   - 例如：当主节点故障时，可以通过从节点接管，确保数据的高可用性。

2. 读写分离：
   - 将读请求分散到从节点上，提高系统的读性能和扩展性。
   - 例如：高并发场景下，主节点负责写操作，从节点负责读操作，减轻主节点压力。

3. 数据备份和恢复：
   - 通过复制机制实现数据的实时备份，方便故障恢复。
   - 例如：定期从从节点生成数据快照，以防止数据丢失。

4. 容灾和灾备：
   - 在不同地理位置部署从节点，实现跨数据中心的容灾备份。
   - 例如：在不同城市部署主从节点，确保数据在自然灾害等情况下的安全性。

实现方式

- 配置Redis的主从复制，通过`SLAVEOF`命令或配置文件设置从节点指向主节点。
- 使用Redis的自动故障转移机制（如Redis Sentinel）来监控主从节点状态并自动切换。
- 常用命令：`SLAVEOF host port`，`INFO replication`，`SENTINEL`命令集。

总结
Redis分布式锁侧重于分布式系统中的并发控制和任务协调，确保多个节点对共享资源的有序访问，防止数据竞争和不一致。而Redis主从复制则侧重于数据的高可用性、读写分离和容灾备份，确保系统的可靠性和可扩展性。

具体使用场景和实现方式如下：

| 特性           | Redis分布式锁                             | Redis主从复制                               |
| -------------- | ---------------------------------------- | ------------------------------------------ |
| 主要用途       | 并发控制、任务调度、限流和分布式事务         | 高可用性、读写分离、数据备份和容灾            |
| 实现方式       | 使用`SET NX EX`命令、Redlock算法等         | 配置主从复制、使用`SLAVEOF`命令、Redis Sentinel |
| 典型场景       | 防止超卖、任务调度、API限流、分布式事务      | 主节点故障切换、提高读性能、跨数据中心容灾     |
| 常用命令       | `SET key value NX EX seconds`，`DEL key`  | `SLAVEOF host port`，`INFO replication`    |

通过了解两者的区别，可以根据具体业务需求选择合适的Redis功能，以实现系统的可靠性和高效性。


Memcache:
memcache是一个高性能的分布式的内存对象缓存系统，用于动态Web应用以减轻数据库负担。
它通过在内存中缓存数据和对象，来减少读取数据库的次数。从而提高动态、数据库驱动网站速度。
memcache通过在内存里维护一个统一的巨大的hash表，它能够用来存储各种格式的数据，包括图像、视频、文件以及数据库检索的结果等。
memcache主要用于分担数据库负的压力，memcache将数据调用到内存中，然后从内存中读取，从而大大提高读取速度。

优点：
1.Memcached可以利用多核优势，单实例吞吐量极高，可以达到几十万QPS（取决于key、value的字节大小以及服务器硬件性能，日常环境中QPS高峰大约在4-6w左右）。适用于最大程度扛量。
2.支持直接配置为session handle。

缺点：
1只支持简单的key/value数据结构，不像Redis可以支持丰富的数据类型。
2.无法进行持久化，数据不能备份，只能用于缓存使用，且重启后数据全部丢失。
3.无法进行数据同步，不能将MC中的数据迁移到其他MC实例中。
4.Memcached内存分配采用Slab Allocation机制管理内存，value大小分布差异较大时会造成内存利用率降低，并引发低利用率时依然出现踢出等问题。需要用户注重value设计。


MongoDB:
MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。
在高负载的情况下，添加更多的节点，可以保证服务器性能。
MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。
MongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。
MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。

优点：
1.更高的写负载，MongoDB拥有更高的插入速度。
2.处理很大的规模的单表，当数据表太大的时候可以很容易的分割表。
3.高可用性，设置M-S不仅方便而且很快，MongoDB还可以快速、安全及自动化的实现节点（数据中心）故障转移。
4.快速的查询，MongoDB支持二维空间索引，比如管道，因此可以快速及精确的从指定位置获取数据。MongoDB在启动后会将数据库中的数据以文件映射的方式加载到内存中。如果内存资源相当丰富的话，这将极大地提高数据库的查询速度。
5.非结构化数据的爆发增长，增加列在有些情况下可能锁定整个数据库，或者增加负载从而导致性能下降，由于MongoDB的弱数据结构模式，添加1个新字段不会对旧表格有任何影响，整个过程会非常快速。

缺点：
1.不支持事务。
2.MongoDB占用空间过大 。
3.MongoDB没有成熟的维护工具。


ShardingSphere：
ShardingSphere是一个开源的分布式数据库中间件，用于实现关系型数据库的分片和分布式数据库操作。
它提供了跨多个数据库实例的数据分片、读写分离、分布式事务等功能，以帮助开发人员构建可伸缩和高可用性的数据库架构。

ShardingSphere 主要包括以下子项目：

1.Sharding-JDBC：用于实现关系型数据库的数据分片（Sharding）和读写分离（Master-Slave）功能。
它允许应用程序在使用关系型数据库时将数据分散到多个数据库实例，从而提高数据库性能和扩展性。
2.Sharding-Proxy：这是一个数据库代理，它支持分片和分布式事务，并允许应用程序通过一个中心化的访问点来访问分布式数据库，而不需要直接连接到底层数据库实例。
3.Sharding-Sidecar：这是 ShardingSphere 的云原生版本，它提供了 Kubernetes 和 Service Mesh（如 Istio）的支持，
以便更轻松地在云原生环境中部署和管理 ShardingSphere。

ShardingSphere 的主要特点包括：

- 数据分片：支持水平分片和垂直分片，允许将数据分散存储在不同的数据库实例中。
- 读写分离：可以将读操作分发到只读数据库实例，从而减轻主数据库的负载。
- 分布式事务：提供分布式事务支持，确保多个数据库实例之间的一致性。
- 高可用性：支持数据库故障切换和自动恢复，以确保系统的高可用性。
- 多种数据库支持：支持多种关系型数据库，如MySQL、PostgreSQL、Oracle等。

ShardingSphere 是一个强大的工具，可用于构建大规模、高性能和高可用性的分布式数据库架构，特别适用于云原生环境和大型企业应用程序。


Java容器技术：
容器技术是一种软件开发和部署技术，允许开发人员在虚拟环境中将应用程序与其依赖关系封装在一起，以实现在不同的环境中的一致性。
容器技术通过使用容器映像来管理应用程序，并在系统上运行容器实例。容器映像是包含所有应用程序代码和依赖关系的可执行文件。容器实例则是运行在特定环境中的应用程序副本。

容器技术的优势在于：
避免了环境问题：应用程序在容器中与其依赖关系一起运行，因此可以避免环境问题导致的故障。
提高了部署效率：容器可以快速部署，并且可以方便地进行版本管理和回滚。
支持微服务架构：容器可以作为微服务架构的基础设施，帮助开发人员快速构建、部署和管理复杂的分布式系统。
目前，Docker是最常用的容器技术，它提供了一个开放的生态系统，可以方便地管理容器。


CI/CD流水线：
CI/CD是持续集成和持续交付或持续部署的软件开发流程模型。CI/CD旨在通过自动化和持续性的构建、测试、部署和交付过程，来提高软件开发和发布的效率和质量。
CI/CD的目标是缩短软件开发和发布的周期，降低开发和发布的成本和风险，以满足快速变化和不断迭代的业务需求。
CI/CD中的"CI"指的是持续集成，是指开发人员将代码不断地提交到源代码管理系统中，在此过程中，
自动化的测试和构建工具会自动从源代码库中获取最新的代码，进行编译、测试、打包等操作，并生成相应的构建产物；
"CD"指的是持续交付和/或持续部署，将构建产物部署到测试环境进行测试和验证，最终生成可部署的产物，
实现自动化的部署和发布，从而实现快速的交付和迭代。

CI/CD 中的 "CI"指的是持续集成：
持续集成属于开发人员的自动化流程，它帮助开发人员更加频繁地将代码的更改合并到主分支或共享分支，
并自动构建和运行不同级别的自动化测试来验证这些更改，确保这些更改没有对应用造成破坏。

CI/CD中的"CD"指的是持续交付和/或持续部署：
持续交付（Continuous Delivery）的目的是最小化部署或释放过程中固有的摩擦，实现自动化构建和部署。
持续部署（Continuous Deployment）指的是自动将开发人员的更改从存储库发布到生产环境，以供客户使用，
主要为了解决因手动流程降低应用交付速度，从而使运维团队超负荷的问题。


CI/CD流水线将程序部署到Kubernetes（k8s）的方式如下（以Jenkins为例）：

1.建立流水线项目。首先建立一个Jenkins流水线项目，并配置相关参数，如选择源代码管理系统、设置构建环境等。
2.从源代码管理系统拉取代码。Jenkins通过参数化的构建方式，从源代码管理系统中拉取代码并下载到本地。
3.编译代码。通过Dockerfile的方式对代码进行编译和制作镜像，并将镜像上传到镜像仓库。
4.部署应用。流水线项目将应用部署到Kubernetes集群中。
5.配置邮件通知功能。配置邮件通知功能，将镜像构建结果通过邮件的方式通知给相关人员。


前端项目如何部署？
前端项目的部署流程：
1.链接服务器：通过ssh @root[ipaddress]命令链接你的服务器（将ipaddress替换成你服务器的公网IP地址）。
2.配置服务器：在服务器上配置Apache或Nginx的静态文件目录，以及反向代理，将客户端请求转发到前端项目的访问入口文件index.html。
3.自动化部署：使用自动化工具，如Jenkins、Travis CI等，在Github中托管代码，Travis CI将自动触发构建和部署流程，
生成可运行的前端代码并部署到服务器上。


Docker的镜像技术是一种特殊的文件系统，它除了提供容器运行时所需的程序、库、资源、配置等文件外，
还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。
镜像不包含任何动态数据，其内容在构建之后也不会被改变。
Docker镜像的只读形式，在启动Docker容器中，文件系统结构和内容都包含在其中，Docker镜像是启动Docker容器的基础。
Docker镜像的文件内容和配置文件组成了Docker容器的静态文件系统运行环境--rootfs。
简单的理解为：Docker镜像是Docker容器的静态视角，Docker容器是Docker镜像的运行状态。
rootfs是Docker容器的根目录，在启动时Docker容器可见到的文件系统，rootfs包含了操作系统运行所需的文件系统，
例如：dev、proc、bin、etc、lib等Unix目录系统以及配置文件、工具等。



镜像技术的优势在于：
简化部署：使用镜像可以快速地将应用程序部署到任何环境中。
提高一致性：镜像文件包含了所有所需的文件和配置，因此可以保证在不同的环境中的一致性。
方便回滚：如果一个镜像不能正常工作，可以方便地回滚到以前的版本。
镜像技术是虚拟化、容器化和云计算等计算机领域的重要技术，在微服务架构、云部署等场景中被广泛使用。
常见的镜像技术有Docker镜像、Kubernetes镜像、OpenShift镜像等。



Docker：
Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。
Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。
容器是完全使用沙箱机制，相互之间不会有任何接口,更重要的是容器性能开销极低。
Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像。
运行中的这个镜像称为容器，容器启动是非常快速的。类似windows里面的ghost操作系统，安装好后什么都有了；

Docker的原理：
Docker的原理基于Linux的核心命名空间（Namespaces）和核心资源隔离机制（Cgroups）以及AUFS文件系统进行创建独立的容器。

Docker利用Linux中的核心分离机制，例如Cgroups，以及Linux的核心Namespace来创建独立的容器。
Docker容器的本质还是一个直接运行在宿主机上面的特殊进程，看到的文件系统是隔离后的，
但是操作系统内核是共享宿主机的，所以说Docker是轻量级的虚拟化技术。


Docker的分层存储：
Docker中的分层存储是指Docker镜像的一种存储方式。它使用联合挂载系统（Union Mount）的技术，
其文件系统是分层的，将多个只读层叠加在一起，形成一个可读写的联合文件系统，以提供Docker镜像的高效存储和管理。
分层存储是Docker镜像轻量级、易于管理，以及更容易与其他Docker操作进行组合的重要原因。


Docker核心概念：
docker镜像(Images)：Docker 镜像是用于创建 Docker 容器的模板。
docker容器(Container)：容器是独立运行的一个或一组应用。
docker客户端(Client)：客户端通过命令行或者其他工具使用
Docker API(https://docs.docker.com/reference/api/docker_remote_api) 与 Docker 的守护进程通信
docker主机(Host)：一个物理或者虚拟的机器用于执行Docker 守护进程和容器。
docker仓库(Registry)：Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。
Docker Hub(https://hub.docker.com) 提供了庞大的镜像集合供使用。


为什么Docker比Vm快？
1、docker有着比虚拟机更少的抽象层。由于docker不需要Hypervisor实现硬件资源虚拟化,
运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。

2、docker利用的是宿主机的内核,而不需要Guest OS。
因此,当新建一个 容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。
仍而避免引导、加载操作系统内核返个比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载GuestOS,返个新建过程是分钟级别的。
而docker由于直接利用宿主机的操作系统,则省略了这个复杂的过程,因此新建一个docker容器只需要几秒钟。

kernel：Linux系统内核

boot file system （bootfs）：包含boot loader和kernel。用户不会修改这个文件系统。Linux系统刚启动时会加载bootfs文件系统；
而在启动完成后，整个内核都会被加载进内存，此时bootfs会被卸载掉从而释放出所占用的内存。
对于同样内核版本的不同的Linux发行版的bootfs都是一致的。

root file system （rootfs）：包含典型的目录结构，包括/dev, /proc, /bin, /etc, /lib, /usr, and /tmp等，
再加上要运行用户应用所需要的所有配置文件，二进制文件和库文件。这个文件系统在不同的Linux发行版中是不同的，
且用户可以对这个文件进行修改。



Kubernetes：
Kubernetes（简称K8s）是一个开源的、用于管理云平台中多个主机上的容器化的应用。

Kubernetes的目标是让部署容器化的应用简单并且高效，它提供了应用部署，规划，更新，维护的一种机制。
传统的应用部署方式是通过插件或脚本来安装应用，这样做有缺点，比如：应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，
不利于应用的升级更新/回滚等操作。

新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。
相对于虚拟机，容器能快速部署，由于容器与底层设施、机器文件系统解耦的，所以它能在不同云、不同版本操作系统间进行迁移。
容器占用资源少、部署快，每个应用可以被打包成一个容器镜像，每个应用与容器间成一对一关系也使容器有更大优势。
使用容器可以在build或release 的阶段，为应用创建容器镜像，因为每个应用不需要与其余的应用堆栈组合，
也不依赖于生产环境基础结构，这使得从研发到测试、生产能提供一致环境。
类似地，容器比虚拟机轻量、更“透明”，这更便于监控和管理。
       
K8S和Docker的区别：

K8S是一套自动化部署工具，可以管理Docker容器，是容器编排层面的，而Docker是一种开放源码的应用容器引擎，是容器层面的。
K8S是一个完整的分布式系统支撑平台，集群管理功能齐全，而Docker只是一种容器化技术。
K8S提供完善的管理工具，涵盖了开发、部署、测试、运行监控等各个环节，而Docker只实现了容器层面的虚拟化。



Java分布式系统：
分布式系统是计算机程序的集合，这些程序利用跨多个独立计算节点的计算资源来实现共同的目标。
它也被称为分布式计算或分布式数据库，并依靠不同的节点通过公共网络进行通信和同步。
这些节点通常代表独立的物理硬件设备，但也可代表单独的软件进程或其他递归封装的系统。
分布式系统旨在消除系统的瓶颈或中心故障点。

分布式框架通常使用以下技术：
1.分布式系统的基本架构，包括节点、通信、一致性、数据分布等。
2.分布式资源管理，如负载均衡、任务调度、数据复制等。
3.分布式通信技术，如RPC、消息队列等。
4.集群管理和监控技术，如ZooKeeper、Nagios等。
5.分布式数据存储技术，如分布式文件系统、NoSQL数据库等。
这些技术与其他技术，如容错、可扩展性、数据一致性等结合使用，以构建一个完整的分布式系统。

分布式系统模式发展阶段：
1.单一应用框架(ORM)
当网站流量很小时，只需一个应用，将所有功能如下单支付等都部署在一起，以减少部署节点和成本。
2.垂直应用框架(MVC)
垂直应用架构解决了单一应用架构所面临的扩容问题，流量能够分散到各个子系统当中，且系统的体积可控，
一定程度上降低了开发人员之间协同以及维护的成本，提升了开发效率。
MVC架构将网络交互层和数据交互层区分开，实现了业务读取和数据的分离。
3.分布式应用架构(RPC)
当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心。
不同业务场景的模块之间互相调用、通信。
4.流动计算架构(SOA)
随着服务化的进一步发展，服务越来越多，服务之间的调用和依赖关系也越来越复杂，诞生了面向服务的架构体系(SOA)，
也因此衍生出了一系列相应的技术，如对服务提供、服务调用、连接处理、通信协议、序列化方式、服务发现、服务路由、日志输出等行为进行封装的服务框架。
在分布式架构的基础上，延申出的架构。

对于分布式系统和垂直系统的理解：
一个微服务是垂直式MVC的集合，而分布式RPC是将多个微服务整合的系统，并保证系统的一致性、高性能、高可用和可拓展性等特性。

可以把分布式系统理解为集群吗？
分布式系统和集群是相关概念，但它们并不完全相同，尽管它们通常一起使用。以下是它们之间的区别：

分布式系统：
分布式系统是一种计算系统，其中多台计算机协同工作，共同完成一组相关任务或服务。
这些计算机可以分布在不同的地理位置，彼此之间通过网络连接。
分布式系统的设计目标是提高性能、可伸缩性、可靠性和可用性，同时允许更好的资源利用和负载分配。
分布式系统通常包括多个独立的计算节点，这些节点可以运行不同的服务或应用程序，并相互协调合作，以提供某种服务。

集群：
集群是一组紧密连接的计算机（节点）或服务器，它们共同工作以提供高可用性、容错性和负载均衡的服务。
这些计算机通常部署在同一物理位置，并连接在一起，以实现共享资源和协同工作。
集群的目标是提高性能和可靠性，以便在一个节点失败时可以无缝切换到其他节点，从而保持服务的连续性。
集群可以是部署在单一数据中心或多个数据中心的服务器集合。
虽然集群通常用于构建分布式系统的基础架构，但分布式系统更广泛，可以跨越多个地理位置，涉及多个应用程序和服务之间的协同工作。集群则更侧重于提供高可用性和性能，通常在相对较小的范围内操作。

因此，分布式系统可以包含多个集群，但它们不是同一概念。集群是分布式系统的一部分，用于支持特定服务或应用程序的高可用性和负载均衡需求。
简单的来说，分布式是指不同的业务分布在不同的地方。而集群指的是将几台服务器集中在一起，实现同一业务。
分布式中每一个节点，都可以做集群。而集群不一定都是分布式的。

微服务和分布式之间的关系：
微服务是实现分布式系统的一种方式。微服务架构是一种云原生架构方法，它将单个应用拆分为众多松散耦合且可单独部署的小型组件或服务。
这些服务拥有自己的技术栈，通过REST API、事件流和消息代理进行通信，并且按照业务能力进行组织。通过微服务架构，可以将大型应用拆分为多个小服务，
每个服务都可以独立地部署、扩展和管理，从而实现分布式系统的特性。

微服务之间的调用可以是RPC（远程过程调用）：
RPC（Remote Procedure Call，远程过程调用）是一种用于在不同的计算机或进程之间进行通信和调用远程方法的协议和框架。
下面是一个简化的RPC框架调用的过程：

1. 定义服务接口：首先，需要定义远程服务的接口，包括方法名、参数和返回值等。这个接口描述了调用方可以通过RPC调用的方法。
2. 生成客户端和服务器端代码：使用RPC框架的工具，将定义的服务接口生成客户端和服务器端的代码，这些代码用于实现远程调用的逻辑。
3. 发起远程调用：在客户端代码中，可以通过调用生成的客户端接口方法来发起远程调用。参数将被序列化，并通过网络发送到服务器端。
4. 传输和通信：客户端将序列化的参数通过网络传输到服务器端，服务器端接收到请求后进行解析和处理。
5. 服务器端处理：服务器端代码会根据方法名和参数执行实际的逻辑，并返回相应的结果。
6. 结果返回：服务器端将处理得到的结果序列化后，通过网络返回给客户端。
7. 客户端处理：客户端接收到服务器端返回的结果，进行反序列化，并将结果返回给调用方。

需要注意的是，上述过程是一个简化的描述，实际的RPC框架通常还包括更多的细节处理，如连接管理、负载均衡、错误处理等。
常见的RPC框架有gRPC、Apache Dubbo、Thrift等，它们提供了丰富的功能和配置选项，可以满足不同的使用需求。
在选择和使用RPC框架时，需要考虑框架的性能、可靠性、跨语言支持等因素。


分布式系统的CAP理论：
Consistency：一致性，在分布式系统中，更新操作执行成功后所有的用户的读操作必须返回最新值；
client写入，server同步至整个系统；
Availability：可用性，只要收到用户的请求，在一定的时间内服务器就必须给出回应，回应的结果可以是成功或是失败；
Partition tolerance：分区容错，即区间通信可能失败，在网络中断，消息丢失的情况下，仍对外提供服务；
一般无法避免，可以认为CAP中的P总是成立

在一个分布式计算机系统中，一致性C，可用性A，分区容错性P这三种保证无法同时得到满足，最多满足两个：
1.放弃P
为了避免分区容错性问题的发生，一种做法是将所有与事务相关的数据都放在一台服务器上，虽然不能保证100%系统不会出错，
但是不会碰到由分区带来的负面效果，这样的做法会严重影响系统的扩展性。
2.放弃A
放弃可用性，一旦遇到分区容错故障，受到影响的服务器需要等待一定的时间，因此会导致在等待期间系统无法对外提供服务。
3.放弃C
这儿说的放弃一致性，并不是完全放弃数据的一致性，而是放弃数据的强一致性，而保留数据的最终一致性。
以网络购物为例，对只剩下一件库存的商品，如果同时接受到了两份订单，那么较晚的订单将被告知商品告罄。

根据CAP理论，一个分布式系统无法同时满足一致性、可用性和分区容错性。因此，在设计分布式系统时，需要根据实际需求进行权衡。

如何避免CAP理论的问题：

尽量减少分区的影响：通过优化网络架构和通信协议，降低通信中断的可能性，从而减少分区容错性的需求。
选择合适的CAP组合：根据实际需求，选择合适的CAP组合。例如，在强一致性和可用性之间进行权衡，或者在分区容错性和一致性之间进行权衡。
使用分布式事务：通过分布式事务来保证数据的一致性和可靠性。分布式事务可以将多个节点的操作绑定在一起，从而保证操作的原子性和一致性。
设计冗余机制：通过数据复制、备份等方式来提高系统的可用性和可靠性。当某个节点出现故障时，可以通过其他节点的数据来恢复系统的正常运行。
使用消息队列：消息队列可以保证数据的有序传输和可靠性，同时可以减轻节点的负载。通过消息队列，可以将数据从生产者节点传输到消费者节点，从而避免直接通信和阻塞问题。
优化数据结构：通过优化数据结构来提高系统的性能和可靠性。例如，使用分布式哈希表来避免单点故障和性能瓶颈问题。
考虑使用云服务：云服务提供商可以提供高可用、高可扩展的分布式系统解决方案。通过使用云服务，可以降低运维成本和提高系统的可靠性。

总之，在设计和实现分布式系统时，需要根据实际需求进行权衡和选择合适的方案。同时，需要不断优化和改进系统的性能和可靠性，以满足日益增长的业务需求。


分布式系统的通信方式：
分布式通信是指在分布式系统中，不同节点之间进行消息传递和交互的方式。
以下是常见的分布式通信方式：

消息队列（Message Queue）
使用消息队列作为中间件，节点之间通过发送和接收消息来实现通信。消息队列提供了异步、解耦和可靠性的通信机制，常见的消息队列系统包括RabbitMQ、Apache Kafka和ActiveMQ等。

消息队列通信原理：
消息队列是用于应用程序之间进行通信的中间件，通过将消息在不同的应用程序之间进行传递来实现应用程序之间的通信。
消息队列的应用层协议连接互联网的方式如下：

应用程序通过消息队列的API连接到消息队列服务器，例如RabbitMQ、ActiveMQ等。
应用程序向消息队列发送消息，消息队列将消息存储在本地数据库或内存中。
目标应用程序从消息队列中接收消息，并进行相应的处理。
消息队列服务器将消息传递给目标应用程序的过程可以通过不同的协议实现，例如AMQP协议、MQTT协议等。

总之，消息队列的应用层协议连接互联网的方式是通过应用程序与消息队列服务器之间的API通信来实现的，
而具体的实现方式则取决于使用的协议和应用场景。


实现方式
消息队列底层的实现可以有多种方式，具体取决于所使用的消息队列系统。以下是一些常见的底层实现方式：

1.基于内存：某些消息队列系统使用内存作为底层实现，以实现高吞吐量和低延迟。消息在内存中进行存储和传递，因此速度非常快。
然而，这种实现方式通常会面临数据持久性和可靠性的挑战，因为内存是易失性存储。

2.基于文件系统：一些消息队列系统使用文件系统作为底层实现。消息被写入到文件中，并通过文件进行传递。
这种实现方式可以提供较好的数据持久性，因为文件可以在系统重启后仍然存在。
然而，相对于内存实现，文件系统实现的吞吐量和延迟可能较低。

3.基于数据库：部分消息队列系统使用数据库作为底层实现。消息被存储在数据库表中，并通过数据库进行传递。
这种实现方式可以提供较好的数据持久性和可靠性，因为数据库通常具有事务支持和持久化机制。
但是，数据库的读写开销可能较大，从而影响性能。

4.基于网络协议：一些消息队列系统使用网络协议作为底层实现，如TCP/IP协议栈。
消息通过网络进行传输，可以在不同的计算节点之间进行通信。这种实现方式提供了跨网络的分布式消息传递，但可能受限于网络延迟和可靠性。

需要注意的是，不同的消息队列系统可能使用不同的底层实现方式，并且可以结合多种技术来实现不同的特性和性能。
例如，一些消息队列系统可能会将内存和文件系统结合使用，以在内存中提供高性能的消息传递，并在需要持久性时将消息写入文件系统。
选择适当的底层实现方式取决于应用程序的需求，包括性能、可靠性和持久性等方面的考虑。

优缺点及适用场景
优点：异步通信、解耦性高、支持可靠性消息传递、支持消息持久化、可扩展性好。
缺点：引入中间件，增加了系统复杂性；消息顺序性可能受到影响；需要额外的管理和维护。
适用场景：分布式系统解耦、异步任务处理、实时数据流处理、事件驱动架构。


RocketMQ:
RocketMQ作为一款纯java、分布式、队列模型的开源消息中间件，支持事务消息、顺序消息、批量消息、定时消息、消息回溯等。

RocketMQ 特点：
>支持发布/订阅（Pub/Sub）和点对点（P2P）消息模型
>在一个队列中可靠的先进先出（FIFO）和严格的顺序传递 （RocketMQ可以保证严格的消息顺序，而ActiveMQ无法保证）
>支持拉（pull）和推（push）两种消息模式
pull其实就是消费者主动从MQ中去拉消息，而push则像rabbit MQ一样，是MQ给消费者推送消息。但是RocketMQ的push其实是基于pull来实现的。
它会先由一个业务代码从MQ中pull消息，然后再由业务代码push给特定的应用/消费者。其实底层就是一个pull模式
>单一队列百万消息的堆积能力 （RocketMQ提供亿级消息的堆积能力，这不是重点，重点是堆积了亿级的消息后，依然保持写入低延迟）
>支持多种消息协议，如 JMS、MQTT 等
>分布式高可用的部署架构,满足至少一次消息传递语义（RocketMQ原生就是支持分布式的，而ActiveMQ原生存在单点性）
>提供 docker 镜像用于隔离测试和云集群部署
>提供配置、指标和监控等功能丰富的 Dashboard

Java创建RocketMQ生产者:
package com.wn.producer;

import com.alibaba.rocketmq.client.exception.MQClientException;
import com.alibaba.rocketmq.client.producer.DefaultMQProducer;
import com.alibaba.rocketmq.client.producer.SendResult;
import com.alibaba.rocketmq.common.message.Message;
public class MQProducer {
public static void main(String[] args) throws MQClientException {
DefaultMQProducer producer=new DefaultMQProducer("rmq-group");
producer.setNamesrvAddr("192.168.138.187:9876;192.168.138.188:9876");
producer.setInstanceName("producer");
producer.start();
try {

for (int i=0;i<10;i++){
Thread.sleep(1000); //每秒发送一次
Message msg = new Message("itmayiedu-topic", // topic 主题名称
"TagA", // tag 临时值
("itmayiedu-"+i).getBytes()// body 内容
);
SendResult sendResult=producer.send(msg);
System.out.println(sendResult.toString());
}

} catch (Exception e) {
e.printStackTrace();
}
producer.shutdown();
}
}



Java创建RocketMQ消费者:

package com.wn.consumer;

import com.alibaba.rocketmq.client.consumer.DefaultMQPushConsumer;
import com.alibaba.rocketmq.client.consumer.listener.ConsumeConcurrentlyContext;
import com.alibaba.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;
import com.alibaba.rocketmq.client.consumer.listener.MessageListenerConcurrently;
import com.alibaba.rocketmq.client.exception.MQClientException;
import com.alibaba.rocketmq.common.message.MessageExt;
import java.util.List;

public class MQConsumer {
public static void main(String[] args) throws MQClientException {
DefaultMQPushConsumer consumer=new DefaultMQPushConsumer("rmq-group");
consumer.setNamesrvAddr("192.168.138.187:9876;192.168.138.188:9876");
consumer.setInstanceName("consumer");
consumer.subscribe("itmayiedu-topic","TagA");
consumer.registerMessageListener(new MessageListenerConcurrently() {
@Override
public ConsumeConcurrentlyStatus consumeMessage(List list, ConsumeConcurrentlyContext consumeConcurrentlyContext) {
for (MessageExt msg:list){
System.out.println(msg.getMsgId()+"---"+new String(msg.getBody()));
}
return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
}
});

consumer.start();
System.out.println("Consumer Started...");
}
}

RabbitMQ:
RabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，
它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。

RabbitMQ特点：
>RabbitMQ是使用Erlang语言开发的开源消息队列系统，基于AMQP协议来实现。
>AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。
>AMQP协议更多用在企业系统内，对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。
>RabbitMQ的可靠性是非常好的，数据能够保证百分之百的不丢失。可以使用镜像队列，它的稳定性非常好。所以说在我们互联网的金融行业。对数据的稳定性和可靠性要求都非常高的情况下，我们都会选择RabbitMQ。当然没有kafka性能好，但是要比AvtiveMQ性能要好很多。也可以自己做一些性能的优化。
>RabbitMQ可以构建异地双活架构，包括每一个节点存储方式可以采用磁盘或者内存的方式。

RabbitMQ创建直连交换机：
import org.springframework.amqp.core.Binding;
import org.springframework.amqp.core.BindingBuilder;
import org.springframework.amqp.core.DirectExchange;
import org.springframework.amqp.core.Queue;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
 
@Configuration
public class DirectRabbitConfig {
 
    //队列 起名：TestDirectQueue
    @Bean
    public Queue TestDirectQueue() {
        // durable:是否持久化,默认是false,持久化队列：会被存储在磁盘上，当消息代理重启时仍然存在，暂存队列：当前连接有效
        // exclusive:默认也是false，只能被当前创建的连接使用，而且当连接关闭后队列即被删除。此参考优先级高于durable
        // autoDelete:是否自动删除，当没有生产者或者消费者使用此队列，该队列会自动删除。
        // return new Queue("TestDirectQueue",true,true,false);
 
        //一般设置一下队列的持久化就好,其余两个就是默认false
        return new Queue("TestDirectQueue",true);
    }
 
    //Direct交换机 起名：TestDirectExchange
    @Bean
    DirectExchange TestDirectExchange() {
      //  return new DirectExchange("TestDirectExchange",true,true);
        return new DirectExchange("TestDirectExchange",true,false);
    }
 
    //绑定  
    //将队列和交换机绑定, 并设置用于匹配键：TestDirectRouting
    @Bean
    Binding bindingDirect() {
        return BindingBuilder.bind(TestDirectQueue()).to(TestDirectExchange()).with("TestDirectRouting");
    }
     
    @Bean
    DirectExchange lonelyDirectExchange() {
        return new DirectExchange("lonelyDirectExchange");
    }
}

写个接口实现消息推送：
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.HashMap;
import java.util.Map;
import java.util.UUID;

@RestController
public class SendMessageController {

    //使用RabbitTemplate,这提供了接收/发送等等方法
    @Autowired
    RabbitTemplate rabbitTemplate;  
 
    @GetMapping("/sendDirectMessage")
    public String sendDirectMessage() {
        String messageId = String.valueOf(UUID.randomUUID());
        String messageData = "test message, hello!";
        String createTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"));
        Map<String,Object> map=new HashMap<>();
        map.put("messageId",messageId);
        map.put("messageData",messageData);
        map.put("createTime",createTime);
        //将消息携带绑定键值：TestDirectRouting 发送到交换机TestDirectExchange
        rabbitTemplate.convertAndSend("TestDirectExchange", "TestDirectRouting", map);
        return "ok";
    }
}


远程过程调用（Remote Procedure Call，RPC）
使用RPC技术进行分布式通信，允许节点之间像调用本地函数一样进行远程调用。RPC提供了一种透明的通信方式，
常见的RPC框架包括gRPC、Apache Thrift和Spring Cloud等。

优缺点及适用场景
优点：简化分布式通信过程；透明的远程调用；支持多种编程语言；高性能。
缺点：服务依赖性高；通信过程对网络要求较高；对系统的管理和维护较为复杂。
适用场景：微服务架构、跨语言通信、高性能计算、分布式计算。

RESTful API
使用基于HTTP协议的RESTful API进行通信，节点之间通过HTTP请求和响应进行数据交换。RESTful API提供了一种简单和标准化的通信方式，常用于构建分布式Web服务和微服务架构。

优缺点及适用场景
优点：简单易用、基于标准的HTTP协议、良好的可扩展性、松耦合。
缺点：同步通信方式、无状态性可能导致性能问题；数据传输较为冗余。
适用场景：Web服务、前后端分离架构、微服务架构。

分布式共享内存（Distributed Shared Memory，DSM）
通过共享内存的方式实现分布式通信，允许多个节点共享内存空间，从而实现数据共享和交互。DSM提供了高性能的通信方式，常见的DSM系统包括Apache Ignite和Hazelcast等。

优缺点及适用场景
优点：高性能的数据共享和交互；简化编程模型；可扩展性好。
缺点：一致性和同步性问题；需要复杂的内存管理和同步机制；依赖于底层网络。
适用场景：分布式缓存、分布式计算、分布式数据库。

网络套接字（Socket）
使用底层的网络套接字进行直接的节点间通信，可以基于TCP或UDP协议实现点对点的数据传输。网络套接字提供了最底层的通信方式，常用于构建自定义的分布式通信协议和框架。

优缺点及适用场景
优点：最底层的通信方式；灵活性高；适用于定制化通信需求。
缺点：需要自行处理通信协议和数据格式；可靠性和一致性问题需要自行解决。
适用场景：定制化分布式通信协议、低级别的网络通信、特定的性能优化需求。

总结
需要根据具体的系统要求、性能需求、可靠性需求、开发团队技术栈和经验等因素，综合考虑选择合适的分布式通信方式。
有时候，系统可能会结合多种方式来满足不同的需求，例如使用消息队列进行异步通信，结合RESTful API进行同步交互。


分布式中间件：
中间件技术是计算机系统中的一个软件层，位于客户端和服务端之间，负责处理两者之间的通信和数据交换。
它可以通过转换格式、路由消息、管理连接等方式支持多种应用场景，并具有提高系统可靠性、安全性、扩展性等优势。
常见的中间件技术包括消息队列、缓存、负载均衡、API网关、数据库代理和文件存储代理等。
它们各具特点，可以应用于分布式系统、微服务架构、大数据系统等不同场景。

中间件技术包括：
消息队列：如RabbitMQ、ActiveMQ、Kafka等。
缓存：如Redis、Memcached等。
负载均衡：如Nginx、HAProxy、F5等。
API网关：如Kong、Zuul、Tyk等。
数据库代理：如MySQL Proxy、PgBouncer等。
文件存储代理：如Ceph、GlusterFS等。



缓存中间件：Redis
Redis是一个使用C语言开发的数据库，与数据库不同，Redis的数据库存在内存中，它是内存数据库。正因为如此他的读写速度非常快，因此Redis作为nosql被广泛应用于缓存方向。
Redis除了做缓存之外，Redis也经常用来做分布式锁，甚至是消息队列。
Redis提供了多种数据类型来支持不同的业务场景。Redis还支持事务、持久化、Lua脚本、多种集群方案。

优点：
读写性能极高
支持数据持久化
支持事务
数据结构丰富
支持主从复制
丰富的特性

缺点：
数据库容量收到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。
主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。

redis可以在分布式系统中当缓存，也可以在分布式系统中用来做分布式锁。
redis用作分布式锁：为了解决分布式系统中的缓存击穿问题，需要分布式锁。
String uuid = UUID.randomUUID().toString();
lock = redisTemplate.opsForValue().setIfAbsent("lock","111"，300，TimeOut.SECONDS); //加锁并设置过期时间，保证了原子性
if(lock){
//加锁成功... 执行业务
Map<String，List<Catelog2Vo>> dataFromDb = getDataFromDb();
//删除锁脚本
String script = "if redis.ca11("get",KEYS[1]) == ARGV[1]then
return redis.ca11("de1",KEYS[1])else return 0 end";
//删除锁
Integer lock1 = redisTemplate.execute(new DefaultRedisScript<INTEGER>(script,Integer.class)
										,Array.asList("lock"),uuid);


return getDataFromDb();
}else {
//加锁失败...重试。
synchronized ()
return getCatalogJsonFromDbwithRedisLock();//自旋的方式
}

可以使用redisson完成分布式锁功能：

一、创建redisson工具类
@Bean(destroyMethod="shutdown")
public RedissonClient redisson() throws IOException {
//1、创建配置
//Redis url should start with redis:// or rediss://
Config config = new Config();
config.useSingleServer().setAddress("redis://192.168,5610:6379");
//2、根据Config创建出RedissonClient示例RedissonClient redissonClient = Redisson.create(config);
return redissonClient;
}

二、redisson创建分布式锁
@ResponseBody
@GetMapping("/he1lo")
public String hello(){

//1、获取一把锁，只要锁的名字一样，就是同一把锁
RLock lock = redisson .getLock( name: "my-lock");

//2、加锁
lock.lock(10,TimeUnit.SECONDS);//10秒自动解锁,自动解锁时间一定要大于业务的执行时间。
//Lock.Lock(10,TimeUnit.SECONDS); 在锁时间到了以后，不会自动续期。
//1、如果我们传递了锁的超时时间，就发送给redis执行脚本，进行占锁，默认超时就是我们指定的时间
//2、如果我们未指定锁的超时时间，就使用30 * 1000[LockWatchdogTimeout看门狗的默认时间];只要占锁成功，
就会启动一个定时任务[重新给锁设置过期时间，新的过期时间就是看门狗的默认时间],每隔10s都会自动续期
//internalLockLeaseTime[看门狗时间] / 3,10s
//1)、锁的自动续期，如果业务超长，运行期间自动给锁续上新的305。不用担心业务时间长，锁自动过期被删掉
//2)加锁的业务只要运行完成，就不会给当前锁续期，即使不手动解锁，锁默认在305以后自动删除。
try{
System.out.println("加锁成功，执行业务..."+Thread.currentThread().getId());
Thread.sleep( millis: 300e0);
}catch (Exception e){

}finally {

//3、解锁 将设解锁代码没有运行，redisson会不会出现死锁
System.out.println("释放锁..."+Thread.currentThread().getId());lock.unlock();
return "hello";
}

Spring缓存：SpringCache
Spring Cache利用了AOP，实现了基于注解的缓存功能，
并且进行了合理的抽象，业务代码不用关心底层是使用了什么缓存框架，只需要简单地加一个注解，就能实现缓存功能了。
@Cacheable({"category"})
public List<CategoryEntity> getLevel1Categorys() {
	......
}

SpringCache自定义缓存：
指定生成的缓存使用的key， key属性指定，接受一个SpEL
@Cacheable(value = {"category"},key = "#root.method.name")
public List<CategoryEntity> getLevel1Categorys() {
    return this.baseMapper.selectList(
            new QueryWrapper<CategoryEntity>().eq("parent_cid",0));
}

将数据保存为json格式，自定义RedisCacheConfiguration即可
@Configuration
@EnableCaching
public class MyCacheConfig {
    @Bean
    RedisCacheConfiguration redisCacheConfiguration(CacheProperties cacheProperties){
        RedisCacheConfiguration config  = RedisCacheConfiguration.defaultCacheConfig();
        //修改key和value的序列化机制
        config = config .serializeKeysWith(
                RedisSerializationContext.SerializationPair.fromSerializer(
                        new StringRedisSerializer()));
        config = config.serializeValuesWith(
                RedisSerializationContext.SerializationPair.fromSerializer(
                        new GenericJackson2JsonRedisSerializer()));

        CacheProperties.Redis redisProperties = cacheProperties.getRedis();
        //将配置文件中的所有配置都生效
        if (redisProperties.getTimeToLive() != null) {
            config = config.entryTtl(redisProperties.getTimeToLive());
        }
        if (redisProperties.getKeyPrefix() != null) {
            config = config.prefixKeysWith(redisProperties.getKeyPrefix());
        }
        if (!redisProperties.isCacheNullValues()) {
            config = config.disableCachingNullValues();
        }
        if (!redisProperties.isUseKeyPrefix()) {
            config = config.disableKeyPrefix();
        }
        return config;
    }
}


数据库中间件Mycat：
从定义和分类来看，它是一个开源的分布式数据库系统，Mycat是一个实现了MySQL协议的服务器，
前端用户可以把它看作是一个数据库代理，用MySQL客户端工具和命令行访问，
而其后端可以用MySQL原生协议与多个MySQL服务器通信，也可以用JDBC协议与大多数主流数据库服务器通信，
其核心功能是分表分库，即将一个大表水平分割为N个小表，存储在后端MySQL服务器里或者其他数据库里。

为什么要使用Mycat?
Java与数据库的紧耦合
高访问量高并发对数据库的压力
读写请求数据不一致



分布式文件系统：
分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点相连。
它允许程序像访问本地文件一样访问或存储独立的文件，允许程序员从任何网络或计算机访问文件。
通过DFS，可以以受控和授权的方式在网络上的用户之间轻松地共享信息和文件。
DFS的主要目的是允许物理分布式系统的用户通过使用公共文件系统（CFS）来共享他们的数据和资源。

以下是几个分布式文件系统的例子：

1.NFS 。网络文件系统是早期的分布式文件系统，主要解决如何在网络环境中共享和管理文件系统的问题。
2.AFS 。AFS 是卡内基梅隆大学开发的分布式文件系统，特点是在客户端上缓存了服务器的部分数据，从而提高了访问速度。
3.Coda 。Coda是一个可扩展的分布式文件系统，由卡内基梅隆大学开发，它通过分发和冗余技术来提高系统的可用性和健壮性。
4.InterPlanetary File System (IPFS)。 IPFS 是一个基于内容寻址的分布式文件系统，
它通过哈希函数将文件的元数据（如名称、内容等）编码成一个唯一的地址，从而可以在全球范围内进行文件的查找和访问。


分布式微服务框架：
Spring Cloud：
Spring Cloud是一个为构建分布式系统的微服务框架。
准确的说，它不是一个框架，而是一个大的容器，将各个微服务通过接口调用，从而简化了开发者的代码量。
Spring Cloud和普通RPC有很大区别，Spring Cloud使用的是http协议的RPC框架，传统RPC框架使用内部调用的方式。

Spring Cloud常用组件：
SpringCloud的五大核心组件为Eureka、Fegin、Ribbon、Hystrix、Zull

1.Eureka（注册中心）2024.1（Eureka已弃用）
SpringCloud提供了多种注册中心的支持，这其中就包括Eureka、ZooKeeper等，我们平时常用的也是这两种
Eureka主要就是用来注册服务的，其中包括两部分：Eureka Client、Eureka Server
Eureka Client：包含服务提供者、服务消费者，主要负责服务注册、心跳续约与健康状况查询
Eureka Server：提供注册服务，各个节点启动后，都会在Eureka Server中注册，可用的节点信息可以在Eureka Server中的服务注册表中找到（Eureka Server之间通过复制的方式来完成数据的同步）
应用启动后，Eureka Client会向Eureka Server发送心跳，一般默认周期为30秒，如果Eureka在多个心跳周期内（一般为90秒）没有接受到某个节点的心跳，Eureka就会进入自我保护机制


其他注册中心：

Alibaba Nacos (dynamic naming and configuration Service):
Alibaba Nacos是一个开源的分布式系统服务发现、配置和管理平台，它致力于帮助开发者更敏捷和容易地构建、交付和管理微服务平台。
Alibaba Nacos是一个功能强大且易于使用的分布式系统服务发现、配置和管理平台。它可以帮助开发者更快速地构建、交付和管理微服务平台，提高系统的可用性和稳定性，降低维护成本。

Nacos特点：
服务发现与注册
Nacos提供了强大的服务发现与注册功能。在微服务架构中，服务提供者可以将自己的服务注册到Nacos中，而服务消费者则可以通过Nacos发现并获取所需服务的地址信息。
这使得服务之间的调用变得更为灵活和便捷，同时也为服务的动态扩展和负载均衡提供了支持。

配置管理
Nacos还提供了动态配置管理功能。开发者可以将应用的配置信息存储在Nacos中，通过Nacos进行统一的配置管理和分发。
这使得配置信息的修改和发布变得更为方便，同时也支持实时生效，无需重启应用。此外，Nacos还支持配置信息的版本控制，方便开发者追踪和回滚配置变更。

健康检查
Nacos提供了对服务的实时健康检查功能。它可以监控服务的运行状态，及时发现并隔离故障节点，保证服务的稳定性和可用性。这对于构建高可用的微服务架构至关重要。

集群管理
Nacos还支持集群管理功能，可以管理多个应用节点，包括它们的服务注册和发现、配置管理等。这使得在分布式环境下管理和维护服务变得更为简单和高效。

可视化界面
Nacos提供了一个简洁易用的UI控制台，开发者可以通过这个界面方便地管理所有的服务和应用的配置。这使得操作更为直观和便捷，提高了开发效率。


将Spring Cloud Nacos作为注册中心：
配置Nacos
在application.properties或application.yml文件中，配置Nacos服务器的地址和端口。例如：

application.yml配置：

spring:  
  cloud:  
    nacos:  
      discovery:  
        server-addr: 127.0.0.1:8848 # Nacos服务器地址  
        namespace: public # 命名空间，默认为public  
        cluster-name: DEFAULT # 集群名称，默认为DEFAULT
启用Nacos Discovery
在你的Spring Boot主类或配置类上，添加@EnableDiscoveryClient注解以启用服务发现。

@SpringBootApplication  
@EnableDiscoveryClient  
public class YourApplication {  
  
    public static void main(String[] args) {  
        SpringApplication.run(YourApplication.class, args);  
    }  
}


现在，当你启动你的Spring Boot应用时，它会自动注册到Nacos服务器。你可以通过Nacos的Web控制台查看已注册的服务列表。



将Spring Cloud Nacos作为配置中心：
配置bootstrap.properties或bootstrap.yml
在Spring Cloud中，配置中心相关的配置通常放在bootstrap.properties或bootstrap.yml文件中，因为这些文件比application.properties或application.yml更早被加载。

bootstrap.yml的配置可以如下所示：

spring:  
  cloud:  
    nacos:  
      config:  
        server-addr: 127.0.0.1:8848 # Nacos服务器地址  
        namespace: public # 命名空间，默认为public  
        data-id: your-data-id # 数据ID，用于唯一标识配置  
        group: DEFAULT_GROUP # 分组，默认为DEFAULT_GROUP  
        file-extension: yaml # 配置文件的格式，如yaml、properties等
其中，data-id是你在Nacos中创建的配置数据的唯一标识，group是分组，file-extension是配置文件的格式。

在Nacos中创建配置
登录Nacos的Web控制台，在配置管理菜单中创建或编辑配置。你需要设置数据ID、分组和配置格式，并在配置内容中输入你的配置信息。

启动你的应用
现在，当你启动你的Spring Boot应用时，它会从Nacos配置中心加载配置信息。你可以通过@Value或@ConfigurationProperties等注解在应用中注入这些配置。
配置配置刷新注解@RefreshScope



Nacos配置中心的命名空间：
在Nacos中，命名空间是一个用于实现环境隔离的概念。每个命名空间都代表一个独立的环境，例如开发环境、测试环境和生产环境等。通过使用命名空间，我们可以将不同环境的配置和服务进行隔离，避免相互干扰。

作用：
隔离环境：在实际开发中，我们通常会遇到多个环境，如开发、测试和生产等。这些环境在配置和服务上可能存在差异。通过使用命名空间，我们可以将不同环境的配置和服务进行隔离，确保它们之间不会相互影响。
这有助于我们在不同的环境中进行独立的开发和测试工作。

分类管理：在大型系统中，我们可能会有多个模块或者团队共同开发和维护。通过使用命名空间，我们可以将不同模块或者团队的配置和服务进行分类管理，方便团队协作和维护。
每个命名空间都可以有自己的配置和服务注册表，使得不同模块或团队之间的配置和服务信息更加清晰和易于管理。

创建一个命名空间：
spring:  
  cloud:  
    nacos:  
      discovery:  
        server-addr: 127.0.0.1:8848 # Nacos服务地址  
        namespace: your-namespace-id # 命名空间ID  
      config:  
        server-addr: 127.0.0.1:8848 # Nacos服务地址  
        namespace: your-namespace-id # 命名空间ID


Nacos配置中心的配置分组：
配置分组是Nacos中的一个功能，允许用户将不同的配置划分到不同的组中，以实现更灵活的配置管理和控制。

作用：
分类管理：在实际开发中，我们通常会有多个配置文件，如数据库配置、缓存配置、日志配置等。通过使用配置分组，我们可以将不同类型的配置文件进行分类管理，使得配置更加清晰、易于维护。
环境隔离：不同的环境（如开发环境、测试环境、生产环境）可能需要不同的配置。通过使用配置分组，我们可以为不同的环境创建不同的配置组，并在启动服务时指定要使用的配置组，从而实现环境隔离。

创建一个配置分组：
1.填写配置信息：
2.在“Data ID”字段中输入你的配置的唯一标识符。
3.在“Group”字段中输入你想要创建的新分组的名称。例如，你可以输入“myNewGroup”。
4.选择配置格式（如YAML、Properties等）。
5.在“配置内容”字段中输入你的配置信息。


2.Feign
Feign是一个HTTP请求的轻量级客户端框架。
通过 接口+注解 的方式发起HTTP请求的调用，而不是像Java中通过封装HTTP请求报文的方式直接调用。

Feign执行流程：
在主程序上添加@EnableFeignClients注解开启对已经加@FeignClient注解的接口进行扫描加载。
调用接口中的方法时，基于JDK动态代理的方式，通过InvokeHandler调用处理器分发远程调用，生成具体的RequestTemplate
RequestTemplate生成Request请求，结合Ribbon实现服务调用负载均衡策略
Feign最核心的就是动态代理，同时整合了Ribbon和Hystrix，具备负载均衡、隔离、熔断和降级功能

1.添加依赖
2.在主程序上添加注解@EnableFeignClients
@SpringBootApplication
@EnableFeignClients
public class Microservice {
    public static void main(String[] args) {
        SpringApplication.run(Microservice.class, args);
    }
}

3.在需要调用其他微服务的模块中，定义一个继承自FeignClient接口的自定义接口

//其中的@FeignClient注解指定了需要调用的远程服务名，@RequestMapping注解指定了远程服务的接口映射路径。
@FeignClient(name = "remote-service")
public interface RemoteServiceClient {

    @RequestMapping(value = "/api/remote-service/hello", method = RequestMethod.GET)
    public String hello();
}

4.注入并使用Feign接口
import org.springframework.beans.factory.annotation.Autowired;  
import org.springframework.stereotype.Service;  
  
@Service  
public class MyService {  
  
    private final RemoteServiceClient remoteServiceClient;  
  
    @Autowired  
    public MyService(RemoteServiceClient remoteServiceClient) {  
        this.remoteServiceClient = remoteServiceClient;  
    }  
  
    public void doSomething() {  
        String response = remoteServiceClient.hello();  
        // 处理响应...  
    }  
}

注意：
1.在默认情况下，OpenFeign的连接超时时间是10秒，而读取超时时间是60秒。这些超时时间的设置可以在源码feign.Request.Options#Options()方法中找到。
2.OpenFeign 的重试机制允许在发生网络故障或服务端错误时，自动重新发起请求。OpenFeign 的重试机制基于 Retryer 接口实现，该接口定义了重试的策略。默认情况下，OpenFeign 可能不会启用重试机制，需要开发者手动开启并配置。

2.1创建一个自定义的Retryer类
import feign.Retryer;  
import java.util.concurrent.atomic.AtomicInteger;  
  
public class CustomRetryer extends Retryer.Default {  
  
    private final int maxAttempts;  
    private final long period;  
    private final TimeUnit unit;  
  
    public CustomRetryer(int maxAttempts, long period, TimeUnit unit) {  
        super();  
        this.maxAttempts = maxAttempts;  
        this.period = period;  
        this.unit = unit;  
    }  
  
    @Override  
    public void continueOrPropagate(RetryableException e) {  
        AtomicInteger attempt = this.attempt.get();  
        if (attempt.get() < maxAttempts) {  
            try {  
                unit.sleep(period);  
                attempt.incrementAndGet();  
            } catch (InterruptedException ignored) {  
                Thread.currentThread().interrupt();  
                throw new RuntimeException(e);  
            }  
            continueWith(e);  
        } else {  
            throw e;  
        }  
    }  
}

2.2 配置 Feign 客户端使用自定义 Retryer
import feign.Retryer;  
import org.springframework.context.annotation.Bean;  
import org.springframework.context.annotation.Configuration;  
  
@Configuration  
public class FeignConfig {  
  
    @Bean  
    public Retryer feignRetryer() {  
        return new CustomRetryer(3, 1000, TimeUnit.MILLISECONDS); // 重试3次，每次间隔1秒  
    }  
  
    @Bean  
    public MyFeignClient myFeignClient(Decoder decoder, Encoder encoder, Client client, Retryer retryer) {  
        return Feign.builder()  
                .encoder(encoder)  
                .decoder(decoder)  
                .client(client)  
                .retryer(retryer) // 使用自定义的 Retryer  
                .target(MyFeignClient.class, "http://example.com");  
    }  
}



3.Ribbon（负载均衡）2024.1（Ribbon已弃用）
Ribbon是一个客户端的负载均衡器，他提供对大量的HTTP和TCP客户端的访问控制
Ribbon负载均衡策略：简单轮询、权重、随机、重试等多种策略

//编写Ribbon配置类
@Configuration
public class ApplicationContextConfig {
    @Bean
    @LoadBalanced//使用@LoadBalanced注解赋予RestTemplate负载均衡的能力
    public RestTemplate restTemplate(){
        return new RestTemplate();
    }
}

//编写Ribbon自定义配置类，配置负载均衡策略
@Configuration
public class MySelfRule {
    @Bean //lRule：根据特定算法中从服务列表中选取一个要访问的服务
    public IRule myRule(){
        return new RandomRule();//随机访问
    }
}

其他负载均衡方案：
LoadBalancer：
Spring Cloud中的LoadBalancer是一个关键的组件，它负责在多个服务实例之间实现负载均衡，以确保系统的高可用性和可伸缩性。
这个负载均衡器是客户端的负载均衡，意味着每个发起服务调用的客户端都存有完整的目标服务地址列表，根据配置的负载均衡策略，由客户端自己决定向哪台服务器发起调用。
LoadBalancer的主要作用是根据负载规则从服务发现组件（如Nacos）获取的可用服务节点列表中选取服务调用目标的地址。
它提供了一种在多个服务的实例之间均衡负载的方式，使得系统可以更加高效、稳定地处理请求。
LoadBalancer支持多种负载均衡算法，包括随机算法、轮询算法、权重算法、最少请求算法等。这些算法可以根据不同的需求和场景进行选择，以实现最佳的负载均衡效果。
在Spring Cloud中，LoadBalancer的配置相对灵活，可以通过配置文件或编程方式进行定制。
此外，Spring Cloud LoadBalancer还提供了智能路由功能，可以根据服务的质量、性能等指标进行路由决策，进一步提高系统的性能和可用性。


创建一个LoadBalancer负载均衡，使用轮询算法：
1.配置Nacos服务注册中心的地址等信息。在application.yml或application.properties中添加如下配置：
spring:  
  cloud:  
    nacos:  
      discovery:  
        server-addr: 127.0.0.1:8848 # Nacos服务注册中心的地址

2.创建一个服务调用者（Client），该调用者会使用LoadBalancer和Nacos来选择合适的服务实例进行调用。

import org.springframework.beans.factory.annotation.Autowired;  
import org.springframework.cloud.client.ServiceInstance;  
import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;  
import org.springframework.stereotype.Service;  
import org.springframework.web.client.RestTemplate;  
  
import java.util.List;  
  
@Service  
public class MyServiceClient {  
  
    @Autowired  
    private LoadBalancerClient loadBalancerClient;  
  
    @Autowired  
    private RestTemplate restTemplate;  
  
    public String callMyService() {  
        // 获取服务名，这通常是在配置文件中定义的，例如application.yml或application.properties  
        String serviceName = "my-service";  
  
        // 获取服务实例列表  
        List<ServiceInstance> serviceInstances = loadBalancerClient.getInstances(serviceName);  
  
        if (serviceInstances.isEmpty()) {  
            throw new RuntimeException("No instances available for service: " + serviceName);  
        }  
  
        // 使用轮询算法选择一个服务实例  
        ServiceInstance serviceInstance = serviceInstances.get(serviceInstances.indexOf(loadBalancerClient.choose(serviceName)));  
  
        // 构造请求的URL  
        String url = String.format("http://%s:%d/my-endpoint", serviceInstance.getHost(), serviceInstance.getPort());  
  
        // 发起调用  
        String response = restTemplate.getForObject(url, String.class);  
  
        // 返回调用结果  
        return response;  
    }  
}


负载均衡算法有哪些？
负载均衡算法有多种，它们各自具有不同的特点和适用场景。以下是一些常见的负载均衡算法：

轮询法（Round Robin）：将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。
加权轮询法（Weighted Round Robin）：与轮询法类似，但考虑了后端服务器的不同性能和负载情况，为每台服务器分配不同的权重，权重高的服务器将处理更多的请求。
随机法（Random）：通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。
加权随机法（Weighted Random）：类似于加权轮询法，但它是按照权重随机请求后端服务器，而非顺序。
最小连接数法（Least Connections）：这是一种动态调度算法，通过服务器中当前所活跃的连接数来估计服务器的负载情况，把新的连接请求分配到当前连接数最小的服务器。
加权最小连接数法（Weighted Least Connections）：此算法考虑服务器的处理性能和当前的连接数，将请求分配到具有较少活动连接且权值较高的服务器上。
源地址哈希法（Source Address Hashing）：根据获取客户端的IP地址，通过哈希函数计算得到一个数值，用这个数值对服务器列表的大小进行取模运算，得到的结果便是客户端要访问的服务器序号。

此外，还有一些其他的负载均衡算法，如最短响应时间法（Least Response Time），它根据服务器响应请求的时间长短来选择服务器，响应时间最短的服务器将被优先选中。


阐述一下客户端负载均衡和服务端负载均衡有什么区别：
客户端负载均衡和服务端负载均衡在多个方面存在显著的区别。

首先，从负载均衡的实现位置来看，客户端负载均衡是在客户端和服务器之间增加一个负载均衡设备，这个设备负责将客户端的请求分发到多个服务器上。
而服务端负载均衡则是通过部署在服务器端的负载均衡器来管理和分发请求。

其次，从负载均衡设备的选择和配置来看，客户端负载均衡设备可以是路由器、交换机、负载均衡器等，需要在客户端和服务器之间增加这些设备，并对其进行相应的配置和管理。
而服务端负载均衡则依赖于服务器端的负载均衡器，如Nginx、HAProxy等，这些负载均衡器能够集中管理负载均衡策略，实现各种负载均衡算法。

再次，从负载均衡的效果和优势来看，客户端负载均衡具有提高响应速度、提高带宽利用率和提高数据处理能力的优点。由于客户端负载均衡可以将请求直接分发到最快的服务器上，减少了网络拥塞，因此可以提高响应速度。
同时，通过将网络流量分担到多个服务器上，客户端负载均衡也可以提高带宽利用率和数据处理能力。然而，客户端负载均衡也存在一些缺点，如增加了网络延迟和复杂度，以及无法处理动态内容。

相比之下，服务端负载均衡具有统一管理和集中控制的优势。通过集中管理负载均衡器，可以方便地对服务实例进行监控和管理，并在负载均衡器上实现各种负载均衡算法。
然而，服务端负载均衡也存在一些潜在的问题，如单点故障和额外开销。如果负载均衡器本身发生故障，整个系统的可用性可能会受到影响。同时，需要额外的负载均衡器设备也会增加系统的复杂性和成本。

综上所述，客户端负载均衡和服务端负载均衡在实现位置、设备配置、效果优势等方面存在明显的区别。选择哪种负载均衡方式取决于具体的业务需求和网络环境。


4.Hystrix（服务熔断）2024.1（Hystrix已弃用）
Hystrix提供两个命令，分别是HystrixCommand、HystrixObservableCommand，通常是在Spring中通过注解和AOP来实现对象的构造
熔断：简单来说，就是我们生活中的“保险丝”。如果熔断器打开，就会执行短路，直接进行降级；如果熔断器关闭，就会进入隔离逻辑。
默认触发熔断器的条件为：最近一个时间窗口（默认为10秒），总请求次数大于等于circuitBreakerRequestVolume Threshold（默认为20次）
并且异常请求比例大于circuitBreakerError ThresholdPercentage（默认为50%），此时熔断器触发
隔离：一般分为两种：线程池隔离和信号量隔离。线程池隔离指的是每个服务对应一个线程池，线程池满了就会降级；
信号量隔离是基于tomcat线程池来控制的，当线程达到某个百分比，走降级流程
降级：作为Hystrix中兜底的策略，当出现业务异常、线程池已满、执行超时等情况，调用fallback方法返回一个错误提示

//在application.properties文件配置Hystrix
hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds=5000
hystrix.command.default.execution.isolation.strategy=THREAD

//创建Hystrix服务
@Service
public class MyService {
    @Autowired
    private RestTemplate restTemplate;

    @HystrixCommand(fallbackMethod = "fallbackMethod")
    public String doSomething() {
        // 这里调用远程服务
        return restTemplate.getForObject("http://example.com/api/something", String.class);
    }

    public String fallbackMethod() {
        // 这里是熔断后的fallback方法，返回一个默认值或者处理异常的逻辑
        return "fallback";
    }
}

其他服务熔断：

Resilience4j：
Resilience4j是一个轻量级的容错库，专为Java 8和函数式编程而设计，旨在帮助开发人员构建弹性和可靠的Java应用程序。它提供了诸如断路器、限流、重试和超时等功能，以保护应用程序免受故障和异常的影响。
其核心概念是“函数式编程”，允许开发人员使用Java 8的函数式编程特性来定义和组合容错策略，这使得容错逻辑更加灵活、可读性更高，并且易于测试和维护。

Resilience4j的熔断降级：
当某个服务或组件出现故障或响应时间过长时，系统不再继续调用该服务或组件，而是快速失败，并执行一些降级逻辑，以确保系统的整体稳定性和可用性。这种机制可以避免由于某个服务或组件的故障导致的整个系统的崩溃。

实现步骤：
1.添加依赖：
首先，你需要在项目的pom.xml文件中添加Resilience4j的熔断器依赖。例如：
<dependency>  
    <groupId>org.springframework.cloud</groupId>  
    <artifactId>spring-cloud-starter-circuitbreaker-resilience4j</artifactId>  
</dependency>

2.修改接口代码，对外暴露接口还需要配置在OpenFeign上
然后，你需要在需要熔断降级的服务接口上使用@CircuitBreaker注解。这个注解会告诉Resilience4j在该接口调用失败率达到指定阈值时触发熔断机制。例如：

@Restcontroller
public class OrderCircuitcontroller{

    @Resource
    private PayFeignApi payFeignApi;

    @GetMapping(value=v"/feign/pay/circuit/{id}")
    @CircuitBreaker(name ="cloud-payment-service", fallbackMethod = "mycircuitFallback") //服务熔断的注解，name为注册的微服务，fallbackMethod为熔断的兜底处理方法
    public string mycircuitBreaker(@PathVariable("id")Integer id){
        return payFeignApi.myCircuit(id);
    }

    //myCircuitFallback就是服务降级后的兜底处理方法
    public string myCircuitFallback(Integer id,Throwable t){
        //这里是容错处理逻辑。返回各用结果
        return"mycircuitFal1back，系统繁忙，请稍后再试-----/(ToT)/~~"
    };

}


3. 配置熔断策略：
resilience4j:  
  circuitbreaker:  
    instances:  
      myService:  
        failureRateThreshold:50#设置50%的调用失败时打开断路器，超过失败请求百分比circuitBreaker 变为OPEN状态。
        slidingWindowType:COUNT BASED #滑动窗的类型
        slidingWindowsize:6 #滑动窗口的大小配置 COUNT_BASED表示6个请求，配置TIME_BASED表示6秒
        minimumNumberofcalls:6 #断路器计算失败率或慢调用率之前所需的最小调用数(每个滑动窗口周期)。最少调用几次，才能计算失败率。
        automaticTransitionFromopenToHalfopenEnabled:true # 是否启用自动从开启状态过波到半开状态，默认值为true。
        waitDurationInopenstate:5s#从OPEN到HALF OPEN状态需要等待的时间
        permittedNumberofcallsInHalfopenstate:2 #半开状态允许的最大请求数，默认值为10。在半开状态下,CircuitBreaker将允许最多permittedNumberOfCallsInHalfOpenstate个请求通过，如果有一个请求失败，CircuitBreaker将重新进入开启状态


Resilience4j的信号量舱壁熔断隔离：
信号量舱壁隔离是一种轻量级的隔离方法，它主要通过限制同时访问某一资源或服务的并发请求数来实现隔离。
这种方法不需要额外的线程或线程池，因此资源消耗相对较小。当请求到达最大并发限制时，新的请求会被快速拒绝，而不是排队等待，这有助于迅速释放系统资源。
这种隔离方式适用于那些不涉及远程调用或耗时操作的轻量级任务，以及对线程资源敏感的场景。

实现步骤：
1.添加依赖：
首先，你需要在项目的pom.xml文件中添加Resilience4j的舱壁隔离依赖。例如：
<dependency>  
    <groupId>io.github.resilience4j</groupId>  
    <artifactId>resilience4j-bulkhead</artifactId>  
</dependency>


2.修改接口代码，对外暴露接口还需要配置在OpenFeign上
然后，你需要在需要熔断降级的服务接口上使用@CircuitBreaker注解。这个注解会告诉Resilience4j在该接口调用失败率达到指定阈值时触发熔断机制。例如：

@Restcontroller
public class OrderCircuitcontroller{

@GetMapping(value ="/feign/pay/bulkhead/{id}")
@Bulkhead(name ="cloud-payment-service",fallbackMethod = "myBulkheadFallback",type = Bulkhead.Type.SEMAPHORE)  //SEMAPHORE信号量，THREADPOOL线程池
//信号量舱壁隔离更注重于共享资源的访问控制，而线程池隔离则通过为每个任务分配独立线程来实现任务之间的隔离

public string myBulkhead(@PathVariable("id")Integer id){
    return payFeignApi.myBulkhead(id);
}
public string myBulkheadFallback(Throwable t){
    return"myBulkheadFal1back，隔板超出最大数量限制，系统繁忙，请稍后再试-----/(ToT)/~~";
}

}

3.配置隔离策略
resilience4j:
    bulkhead:
        configs:
            default:
                maxConcurrentcalls:2 #隔离允许并发线程执行的最大数量
                maxwaitDuration:1s   #当达到并发调用数量时，新的线程的阻塞时间，我只愿意等待1秒，过时不候进舱壁兜底faLlback
        instances:
            cloud-payment-service:
                baseConfig:default
    timelimiter:
        configs:
            default:
                timeout-duration:20s


Resilience4j的限流器：
Resilience4j的限流器会设定一个固定时间段（如每秒、每分钟或每小时）内允许的最大请求数。当有请求到达时，限流器会检查当前时间段内已经处理的请求数，并与设定的最大请求数进行比较。
如果当前请求数尚未达到最大限制，限流器会允许请求继续处理；如果已经达到或超过最大限制，限流器则会根据配置的策略对请求进行处理，如拒绝请求、返回错误信息或将其放入队列等待处理。

实现步骤：
1.添加依赖（略）
2.修改接口代码，对外暴露接口还需要配置在OpenFeign上
@Restcontroller
public class OrderCircuitcontroller{

@GetMapping(value ="/feign/pay/retelimit/{id}")
@Bulkhead(name ="cloud-payment-service",fallbackMethod = "myRelateLimitFallBack")  
public string myBulkhead(@PathVariable("id")Integer id){
    return payFeignApi.myRelateLimit(id);
}

public string myRelateLimitFallBack(Throwable t){
    return"你被限流了，禁止访问-----/(ToT)/~~";
}

}

3.配置限流策略
resilience4j:
    bulkhead:
        configs:
            default:
                limitForPeriod:2#在次侧新周期内，允许执行的最人请求数
                timeout-duration:1#线程等待权跟的默认等待时间
                limitRefreshperiod:1s # 从流器每厢limitRefreshperiod则新一次，将允许处理的最大诗求数量质置为limitForperiod 
        instances:
            cloud-payment-service:
                baseConfig:default


5.MicroMeter（分布式链路追踪）
分布式链路追踪（Distributed System Tracing，简称DST）是一种用于追踪和分析分布式系统中请求调用链路的技术。
在分布式系统中，一个请求可能经过多个服务节点，涉及多个网络调用，因此追踪整个请求的处理过程对于排查问题、优化性能以及理解系统行为至关重要。

链路追踪原理：
>跟踪请求流转：当一个请求在分布式系统中被发起时，链路追踪系统会开始跟踪这个请求的流转路径。它会记录请求从起点开始，经过哪些服务节点，以及在这些节点上的处理情况。
>引入Span概念：为了表达请求在分布式系统中的父子关系，链路追踪系统引入了Span的概念。每个Span代表了一个操作或一段工作单元，比如一个远程过程调用或者一段本地代码的执行。
Span之间通过父子关系连接，形成了一个完整的调用链。
>收集与传输数据：在请求流转的过程中，链路追踪系统会收集每个Span的信息，包括起始时间、结束时间、服务节点信息、调用状态等。这些信息会被发送到后端的分析系统或存储系统中。
>数据分析与可视化：后端的分析系统会对收集到的Span数据进行处理和分析，生成调用链的拓扑图，展示各个服务节点的调用关系、耗时以及请求状态等。
同时，它还可以提供性能可视化的功能，让用户可以直观地看到系统的运行情况。
>故障定位与性能优化：通过链路追踪系统生成的调用链和性能数据，用户可以快速地定位到故障发生的节点，分析性能瓶颈，从而进行相应的优化。

MicroMeter监控链路实现步骤：
1.添加依赖（略）
2.修改接口代码，对外暴露接口还需要配置在OpenFeign上
@Restcontroller
public class OrderMicroMetercontroller{

    @Resource
    private PayFeignApi payFeignApi；

    @GetMapping(value ="/feign/microMeter/{id}")
    public string myMicroMeter(@PathVariable("id")Integer id){
        return payFeignApi.myMicroMeter(id);
    }

}

3.配置yaml
management:
    zipkin:
        tracing:
            endpoint:http://localhost:9411/api/v2/spans
    tracing:
        sampling:
            probability:1.0 #采样率默认为0.1(0.1就是10次只能有一次被记录下来)，值越大收集越及时。

6.GateWay（网关）
Spring Cloud Gateway是一个由WebFlux、Netty和Reactor实现的响应式API网关，旨在为微服务架构提供简单且有效的API路由管理方式。以下是Spring Cloud Gateway的三大核心概念：

路由：路由是Spring Cloud Gateway的基本构建组成，表示一个具体的路由信息载体。它负责将客户端的请求转发到正确的服务实例。
Spring Cloud Gateway支持动态路由和灵活的路由策略，使得路由的配置和管理变得非常便捷。

过滤器：过滤器是Spring Cloud Gateway中的另一个核心概念，它可以在请求和响应的处理过程中添加自定义的过滤逻辑。这些过滤逻辑可以包括身份验证、请求日志记录、限流、熔断等各种功能。
通过过滤器，我们可以对进入和离开网关的请求和响应进行精细化的控制和处理。

服务发现与集成：Spring Cloud Gateway能够与Spring Cloud的服务注册与发现组件进行集成，自动发现并路由到可用的服务实例。
这使得网关能够动态地感知后端服务的状态，并根据需要调整路由规则，从而保证了服务的高可用性和稳定性。



配置网关路由：
server:
    port: 9527

spring:
    application:
        name:cloud-gateway#以微股务注册进consul或nacos股务列表内
    cloud:
        consul:#配置consul地址
        host:localhost
        port:8500
        discovery:
            prefer-ip-address:true
            service-name:${spring.application.name}
        gateway:
            routes:
                -id:pay routh1 #pay routh1 #路由的ID(类似mysql主键ID)，没有固定规则但要求唯一，建议配合服务名#匹配后提供服务的路由地址
                uri:http://localhost:8001  
                predicates:
                    Path=/pay/gateway/get/** # 断言，路径相匹配的进行路由 

                -id: pay routh2 #pay routh2 # 路由的ID(类似mysql主键ID)，没有固定规则但要求唯一，建议配合服务名#匹配后提供服务的路由地址
                uri: http://localhost:8001
                predicates:
                    Path=/pay/gateway/info/** # 断言、路径相匹配的进行路由

GateWay主流用法：以服务名动态获取服务URI
修改配置 uri: lb://cloud-payment-service #微服务的名字


另一种分布式管理的工具：
Dubbo：
Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的RPC实现服务的输出和输入功能，可以和Spring框架无缝集成。
它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。

Dubbo架构：
Provider: 暴露服务的服务提供方。
Consumer: 调用远程服务的服务消费方。
Registry: 服务注册与发现的注册中心。
Monitor: 统计服务的调用次数和调用时间的监控中心。

调用流程
0.服务容器负责启动，加载，运行服务提供者。
1.服务提供者在启动时，向注册中心注册自己提供的服务。
2.服务消费者在启动时，向注册中心订阅自己所需的服务。
3.注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
4.服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
5.服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心

Dubbo注册中心
对于服务提供方，它需要发布服务，而且由于应用系统的复杂性，服务的数量、类型也不断膨胀；
对于服务消费方，它最关心如何获取到它所需要的服务，而面对复杂的应用系统，需要管理大量的服务调用。
而且，对于服务提供方和服务消费方来说，他们还有可能兼具这两种角色，即既需要提供服务，有需要消费服务。

通过将服务统一管理起来，可以有效地优化内部应用对服务发布/使用的流程和管理。服务注册中心可以通过特定协议来完成服务对外的统一。
Dubbo提供的注册中心有如下几种类型可供选择：
Multicast注册中心
Zookeeper注册中心
Redis注册中心
Simple注册中心

Dubbo优缺点
优点：
透明化的远程方法调用
- 像调用本地方法一样调用远程方法；只需简单配置，没有任何API侵入。
软负载均衡及容错机制
可在内网替代nginx lvs等硬件负载均衡器。
服务注册中心自动注册 & 配置管理
-不需要写死服务提供者地址，注册中心基于接口名自动查询提供者ip。
使用类似zookeeper等分布式协调服务作为服务注册中心，可以将绝大部分项目配置移入zookeeper集群。
服务接口监控与治理
-Dubbo-admin与Dubbo-monitor提供了完善的服务接口管理与监控功能，针对不同应用的不同接口，可以进行 多版本，多协议，多注册中心管理。

缺点：
只支持JAVA语言

分布式应用程序协调服务：
什么是注册中心？
注册中心是微服务架构中的纽带，类似于“通讯录”，它记录了服务和服务地址的映射关系。
在分布式架构中，服务会注册到这里，当服务需要调用其它服务时，就到这里找到服务的地址并进行调用。
注册中心本质上是为了解耦服务提供者和服务消费者。对于任何一个微服务，原则上都应存在或者支持多个提供者，这是由微服务的分布式属性决定的，
更进一步，为了支持弹性扩缩容特性，一个微服务的提供者的数量和分布往往是动态变化的，也是无法预先确定的。
因此，原本在单体应用阶段常用的静态LB机制就不再适用了，需要引入额外的组件来管理微服务提供者的注册与发现，而这个组件就是服务注册中心。

注册中心的核心功能：
服务注册：服务实例将自身服务信息注册到注册中心
服务发现：服务实例通过注册中心，获取到注册到其中的服务实例的信息，通过这些信息去请求它们提供的服务
服务剔除：服务注册中心将出问题的服务自动剔除到可用列表之外，使其不会被调用到


Zookeeper：注册中心
eureka：注册中心
nacos: 注册中心
Ribbon：负载均衡组件
Hystrix：断路器
Zuul：网关服务组件
SpringCloud Config：配置中心

zookeeper和eureka有什么区别？
1：zookeeper保证cp原则（一致性）而Eureka保证的是ap原则（可用性）
2：zookeeper在选举期间注册服务瘫痪不可用，而Eureka各个节点平等，只要一台就能保证服务可以，但查询到的数据不一定是最新的，可以很好的应对网络故障导致的部分节点失联
3：zookeeper有header和follower角色（当header挂掉，会从剩下的follower里面选举一个header），Eureka各个节点平等
4：zookeeper采用半数存活原则（避免脑裂），Eureka采用自我保护机制来解决分区问题
5：kafka就是使用的zookeeper作为注册中心，理论上Eureka更适合作为注册中心

分布式服务跟踪ELK：
ELK 是elastic公司提供的一套完整的日志收集、展示解决方案，是三个产品的首字母缩写，分别是ElasticSearch、Logstash 和 Kibana：

ElasticSearch简称ES，由Java 语言编写，它是一个建立在全文搜索引擎Apache Lucene基础上的、实时的、分布式的搜索和分析引擎，
它可以用于全文搜索，结构化搜索以及分析；
Logstash是一个具有实时传输能力的数据收集引擎，用来进行数据收集（如：读取文本文件）、解析，并将数据发送给ES；
Kibana为Elasticsearch提供了分析和可视化的Web平台。
它可以在Elasticsearch的索引中查找，交互数据，并生成各种维度表格、图形；
在ELK中beats是用于数据采集的工具，相较于logstash，beats是更轻量级的工具。Beats 平台集合了多种单一用途数据采集器，这些采集器安装后可用作轻量型代理，从成百上千或成千上万台机器向 Logstash 或 Elasticsearch 发送数据。

1、elasticSearch
是一个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。
2、logstash
主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式（支持以TCP/UDP/HTTP多种方式收集数据）。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作，再一并发往elasticSearch上去。
3、kibana
是一个开源工具，kibana可以为logstash和elasticSearch提供的日志分析友好的web界面，可以帮助汇总、分析和搜索重要数据日志。
4、filebeat
filebeat隶属于beats。目前beats包含四种工具：
（1）packetbeat（搜索网络流量数据）
（2）topbeat（搜索系统、进程和文件系统级别的CPU和内存使用情况等数据）
（3）filebeat（搜集文件数据）
（4）winlogbeat（搜集windows事件日志数据）



安全：
登录-安全框架Spring security：
Spring Security是一个能够为基于Spring的企业应用系统提供声明式的安全访问控制解决方案的安全框架。
它提供了一组可以在Spring应用上下文中配置的Bean，充分利用了Spring IoC，DI（控制反转Inversion of Control ,DI:Dependency Injection 依赖注入）和AOP（面向切面编程）功能，为应用系统提供声明式的安全访问控制功能，减少了为企业系统安全控制编写大量重复代码的工作。

Spring Security一般流程为：
1.当用户登录时，前端将用户输入的用户名、密码信息传输到后台，后台用一个类对象将其封装起来，通常使用的是UsernamePasswordAuthenticationToken这个类。
2.程序负责验证这个类对象。验证方法是调用Service根据username从数据库中取用户信息到实体类的实例中，比较两者的密码，如果密码正确就成功登陆，
同时把包含着用户的用户名、密码、所具有的权限等信息的类对象放到SecurityContextHolder（安全上下文容器，类似Session）中去。
3.用户访问一个资源的时候，首先判断是否是受限资源。如果是的话还要判断当前是否未登录，没有的话就跳到登录页面。
如果用户已经登录，访问一个受限资源的时候，程序要根据url去数据库中取出该资源所对应的所有可以访问的角色，然后拿着当前用户的所有角色一一对比，判断用户是否可以访问（这里就是和权限相关）。

创建一个配置类，配置Spring Security以启用JWT认证。您需要配置JWT的过滤器以验证和解析令牌。以下是一个示例配置：
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;
import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;

@Configuration
@EnableWebSecurity
public class SecurityConfig extends WebSecurityConfigurerAdapter {
    
    @Override
    protected void configure(HttpSecurity http) throws Exception {
        http.csrf().disable()
            .authorizeRequests()
            .antMatchers("/api/login").permitAll() // 允许登录端点
            .anyRequest().authenticated();
        http.addFilterBefore(jwtAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class);
    }
    
    @Bean
    public JwtAuthenticationFilter jwtAuthenticationFilter() {
        return new JwtAuthenticationFilter();
    }
}

创建登录端点：
创建一个登录端点，用户可以通过该端点进行身份验证，并生成JWT令牌。
@RestController
@RequestMapping("/api")
public class AuthenticationController {
    @Autowired
    private AuthenticationManager authenticationManager;

    @Autowired
    private JwtTokenUtil jwtTokenUtil;

    @PostMapping("/login")
    public ResponseEntity<?> createAuthenticationToken(@RequestBody AuthenticationRequest request) {
        // 在这里验证用户身份，并生成JWT令牌
    }
}
这些步骤只是整合Spring Security和JWT的基本步骤。您还需要根据您的项目需求进一步定制和实现这些部分。
确保配置Spring Security和JWT的过滤器以进行身份验证，生成和验证JWT令牌，并提供登录端点用于用户登录。


Shiro：
Shiro是一个Java安全框架，它提供了身份验证、授权、密码和会话管理等安全特性。Shiro可以帮助开发者快速构建安全的应用程序，包括Web应用、移动应用和企业级应用。

使用Shiro进行登录身份验证：

1.创建Shiro的认证类
package com.yourcompany.yourapp.yourpackage;  
  
import org.apache.shiro.authc.*;  
import org.apache.shiro.realm.AuthenticatingRealm;  
import org.apache.shiro.subject.PrincipalCollection;  
import java.util.*;  
  
public class CustomAuthenticationRealm extends AuthenticatingRealm {  
    @Override  
    protected AuthenticationInfo doGetAuthenticationInfo(PrincipalCollection pc) throws AuthenticationException {  
        String username = pc.getPrimaryPrincipal().toString();   
        User user = new User(username, "password"); // 创建用户对象，这里只是简单示例，实际中应该从数据库或其他地方获取用户信息。  
        if (user.authenticate(username, "password")) {   
            return new SimpleAuthenticationInfo(user, user.getPassword(), getName());   
        } else {   
            throw new UsernameNotFoundException("User not found");   
        }   
    }   
}

2.使用Shiro进行安全验证
@RestController  
public class MyController {  
    @RequestMapping("/secure")  
    public String secure() {  
        Subject currentUser = SecurityUtils.getSubject();   
        currentUser.checkPermission("secure:view"); // 检查用户是否有查看这个页面的权限。如果没有，会抛出AuthorizationDeniedException异常。  
        return "This is a secure page.";  
    }  
}

使用Shiro进行密码管理：



1. 配置Shiro Realm：
创建一个自定义的Shiro Realm，这个Realm负责处理认证和授权的逻辑，包括密码的验证和存储。
扩展 AuthorizingRealm 类，并实现 doGetAuthenticationInfo 方法来处理密码验证。

   public class CustomRealm extends AuthorizingRealm {
       // 实现认证逻辑
       @Override
       protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {
           UsernamePasswordToken upToken = (UsernamePasswordToken) token;
           String username = upToken.getUsername();
           // 获取用户的密码（通常从数据库中查询）
           String storedPassword = getPasswordFromDatabase(username);

           if (storedPassword == null) {
               throw new UnknownAccountException("User not found.");
           }

           return new SimpleAuthenticationInfo(username, storedPassword, getName());
       }

       // 从数据库获取用户的密码（模拟）
       private String getPasswordFromDatabase(String username) {
           // 这里可以连接数据库查询用户密码
           // 通常需要使用加密的密码存储方式，例如BCrypt
           return "hashed_password"; // 替换为实际的密码
       }
   }

2. 存储密码安全：
在实际应用中，开发者应该存储用户密码的哈希值而不是明文密码。
Shiro本身不提供密码哈希功能，所以你可以使用其他库如BCrypt或PBKDF2来安全地存储密码。以下是一个示例使用BCrypt的方法：

import org.mindrot.jbcrypt.BCrypt;

// 生成并存储BCrypt哈希密码
String plainPassword = "password";
String hashedPassword = BCrypt.hashpw(plainPassword, BCrypt.gensalt());
// 存储hashedPassword到数据库


3. 配置Shiro SecurityManager：
   在Shiro配置中，创建 SecurityManager 并将自定义的Realm添加到 SecurityManager 中。

DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
securityManager.setRealm(yourCustomRealm);
SecurityUtils.setSecurityManager(securityManager);
   
   
4. 使用Shiro验证密码：
   在用户登录时，使用Shiro来验证用户提供的密码。Shiro会自动从Realm中获取用户的密码进行验证。

UsernamePasswordToken token = new UsernamePasswordToken(username, password);
try {
    SecurityUtils.getSubject().login(token);
    // 登录成功
} catch (AuthenticationException e) {
    // 登录失败
}
   

这是一个基本的使用Shiro进行密码管理的示例。在实际应用中，开发者需要根据具体需求来配置Shiro的Realm、密码哈希算法、密码策略等。
确保密码存储和验证的安全性，以保护用户的凭据。

Java在处理敏感数据的时候如何实现数据的安全性？

使用加密算法：Java提供了多种加密算法，如AES、DES、RSA等，可以对敏感数据进行加密，保护数据不被非法获取。
使用安全协议：Java提供了SSL/TLS协议，可以用于实现加密通信，保证数据在传输过程中的安全性。
使用安全容器：Java提供了安全容器，如Java KeyStore、Java CredentialStore等，可以用于存储密钥和凭据，保护敏感数据不被非法访问。
权限控制：Java可以通过权限控制机制，如Java安全管理器（Security Manager），对敏感数据进行访问控制，防止非法访问和操作。
数据备份：对于重要的敏感数据，应该定期备份，并存储到安全的地方，以防止数据丢失或损坏。

Java安全编码指南：
安全编码指南
安全编码指南为开发人员提供了一组在编写代码时应遵循的建议,以最大程度地减少安全漏洞并减轻常见威胁。

1、输入验证和输出编码
>验证和清理所有用户输入,以防止SQL注入和跨站,点脚本(XSS)等攻击。
>使用参数化查询或预准备语句来防止SQL注入。
>显示用户生成的内容时应用适当的输出编码或转义技术以防止XSS攻击。
>使用强大的数据类型和长度验证来确保输入数据符合预期的格式。

2、认证与授权
>实施强大且安全的身份验证机制,例如多重身份验证(MFA)或强密码策略。
>使用安全密码存储技术(例如盐散列)来保护用户凭据。
>实施适当的授权检查,确保只有授权用户才能访问敏感资源原或执行特权操作。
>实施最小权限原则,仅向每个用户或角色授予必要的权限。

3、安全会话管理
>生成强大的随机会话标识符并安全地处理它们,例如使用仅HTTP和安全cookie。
>实施会话超时和适当的会话终止机制。
>通过在身份验证时生成新的会话标识符来避免会话固定漏洞。
>通过加密或使用服务器端而不是客户端存储来保护敏感会话数据。

4、安全的错误处理和日志记录
>避免在错误消息中暴露可能为攻击者提供帮助的敏感信息。
>实施适当的错误处理机制以防止信息泄露。
>安全地记录与安全相关的事件和异常,避免日志中敏感数据的暴露。
>定期检查和监控应用程序日志是否存在安全事件或异常活动。

5、安全通讯
>使用安全通信协议(例如HTTPS)来保护传输中的数据。
>实施适当的证书验证并避免使用自签名或过期的证书。
>在传输和存储过程中对密码或个人信息等敏感数据进行加密。
>避免通过不安全的渠道(例如电子邮件或未加密的HTTP)传输敏感数据。

6、安全文件和资源处理
>验证和清理文件上传,以防止任意文件执行或目录遍历攻击。
>避免将敏感信息存储在明文或不安全的存储位置。
>设置适当的文件和目录权限以限制仅授权用户的访问。
>谨防不安全的文件操作,例如文件删除或重命名,这可能会导致安全漏洞。


常用的密码加密方式：
哈希算法加密：
哈希算法是一种常用于加密和数据完整性验证的技术。它将任意长度的输入数据映射为固定长度的输出，通常是一个固定长度的哈希值。这个过程是单向的，即从哈希值无法还原出原始数据。因此，哈希算法通常用于存储密码、验证数据完整性等场景。

以下是一些哈希算法加密的关键特性和应用：

1. 单向性（One-way）：哈希函数是单向的，无法通过哈希值还原出原始数据。这使得哈希算法特别适用于密码存储，因为即使哈希值被盗，攻击者也很难还原出原始密码。
2. 固定长度输出：不论输入数据的长度如何，哈希算法的输出长度是固定的。这使得哈希值在存储和比较时更加方便。
3. 唯一性：不同的输入数据应产生唯一的哈希值，即不同的输入不能得到相同的输出。这有助于确保数据的唯一性和完整性。
4. 快速计算：哈希算法需要在合理的时间内计算出哈希值，以便在实际应用中能够高效地使用。
5. 碰撞概率低：碰撞是指两个不同的输入产生相同的哈希值。好的哈希算法应该具有极低的碰撞概率，这有助于保证数据的完整性。
6. 不可逆性：由于哈希是单向的，理论上不应该存在能够逆向计算原始数据的方法。然而，由于哈希函数输出的是固定长度的值，存在哈希碰撞的概率，即不同的输入可能得到相同的哈希值，因此一定程度上是可逆的。

常见的哈希算法包括MD5、SHA-1、SHA-256等。然而，由于计算能力的提升和一些算法漏洞的发现，一些哈希算法已经被认为不再安全，推荐使用更安全的算法，如SHA-256。
在密码存储方面，推荐使用带有盐（salt）的哈希算法，以提高安全性。盐是一个随机值，与密码结合使用，使得相同的密码在不同用户之间产生不同的哈希值。


对称加密：
对称加密是一种加密方法，它使用相同的密钥（称为密钥）来进行加密和解密数据。在对称加密中，发送方和接收方必须共享相同的密钥，这样他们就能够互相加密和解密彼此传递的信息。
对称加密的速度通常很快，因为它只涉及一个密钥，但它也面临一些安全性方面的挑战。

以下是对称加密的一些关键特点和用途：

1. 相同的密钥用于加密和解密：对称加密使用相同的密钥来加密和解密数据。这就要求在通信双方之间安全地交换和保存密钥，以确保没有未授权的第三方能够访问它。
2. 速度快：由于只涉及一个密钥，对称加密通常比非对称加密（如公钥加密）更快。这使得它适用于大量数据的加密和解密操作。
3. 适用于加密通信：对称加密常用于保护网络通信，如TLS/SSL协议中使用的对称加密算法。它也可以用于加密存储在计算机或服务器上的数据。
4. 密钥管理挑战：共享密钥的安全性对对称加密至关重要。在实践中，确保密钥在通信双方之间的安全交换和存储是一个挑战。如果密钥泄漏，可能导致数据的不安全性。
5. 不提供身份验证：对称加密本身不提供身份验证机制。这意味着如果有人截获了密钥并冒充通信的一方，他们可以轻松地解密和访问通信内容。

常见的对称加密算法包括DES（Data Encryption Standard）、3DES、AES（Advanced Encryption Standard）等。AES目前是最常用的对称加密算法之一，由于其高度的安全性和性能表现，广泛应用于安全通信和数据加密领域。


非对称加密：
非对称加密，也称为公钥加密，是一种使用一对密钥来进行加密和解密的加密技术。这一对密钥包括公钥和私钥，它们是数学相关的，但在使用上有着不同的功能。公钥可以公开分享，而私钥必须保持机密。

以下是非对称加密的一些关键特点和用途：

1. 一对密钥： 非对称加密使用一对密钥，分别是公钥和私钥。公钥用于加密数据，而私钥用于解密数据。由于这两个密钥是数学上相关的，通过公钥加密的数据只能通过私钥解密，反之亦然。
2. 加密和数字签名：公钥用于加密敏感信息，私钥用于解密。这使得非对称加密适用于安全地传递密钥或加密小块数据。此外，私钥还可以用于数字签名，以验证数据的来源和完整性。
3. 身份验证：公钥可以用于验证消息的来源。如果消息能够成功解密，说明它是由持有相应私钥的合法用户发送的。这提供了一种身份验证机制。
4. 密钥交换：非对称加密常用于安全地交换对称密钥。两方可以使用对方的公钥来加密一个对称密钥，然后将其传输给对方，对方再使用私钥解密得到对称密钥。接下来的通信可以使用对称密钥进行加密，提高了效率。
5. 安全性高：非对称加密提供了更高的安全性，因为即使攻击者获得了公钥，也无法通过公钥反向计算出私钥。这与对称加密不同，对称加密中同一密钥用于加密和解密，因此一旦密钥泄漏，整个系统的安全性就会受到威胁。

常见的非对称加密算法包括RSA（Rivest–Shamir–Adleman）、DSA（Digital Signature Algorithm）、ECC（Elliptic Curve Cryptography）等。RSA是其中最为广泛应用的非对称加密算法之一。


基于证书的加密：
基于证书的加密是一种使用数字证书来实现身份验证和数据加密的加密方法。数字证书是一种由可信的第三方机构颁发的电子文档，用于确认公钥的拥有者身份。基于证书的加密通常与非对称加密技术相结合，以确保通信的安全性和可信度。

以下是基于证书的加密的关键特点和过程：

1. 数字证书：数字证书是包含公钥及其拥有者身份信息的电子文档。证书由证书颁发机构（CA）签名，CA是一个受信任的第三方机构。数字证书通常包含以下信息：
   - 公钥
   - 拥有者的身份信息，如名称、电子邮件等
   - 证书颁发者的数字签名
   - 证书的有效期限

2. 证书颁发机构（CA）：CA是负责验证申请者身份并签署数字证书的机构。CA的数字签名用于验证证书的真实性。常见的CA包括Verisign、Let's Encrypt等。
3. 身份验证：当通信的一方希望与另一方进行安全通信时，它们可以交换数字证书。通过验证证书的签名和有效期，通信双方可以确保对方的身份是合法的。
4. 密钥交换：一旦身份得到验证，通信的双方可以使用证书中包含的公钥来进行密钥交换。通常，此阶段可以使用非对称加密技术，例如RSA。
5. 对称加密：一旦密钥交换完成，通信的双方可以使用共享的对称密钥来进行数据加密和解密。对称加密相对较快，因此在保护大量数据的传输中，对称密钥通常用于实际的数据加密。
6. 加密通信：现在，通信双方可以使用共享的对称密钥加密和解密其通信内容，确保数据的机密性和完整性。

基于证书的加密提供了一种安全且可信的通信方式，因为它结合了数字证书的身份验证和非对称加密的安全性。这种方法常用于安全套接字层（SSL）和传输层安全性（TLS）协议中，以保护网站和网络通信。

搜索引擎：
搜索引擎的原理：

抓取页面：每个独立的搜索引擎都有自己的网页抓取程序（蜘蛛）。爬虫Spider顺着网页中的链接，从一个链接到另一个链接，从一个网站到另一个网站。通过链接连续访问更多的页面，被抓取的页面称之为网页快照。
由于互联网中的链接应用很普遍，理论上讲，从一定的网页出发就能搜索绝大多数的网页。

处理页面：搜索引擎抓取页面后，还要做大量的预处理工作才能提供检索服务。其中最重要的就是提取关键词，建立索引库和索引，其他还包括去除重复的页面和蜘蛛认为没有意义的页面。然后判断网页类型和分析链接，计算网页的重要度、丰富度等。

提供检索服务：用户利用搜索引擎的搜索框输入想要查询的关键词，搜索引擎在自己的索引库中找到匹配关键词的所有页面。为了便于用户的判断，除了网页标题和URL之外，还会提供一段来自网页的摘要以及其他信息。

分布式搜索引擎Elasticsearch：
Elasticsearch 是一个分布式、高扩展、高实时的搜索与数据分析引擎。它能很方便的使大量数据具有搜索、分析和探索的能力。
Elasticsearch 的实现原理主要分为以下几个步骤，首先用户将数据提交到Elasticsearch 数据库中，再通过分词控制器去将对应的语句分词，
将其权重和分词结果一并存入数据，当用户搜索数据时候，再根据权重将结果排名，打分，再将返回结果呈现给用户。

1.首先要存储数据，必须要建立索引。在es中索引实际仅仅是一个逻辑上的命名空间，这个命名空间指向的是一个或多个的分片shard，其中分片的出先可以实现水平扩容，副本分片的出现作用于高可用。
2.一个分片是一个仅仅保存了索引中所有数据中一部分分片数据的低级别的工作单元
3.一个分片就是一个Lucene实例，对于他本身而言就是一个完整的查询搜索引擎。
4.我们的文档是被存储索引在分片中的，但是客户端并不直接和分片交互，而是和index索引进行交互
5.众多分片是ES集群分布式存储数据的方式，可以简单把分片当作存储数据的容器。文档存储在分片中，分片被分配到集群中的节点Node中。
6.当集群扩容、缩容时，es能够自动的对集群节点中分片上的数据进行迁移，最终仍能保证数据均衡分布
7.分片分为主分片和副本分片
8.每个分片从技术实现上来讲，最大可以索引Integer.MAX_VALUE-128个文档；实际上存储的上限取决于你的硬件资源、文档结构的复杂度等

primary shard 主分片
每个文档都只会存储在一个主分片中，不会分散存储在多个主分片中，并且每个分片所存储的容量是有上限的，因此我们配置的每个索引的主分片数量，就决定了该索引最大的容量。主分片的数量是在创建索引时就确定的，不能再修改

replica shard 副本分片
副本分片是主分片的一个copy，有两个目的一是：解决单点问题，用于冗余数据，应对硬件故障，实现高可用；
二是：大流量支撑，每个副本都可以提供查询、检索请求。副本数量在任何时刻都可以变更。
每个主分片的副本分片，不会被和他的主分片分配在同一个Node节点上，因为在同一个节点上的主、副本分配上没有任何意义的，一旦该Node挂掉，这个Node节点上的主、副本副片都会丢失。

在Elasticsearch中，建立索引的过程是通过定义一个映射（mapping）来完成的。映射定义了索引的结构，包括字段的类型、分析器、索引选项等。


Elasticsearch中的索引有什么特点？
在Elasticsearch中，索引具有以下特点：

数据组织：索引是相关文档的集合，每个文档都有自己的特定结构和字段。索引可以根据不同的需求和使用场景进行创建和设计，以便更好地组织和检索数据。
分片和复制：索引中的数据在物理上被分为多个分片，每个分片可以在不同的节点上存储和处理。这种分片使得索引数据可以在集群中分布式存储和查询。复制机制则确保了数据的冗余备份，提高了系统的可靠性和容错性。
多种索引类型：Elasticsearch支持多种索引类型，包括文本索引、数值索引和时间索引等。这些索引类型适用于不同类型的数据和查询需求，可以根据实际情况进行选择和设置。
动态映射：Elasticsearch中的索引可以动态地映射文档类型和字段，根据需要自动确定字段的类型和属性。这使得索引的创建和使用更加灵活和方便。
近实时搜索：Elasticsearch的搜索功能是近实时的，意味着在索引中添加或更新文档后，几乎可以立即进行搜索和查询。这为实时分析和响应提供了便利。
查询功能强大：Elasticsearch提供了强大的查询功能，包括布尔查询、范围查询、匹配查询等。用户可以通过简单的查询语句或复杂的查询语法来搜索和过滤数据。
可扩展性：Elasticsearch是一个分布式搜索引擎，可以轻松地扩展到多个节点和服务器上。这使得它能够处理大规模的数据集和高并发请求。
灵活的配置选项：Elasticsearch提供了许多配置选项，可以根据需求进行灵活的配置和调整，以满足不同的使用场景和性能要求。

总之，Elasticsearch中的索引具有数据组织、分片和复制、多种索引类型、动态映射、近实时搜索、查询功能强大、可扩展性和灵活的配置选项等特点，这些特点使得Elasticsearch成为一个强大、灵活和可靠的搜索引擎。


Elasticsearch中如何建立索引？
创建一个索引：使用PUT请求和index端点来创建一个新的索引。例如，要创建一个名为my_index的索引，可以使用以下命令：
PUT /my_index

定义映射：在创建索引时，需要定义一个映射来指定索引的结构。映射定义了每个字段的类型、分析器和其他索引选项。可以使用PUT请求和mapping端点来定义映射。例如，以下命令定义了一个名为my_index的索引的映射：
PUT /my_index/_mapping  
{  
  "properties": {  
    "title": {  
      "type": "text"  
    },  
    "content": {  
      "type": "text",  
      "analyzer": "standard"  
    }  
  }  
}
在上面的示例中，我们定义了两个字段：title和content。title字段的类型为text，而content字段的类型也为text，并指定了分析器为standard。

添加文档：一旦建立了索引和映射，就可以向索引中添加文档了。文档是包含要索引的数据的JSON对象。可以使用POST请求和_doc端点来添加文档。例如，以下命令将一个文档添加到名为my_index的索引中：
POST /my_index/_doc/1  
{  
  "title": "Elasticsearch",  
  "content": "Elasticsearch is a distributed search and analytics engine."  
}
在上面的示例中，我们添加了一个包含title和content字段的文档，并指定了文档的ID为1。

搜索文档：一旦文档被添加到索引中，就可以使用Elasticsearch的查询API来搜索文档了。可以使用各种查询类型和查询语句来搜索文档，例如布尔查询、范围查询、匹配查询等。
可以使用GET请求和_search端点来执行搜索操作。例如，以下命令搜索名为my_index的索引中的所有文档：
GET /my_index/_search  
{  
  "query": {  
    "match_all": {}  
  }  
}  
在上面的示例中，我们执行了一个匹配所有文档的查询。查询结果将返回匹配的所有文档。



分布式版本控制系统Git：
Git是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。

从一般开发者的角度来看，git有以下功能：
1、从服务器上克隆完整的Git仓库（包括代码和版本信息）到单机上。(指令 git clone)
2、在自己的机器上根据不同的开发目的，创建分支，修改代码。	(指令 git branch xxx )
3、在单机上自己创建的分支上提交代码。	(指令 git add .)
4、在单机上合并分支。 (指令 git merge xxx(分支名字))
5、把服务器上最新版的代码fetch下来，然后跟自己的主分支合并。
6、生成补丁（patch），把补丁发送给主开发者。
7、看主开发者的反馈，如果主开发者发现两个一般开发者之间有冲突（他们之间可以合作解决的冲突），就会要求他们先解决冲突，然后再由其中一个人提交。如果主开发者可以自己解决，或者没有冲突，就通过。
8、一般开发者之间解决冲突的方法，开发者之间可以使用pull 命令解决冲突，解决完冲突之后再向主开发者提交补丁。

从主开发者的角度（假设主开发者不用开发代码）看，git有以下功能：
1、查看邮件或者通过其它方式查看一般开发者的提交状态。
2、打上补丁，解决冲突（可以自己解决，也可以要求开发者之间解决以后再重新提交，如果是开源项目，还要决定哪些补丁有用，哪些不用）。
3、向公共服务器提交结果，然后通知所有开发人员。

.git文件包含哪些信息？
1.  branches ：这个目录包含分支的相关信息。
2.  hooks ：在此目录中，您可以定义Git挂钩脚本，这些脚本可以在特定的Git操作（如提交或合并）发生时触发。
3.  info ：这个目录包含一些全局信息，如排除文件的规则。
4.  objects ：Git存储数据的核心目录，包括所有提交、树和文件对象。
它有三个子目录： objects/commit 、 objects/tree  和  objects/blob ，分别用于存储提交、树和文件对象。
5.  refs ：此目录包含指向各种Git引用（如分支和标签）的文件，这些引用用于跟踪不同提交。
6.  logs ：包含引用日志，用于跟踪引用的历史变化。
7.  config ：存储仓库特定的配置信息。
8.  description ：一个可读的仓库描述文件。
9.  HEAD ：指向当前分支的符号链接。
10.  index ：Git的暂存区文件，用于跟踪已修改但尚未提交的更改。
11.  packed-refs ：包含指向引用的压缩文件，通常在引用较多时使用。
12.  info/exclude ：包含Git忽略规则的文件，类似于项目根目录中的 .gitignore  文件。

这些文件和目录包含了Git仓库的核心信息，允许Git跟踪历史、分支、标签、配置和文件更改等信息。
请注意， .git  文件夹通常是隐藏的，因此在文件浏览器中不容易看到，但它是Git版本控制系统的核心。
不要删除或修改 .git  文件夹中的文件，以免破坏Git仓库的完整性。

Git和GitHub的关系
Git 是底层的版本控制系统，负责跟踪和管理代码的更改。
GitHub 是建立在 Git 之上的代码托管平台，提供了丰富的协作功能和项目管理工具。

GitHub
GitHub 是一个基于 Git 的代码托管平台，为开源及私有软件项目提供托管服务。GitHub 允许用户托管自己的 Git 仓库，并提供了一个协作的平台，使得开发者可以轻松地分享、发现和贡献代码。
GitHub 提供了许多额外的功能，如代码审查（Pull Requests）、项目管理工具（如 Issues、Milestones、Projects）、社交网络功能（如关注、Star、Fork）、代码托管服务的整合（如 CI/CD 集成）等。

GitHub的一些功能：
watch：观察项目，如果watch了某个项目，表示你以后会关注这个项目的所有动态，以后只要这个项目发生变动，
如别人提交了pull request、被别人发起了issue等等情况，就会收到关于这个项目的通知提醒。
issue：问题，在项目中遇到问题时，可以通过issue来寻求他人的帮助或者提供相关信息；也可以通过issue来告知开发者项目存在的问题。
fork：从别人的仓库里复制一份到自己的仓库里，可以理解成复制一个分支，在复制的这个分支上进行修改、提交，
原仓库的更新不会影响这个分支。
pull request：将自己的修改推送给原仓库，向原仓库的开发者申请将自己在其他分支做的修改合并到原仓库的主分支中，
原仓库的开发者同意后，会自动将修改合并到原仓库的主分支中，这就实现了多人协作开发同一个项目。



分布式计算-大数据：
从业务层面来说，"大数据" 指的是由企业、组织和个人生成的大量和复杂的数据集。
这些数据集太大、太快或太复杂，它们无法用传统的数据处理工具进行处理和分析，它们通常来自多种来源，包括但不限于社交媒体、电子商务交易、移动设备、传感器和日志文件渠道获取数据。大数据既给社会带来机会，也给行业带来挑战。
一方面，企业可以利用大数据获得有价值的洞察力、改进决策和优化运营。另一方面，管理、存储和处理大数据需要专门的工具和技术，例如分布式系统、并行处理和机器学习算法。
为了处理大数据，企业通常使用一系列技术，包括 Hadoop、Spark、NoSQL 数据库和云计算平台。这些技术使企业能够实时存储、处理和分析大量数据，并根据数据派生的洞察力做出明智的决策。

总之，大数据是一个快速发展的领域，涉及从多种来源收集、管理和分析大量数据。
从数据的多方面收集，再到企业的大数据系统，最后再根据数据的应用、数据的分析，来达到影响企业决策的目的，
这才是大数据的终极目标，它有可能通过为组织提供新的洞察力和创新机会，来改变企业和行业。

从技术层面来说，大数据的技术主要是基于Hadoop为底层实现，由hive、spark实现分布式批计算，
spark streaming、flink实现分布式流式传输，由kafka实现消息队列，由Flume实现数据抽取，
zookeeper实现分布式注册中心，clickhouse、hbase实现列式存储，有些系统架构时需要元数据管理、
调度系统、监控系统的一整套处理海量数据的数据处理系统。
大数据的框架众多，大数据技术也在不断进步，处理批数据的pig和流式传输storm已经被淘汰。

Hadoop 的三大主要组件是：
Hadoop 分布式文件系统 (HDFS)：一个分布式文件系统，可以在一组商业化硬件节点上存储大量数据集。
HDFS 旨在具有高度可扩展性和容错性，并提供数据复制以确保数据在硬件故障的情况下可用。
MapReduce：一种在计算机集群上并行处理大数据集的编程模型。
MapReduce 将处理逻辑与数据存储分离，并自动处理数据分布、容错性和负载平衡。
YARN (Yet Another Resource Negotiator)：一个资源管理系统，
可协调在 Hadoop 集群上运行的各种应用程序之间的资源（如 CPU、内存和存储）的分配。
YARN 提供了一个灵活且可扩展的平台，用于运行各种大数据应用，例如批处理、实时流处理和机器学习算法。

总之，这三个组件组成了 Hadoop 生态系统的核心，并提供了一个强大的平台，用于存储、处理和分析大量复杂数据集。。

MapReduce 是一个分布式运算程序的编程框架，是用户开发"基于 Hadoop 的数据分析应用"的核心框架。
MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。
1.MapReduce运算程序一般需要分成2个阶段：Map阶段和Reduce阶段
2.Map阶段的并发MapTask，完全并行运行，互不相干
3.Reduce阶段的并发ReduceTask，完全互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出
4.MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行

用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。
1．Mapper阶段
（1）用户自定义的Mapper要继承自己的父类
（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）
（3）Mapper中的业务逻辑写在map()方法中
（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）
（5）map()方法（MapTask进程）对每一个<K,V>调用一次

2．Reducer阶段
（1）用户自定义的Reducer要继承自己的父类
（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV
（3）Reducer的业务逻辑写在reduce()方法中
（4）ReduceTask进程对每一组相同k的<k,v>组调用一次reduce()方法

3．Driver阶段
相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是
封装了MapReduce程序相关运行参数的job对象

大数据资源管理系统YARN：
Yarn是Hadoop的分布式资源调度平台，负责为集群的运算提供运算资源。
如果把分布式计算机和单个计算机相对应的话，HDFS就相当于计算机的文件系统，
Yarn就是计算机的操作系统，MapReduce就是计算机上的应用程序。

Yarn主要由三个组件组成：
ResourceManager：他是整个集群资源的老大，负责整个集群系统的资源分配与调度。
NodeManager：他是单个节点的老大，管理本节点的用户作业和工作流。
ApplicationMaster：他是单个应用程序的老大，负责单个应用的监控运行。

1. ResourceManager
ResourceManager 负责整个集群的资源管理和分配，是一个全局的资源管理系统。
NodeManager 以心跳的方式向 ResourceManager 汇报资源使用情况（目前主要是 CPU 和内存的使用情况）。
RM 只接受 NM 的资源回报信息，对于具体的资源处理则交给 NM 自己处理。
YARN Scheduler 根据 application 的请求为其分配资源，不负责application job 的监控、追踪、运行状态反馈、启动等工作。

2. NodeManager
NodeManager 是每个节点上的资源和任务管理器，它是管理这台机器的代理，负责该节点程序的运行，以及该节点资源的管理和监控。YARN 集群每个节点都运行一个NodeManager。
NodeManager 定时向 ResourceManager 汇报本节点资源（CPU、内存）的使用情况和Container 的运行状态。当 ResourceManager 宕机时 NodeManager 自动连接 RM 备用节点。
NodeManager 接收并处理来自 ApplicationMaster 的 Container 启动、停止等各种请求。

3. ApplicationMaster
用户提交的每个应用程序均包含一个 ApplicationMaster ， 它可以运行在ResourceManager 以外的机器上。
负责与 RM 调度器协商以获取资源（用 Container 表示）。
将得到的任务进一步分配给内部的任务(资源的二次分配)。
与 NM 通信以启动/停止任务。
监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。
当前 YARN 自带了两个 ApplicationMaster 实现，一个是用于演示AM 编写方法的实例程序 DistributedShell，
它可以申请一定数目的Container 以并行运行一个 Shell 命令或者 Shell 脚本；
另一个是运行 MapReduce 应用程序的 AM—MRAppMaster。

YARN工作机制：
1.客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例。
2.ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例。
3.ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，
以后就可以和自己的ApplicationMaster直接交互了。
4.在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求。
5.当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动
Container，container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料。
6.应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster。
7.在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，
交流的协议也是application-specific协议。
8.一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，
用到所有的Container也归还给系统。


数据仓库架构：
离线数仓：
数据由大数据技术保存在数据仓库中，而离线数据仓库的架构通常是什么？
离线数仓（离线数据仓库）的架构通常包含以下几个主要组件和步骤：

1. 数据源层（Data Sources Layer）：
    - 包括各种原始数据源，如关系数据库（MySQL、PostgreSQL）、日志文件、CSV文件、API数据等。
    - 数据源可以是结构化、半结构化或非结构化的数据。

2. 数据采集层（Data Ingestion Layer）：
    - 使用ETL（Extract, Transform, Load）工具或ELT（Extract, Load, Transform）工具进行数据采集。
    - 常见工具有Apache Sqoop、Flume、Kafka、Nifi等。
    - 负责从数据源提取数据并加载到数据仓库的暂存区（Staging Area）。

3. 暂存区（Staging Area）：
    - 用于存放从数据源提取的原始数据，通常是未经过处理的数据。
    - 这部分数据可以是文件系统（如HDFS）或数据库表。
    - 在暂存区进行初步的数据清洗和格式转换。

4. 数据存储层（Data Storage Layer）：
    - 数据仓库的核心存储区，用于存放清洗和转换后的数据。
    - 通常使用列式存储的分布式数据库，如Apache Hive、HBase，或者MPP（大规模并行处理）数据库如Greenplum、Snowflake、Amazon Redshift等。
    - 数据按照主题域（主题分区）进行存储，以便于分析和查询。

5. 数据处理层（Data Processing Layer）：
    - 使用批处理框架进行数据处理和计算，如Apache Spark、MapReduce等。
    - 数据处理的任务包括数据清洗、转换、聚合、分析等。
    - 处理后的数据会存入数据仓库的事实表和维度表中。

6. 数据访问层（Data Access Layer）：
    - 提供数据查询和分析的接口。
    - 包括SQL查询工具（如Apache Hive、Presto、Impala）、BI（商业智能）工具（如Tableau、Power BI）、数据科学工具（如Jupyter Notebook、R Studio）等。

7. 数据展现层（Data Presentation Layer）：
    - 数据的最终展示和可视化层。
    - 使用BI工具、报表工具、仪表盘等将数据以图表、报表等形式展示给最终用户。

8. 元数据管理（Metadata Management）：
    - 负责管理数据仓库的元数据，包括数据的结构信息、血缘关系、数据质量等。
    - 常见工具有Apache Atlas、AWS Glue等。

9. 数据治理和安全（Data Governance and Security）：
    - 确保数据的一致性、完整性和安全性。
    - 包括数据访问控制、数据审计、数据隐私保护等。

## 常见的离线数仓架构图示例

```plaintext
+----------------+      +----------------+      +----------------+      +----------------+      +----------------+      +----------------+
|   数据源层     | ---> |   数据采集层   | ---> |    暂存区      | ---> |   数据存储层   | ---> |   数据处理层   | ---> |   数据访问层   |
| (Data Sources) |      | (Data Ingestion)|     | (Staging Area) |     | (Data Storage) |     | (Data Processing)|     | (Data Access)  |
+----------------+      +----------------+      +----------------+      +----------------+      +----------------+      +----------------+
                                                                                                                                   |
                                                                                                                                   |
                                                                                                                                   v
                                                                                                                         +----------------+
                                                                                                                         |   数据展现层   |
                                                                                                                         | (Data Presentation)|
                                                                                                                         +----------------+
```

这种架构有助于确保离线数据仓库能够高效、稳定地处理大量数据，支持复杂的数据分析和查询需求。



实时数仓：
实时数仓（实时数据仓库）的架构通常更加复杂，因为它需要处理实时数据流，同时也要保持高效的查询和分析能力。以下是一个典型的实时数仓架构组件和步骤：

1. 数据源层（Data Sources Layer）：
    - 包括各种实时数据源，如数据库变更数据（CDC，Change Data Capture）、实时日志、消息队列（如Kafka、RabbitMQ）、IoT设备数据等。

2. 数据采集层（Data Ingestion Layer）：
    - 使用实时数据采集工具，如Kafka、Flink、Spark Streaming、NiFi等。
    - 负责将实时数据流从数据源采集并加载到流处理系统。

3. 流处理层（Stream Processing Layer）：
    - 使用流处理框架进行数据的实时处理和分析。
    - 常用工具包括Apache Flink、Apache Kafka Streams、Apache Spark Streaming等。
    - 任务包括实时数据清洗、转换、聚合、窗口操作等。
    - 处理后的数据可以实时写入到数据存储系统。

4. 数据存储层（Data Storage Layer）：
    - 实时数仓需要支持低延迟的存储系统。
    - 包括内存数据库（如Redis）、NoSQL数据库（如Cassandra、HBase）、实时分析数据库（如Druid、ClickHouse）、传统数据仓库的实时扩展（如Snowflake、Amazon Redshift）等。
    - 数据按照主题域或时间分区进行存储，以支持高效查询。

5. 数据访问层（Data Access Layer）：
    - 提供实时数据查询和分析的接口。
    - 包括实时SQL查询引擎（如Presto、Trino、Druid SQL）、BI工具、实时仪表盘（如Grafana、Tableau）等。

6. 数据展现层（Data Presentation Layer）：
    - 实时数据的可视化和展示层。
    - 使用BI工具、报表工具、仪表盘等，将数据以图表、报表等形式展示给最终用户，支持实时刷新和动态更新。

7. 元数据管理（Metadata Management）：
    - 负责管理数据仓库的元数据，包括数据的结构信息、血缘关系、数据质量等。
    - 需要支持实时数据的元数据管理。

8. 数据治理和安全（Data Governance and Security）：
    - 确保数据的一致性、完整性和安全性。
    - 包括数据访问控制、数据审计、数据隐私保护等。

## 常见的实时数仓架构图示例

```plaintext
+----------------+      +----------------+      +----------------+      +----------------+      +----------------+      +----------------+
|   数据源层     | ---> |   数据采集层   | ---> |   流处理层     | ---> |   数据存储层   | ---> |   数据访问层   | ---> |   数据展现层   |
| (Data Sources) |      | (Data Ingestion)|     | (Stream Processing) | | (Data Storage) |     | (Data Access)  |     | (Data Presentation)|
+----------------+      +----------------+      +----------------+      +----------------+      +----------------+      +----------------+
```

详细说明：
1. 数据源层（Data Sources Layer）：
    - 数据源可以是应用日志、用户行为日志、交易系统数据、传感器数据等。

2. 数据采集层（Data Ingestion Layer）：
    - 使用Kafka Connect、Flink Connectors等将数据源的数据流式采集到数据处理系统中。

3. 流处理层（Stream Processing Layer）：
    - 在这一层，实时数据被处理、清洗和转换。
    - 例如，使用Flink进行数据聚合和复杂事件处理（CEP）。

4. 数据存储层（Data Storage Layer）：
    - 处理后的数据被存储在支持高并发和低延迟查询的存储系统中。
    - 常用的存储系统有Druid、ClickHouse、Cassandra等。

5. 数据访问层（Data Access Layer）：
    - 提供查询接口，允许用户通过SQL或API访问实时处理后的数据。
    - 例如，使用Presto或Trino进行实时数据查询。

6. 数据展现层（Data Presentation Layer）：
    - 使用Grafana、Tableau等工具将数据以实时仪表盘和报表的形式展示给用户。

这种架构确保了实时数据能够快速被处理和分析，支持实时决策和动态报告。


数据仓库工具Hive：
Hive是由 Facebook 开源用于解决海量结构化日志的数据统计工具，Hive 是基于 Hadoop 的一个数据仓库工具，
可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。

Hive常见指令如下：

查看数据库：show databases。
进入数据库：use database_name。
查看库下所有的表：show tables。
查看表结构：desc table_name 或 show create table table_name。
查看Hive分区：show partitions table_name。
删除数据库：drop database database_name。
删除表数据：drop table table_name删掉整个表；truncate table table_name只是删除表的内容，并不会删除表；
delete from table_name where column_name = value，用于删除表中某一行的内容。
插入语句：insert into table_name values(value1,value2...)，或 insert overwrite table table_name select column1,column2... from another_table。

Hive和SQL在建表语句、对等值连接和非等值连接、存储NULL值、落地方式等方面有较大的差异。

建表语句：Hive在创建表时，会根据导入的数据格式来指定字段分隔符和列分隔符，而SQL没有这方面需求。
等值连接：Hive不支持等值连接，如果想进行等值连接操作，需要使用SQL语句。
存储NULL值：在传统数据库中字段没有值或者为空即表示为NULL，但是在Hive中默认的NULL值是\N。
落地方式：Hive将数据落地到HDFS，而SQL将数据落地到文件。


Hive的本质是将HQL转化成MapReduce程序：
Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的 Driver，结合元数据(MetaStore)，
将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。

Hive的优缺点：
优点：
（1）操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）。
（2）避免了去写 MapReduce，减少开发人员的学习成本。
（3）Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。
（4）Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。
（5）Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。

缺点：
1、Hive 的 HQL 表达能力有限
①迭代式算法无法表达
②数据挖掘方面不擅长，由于 MapReduce 数据处理流程的限制，效率更高的算法却无法实现。

2、Hive 的效率比较低
①Hive 自动生成的 MapReduce 作业，通常情况下不够智能化
②Hive 调优比较困难，粒度较粗

Hive的底层原理：
（1）解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，
这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，
比如表是否存在、字段是否存在、SQL语义是否有误。
（2）编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。
（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。
（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。

Scala：
Scala是一种多范式的编程语言，它集成了面向对象编程和函数式编程的特性，运行在Java虚拟机（JVM）上，并兼容Java程序。下面我们将用更专业的语言对Scala进行详细阐述。具有函数式编程，面向对象编程等特点。

1. 语言特性
Scala的语言设计旨在提供简洁、优雅且类型安全的代码。它支持模式匹配、隐式转换、高阶函数、不可变集合等特性，使得开发者能够编写出更加表达力强且易于维护的代码。
此外，Scala的case类提供了代数数据类型（ADT）的实现，这对于领域特定语言（DSL）的构建特别有用。

2. 函数式编程
Scala强调函数式编程，支持不可变数据、高阶函数和尾递归优化等。这使得Scala在处理并行和并发编程时具有优势，因为函数式编程范式天然地支持无共享状态的并发执行。此外，Scala的集合库提供了丰富的函数式操作，使得数据处理变得简单而高效。

3. 面向对象编程
尽管Scala以函数式编程为特色，但它同样支持传统的面向对象编程特性，如类、继承、封装和多态等。Scala的类系统比Java更加灵活，支持特质（traits）这一特性，可以实现类似于多重继承的效果，同时避免了多重继承带来的复杂性。

4. 与Java的互操作性
Scala运行在JVM上，因此可以与Java代码无缝集成。Scala代码可以调用Java库，反之亦然。这种互操作性使得Scala能够充分利用Java生态系统的丰富资源，同时也为Java开发者提供了一种更加现代和灵活的编程选择。

5. 并发和并行编程
Scala提供了强大的并发和并行编程支持，包括Actor模型、Futures和Promises等机制。这些特性使得Scala在处理大规模并发任务时表现出色，特别是在构建高性能的分布式系统时具有显著优势。

6. 类型系统
Scala的类型系统非常强大且灵活，支持泛型、类型参数化、类型推断和类型别名等特性。这使得开发者能够编写出更加安全且易于理解的代码，同时减少了运行时错误的可能性。

Scala的编译过程是运用了Java jvm生成单例模式，并用静态方法调用它。Scala底层运行在Java虚拟机（JVM）上，并且可以很好地与Java代码互操作。
在Scala中创建单例模式并利用静态方法调用这个单例实例的过程与在Java中非常相似，但Scala提供了更简洁的语法。

object MySingleton {  
  def sayHello(): Unit = {  
    println("Hello from MySingleton!")  
  }  
}  
  
object SingletonCaller {  
  def main(args: Array[String]): Unit = {  
    MySingleton.sayHello() // 调用单例对象的静态方法  
  }  
}

在这个例子中，MySingleton 是一个单例对象，而不是一个类。在Scala中，使用 object 关键字来定义一个单例对象。Scala会自动为这个对象提供线程安全的懒加载实现，保证它在首次使用时才被创建，并且在整个应用程序生命周期中只有一个实例。
sayHello 方法是一个成员方法，但它看起来就像是静态方法一样被调用，因为我们不需要创建 MySingleton 的实例来调用它。实际上，MySingleton 本身就是那个唯一的实例。
SingletonCaller 是另一个单例对象，它包含一个 main 方法作为程序的入口点。在 main 方法中，我们直接调用 MySingleton.sayHello()来执行单例对象中的方法。

这个过程在JVM上的工作原理如下：

1.当Scala程序启动时，Scala运行时环境会初始化所有的单例对象。这包括为它们分配内存空间，并执行它们的构造器代码（如果有的话）。
2.由于 MySingleton 是一个单例对象，Scala运行时环境会确保在整个JVM中只有一个 MySingleton 实例存在。
3.当 SingletonCaller.main 方法被调用时，Scala运行时环境已经准备好了 MySingleton 实例，因此可以直接调用其上的 sayHello 方法。
4.调用 MySingleton.sayHello() 时，实际上是在调用单例对象 MySingleton 的一个成员方法。Scala语法隐藏了创建和访问单例实例的细节，使得调用看起来像是静态方法调用一样简洁。
5.JVM执行 sayHello 方法中的代码，打印出消息 "Hello from MySingleton!"。

这个过程利用了Scala对单例对象的隐式支持和JVM的对象模型。Scala的单例对象语法糖让代码更简洁，而JVM则提供了实际的对象内存管理和方法执行机制。

为什么说Scala是纯面向对象语言？
Scala之所以被认为是纯面向对象语言，主要基于以下几个方面的原因：

一切皆为对象：在Scala中，每一个值都是一个对象，无论是基本数据类型还是复杂的数据结构，都可以被当作对象来处理。
这种设计使得Scala的编程模型更加统一和简洁，开发者可以用一致的方式来处理各种类型的数据。

类和对象的中心地位：Scala中的类和对象是编程的基础。类是对象的模板，定义了对象的属性和方法；而对象是类的实例化，是实际运行的实体。
Scala通过类和对象的概念，实现了代码的模块化和复用，提高了程序的可维护性和可扩展性。

无基本数据类型：与Java等语言不同，Scala没有基本数据类型（如int、float等）。在Scala中，所有的数据类型，包括所谓的“基本”数据类型，都是对象。
这种设计使得Scala的面向对象特性更加纯粹，也更容易理解和使用。

灵活的对象扩展机制：Scala提供了灵活的对象扩展机制，如子类化和基于混合的组合机制。这使得开发者可以方便地扩展对象的功能，
实现多重继承的效果，同时避免了多重继承带来的复杂性。

函数也是对象：Scala不仅是一种面向对象的语言，还融合了函数式编程的特点。在Scala中，函数也是对象，可以像其他对象一样被传递、赋值和调用。
这种设计使得Scala在保持面向对象特性的同时，也具备了函数式编程的灵活性和高效性。


Scala基本数据类型：

数值类型（AnyVal）：
整型：Scala中的整型用于存放整数值，如10、123、3456等。整型分为Byte、Short、Int、Long四类，每种类型都有固定的表示范围和字段长度，不受具体操作系统的影响，保证了Scala程序的可移植性。
在Scala中，整型默认为Int型，如果需要声明Long型，需要在数字后面添加'l'或'L'。
浮点型：Scala的浮点类型用于表示小数，如123.4f、7.8、0.12等。浮点型分为Float和Double两类，其中Float是单精度的，Double是双精度的。在需要高精度小数时，建议使用Double类型。
字符类型（Char）：字符类型可以表示单个字符。
其他数值类型：Scala还包括如Unit等数值类型，Unit对应于Java中的void，表示方法没有返回值。
引用类型（AnyRef）：
字符串类型：Scala中的StringOps是对Java中String的增强，提供了更多的字符串操作功能。
其他引用类型：Scala还包括如数组、自定义类等引用类型。
此外，Scala的数据类型还有一些特殊的规则和特性：

隐式转换：Scala中仍然遵守低精度的值类型向高精度的值类型自动转换（隐式转换）的规则，例如char类型可以自动转换为Int类型。
字面量：Scala中的字面量包括整型字面量、浮点型字面量、布尔型字面量和符号字面量等，它们分别用于表示不同类型的值。

在Scala中表示空有三种：
Unit类型：表示返回值为空，类似Java里的void。
Null类型：实例值Null
Nothing类型：Nothing 类型是一个特殊的底类型（bottom type），它表示不可能有值的类型，它是所有其他类型的子类型。

Scala函数式编程：
def sum ( x : Int, y : Int) : Int = {
    x + y
}
def：定义函数的关键字
sum：函数名
x，y：参数名
Int：参数类型
外层Int：函数返回值类型

函数至简原则：
//return可以省略，Scala会使用两数体的最后一行代码作为返回
def f1(name:string):string ={
        name
}

//如果函数体只有一行代码，可以省略花括号
def f2(name: string):string = name

//返回值类型如果能够推断出来，那么可以省略(:和返回值类型一起省路)
def f3(name :string) = name

//Scala如果期望是无返回值类型，可以省略等号
def f6(name: string){
    println(name)
}

//如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加
def f7(): unit ={
printin('atguigu")
}

调用：
f7

Scala匿名函数：
val fun=(name:String)=>{println(name)}
fun("atguigu")

//柯里化:函数柯里化是高阶函数的一种，高阶函数是指接收函数作为参数或返回函数的函数。柯里化提供了逐步传参的过程，使得函数的调用更加灵活和模块化。
def addcurrying(a:Int)(b:Int):Int={
    a + b 
}

Scala变量和常量:
var 变量名：变量类型 = 初始值
val 常量名：常量类型 = 初始值
 
tips：1.声明变量时，类型可以省略
      2.类型确定后，就不能修改。（Scala是强类型语言）
      3.变量声明时，必须要有初始值
      
Scala字符串：
通过+号拼接 
val name: String ="I"
val age:Int =18
println(age +"岁的"+ name +"在学习Scala")

printf用法：字符串，通过%传值
printf("%d岁的%s在学习Scala"，age，name)

通过$获取变量值
println(s"${age}岁的S{name}在学习Scala")

Scala处理文件：
//从文件中读取数据
Source.fromFile("src/main/resources/test.txt").foreach(print)

//.将数据写入文件
writer.write("hello scala from java writer")
val writer = new PrintWriter(pew File( pathname= "src/main/resources/output.txt"))
writer.close()

Scala尾递归计算阶乘：
在Scala中，你可以使用尾递归来优化阶乘的计算，以避免栈溢出的问题，尤其是当处理大整数时。
尾递归是函数式编程语言中一种重要的递归形式，因为它可以确保在递归调用之后没有其他的计算需要执行，因此编译器可以优化它，使得递归调用类似于循环，避免了递归深度的限制。

def factorialTailRec(n: BigInt, acc: BigInt = BigInt(1)): BigInt = {  
  if (n <= 0) acc  
  else factorialTailRec(n - 1, n * acc)  
}  
  
// 使用示例  
val result = factorialTailRec(BigInt(5)) // 计算5的阶乘  
println(result) // 输出120

Scala传名函数：
在Scala中，传名参数（By-name Parameter）是一个特殊的参数传递机制，它允许你将一个函数或代码块作为参数传递，并且这个代码块在每次被引用时都会被重新计算。
这与传值参数（By-value Parameter）不同，后者在传递给函数时只计算一次，并将结果传递给函数。

def f1(): Int ={
    printin("f1调用")
    12
}
def f2(a:=>Int):Unit ={
    println("a:" + a)
    println("a:"+ a)
}

输出:
a:23
a:23
f1调用
a:12
f1调用
a:12

当f2函数被调用时，传入的代码块（或表达式）并不会立即执行，而是每次当a在函数体中被引用时才会执行。因此，在你的f2函数示例中，a会被计算两次，并且每次计算时都会打印出a的值。

Scala惰性加载：
Scala中的惰性加载（Lazy Loading）是一种编程模式，用于延迟对象的初始化，直到真正需要使用该对象时再进行初始化。

惰性加载的主要优势在于，它可以帮助我们优化程序的性能。当我们有一个需要消耗大量资源或者时间来进行初始化的对象时，如果我们在程序启动时立即进行初始化，那么可能会浪费资源，
因为该对象可能在程序运行的大部分时间内都不被使用。通过使用惰性加载，我们可以将初始化推迟到真正需要使用该对象的时候，从而节省资源。

lazy val expensiveObject: ExpensiveType = createExpensiveObject()

expensiveObject是一个惰性加载的变量。在程序启动时，createExpensiveObject()函数不会被调用，expensiveObject也不会被初始化。
只有当程序首次访问expensiveObject时，createExpensiveObject()函数才会被调用，expensiveObject才会被初始化。

需要注意的是，惰性加载虽然可以优化性能，但也可能引入一些新的问题。例如，如果在多线程环境下使用惰性加载，需要确保对象的初始化是线程安全的。
此外，如果对象的初始化依赖于其他对象的初始化，那么使用惰性加载时也需要特别注意这些依赖关系。

Scala伴生对象（单例对象）：
在Scala中，伴生对象（companion object）是与类定义相关联的一个单例对象，它提供了与类相关的静态方法或字段的集合。
伴生对象在Scala中是一种特殊的对象，它与一个类共享相同的名称，但不属于类的实例成员，而是作为一个独立的对象存在。

伴生对象的主要用途是提供与类相关的静态功能，而不需要使用Java风格的静态方法或字段。在Scala中，没有显式的静态成员的概念，而是通过使用伴生对象来模拟静态行为。
伴生对象在Scala中的定义非常简单，它只需要在类定义之外使用object关键字，并赋予与类相同的名称即可。例如：

class MyClass {  
  // 类的实例成员  
}  
  
object MyClass {  
  // 伴生对象的成员  
  def myStaticMethod(): Unit = {  
    // 静态方法的实现  
  }  
    
  val myStaticField = "Some value"  
}
在上面的代码中，MyClass是一个类定义，而object MyClass则是与该类相关联的伴生对象。myStaticMethod是伴生对象的一个方法，可以像调用静态方法一样调用它，而不需要创建类的实例。
同样地，myStaticField是伴生对象的一个字段，可以作为静态字段使用。

要使用伴生对象的成员，可以通过类名直接访问，就像访问静态成员一样：

MyClass.myStaticMethod() // 调用伴生对象的方法  
val value = MyClass.myStaticField // 访问伴生对象的字段
伴生对象在Scala中提供了一种优雅的方式来组织与类相关的静态功能，避免了Java中常见的静态方法和字段的使用。
同时，由于伴生对象是一个独立的对象，它也可以拥有自己的状态和行为，这使得它在某些场景下比静态成员更加灵活和强大。

需要注意的是，伴生对象并不是类的成员，它们只是与类共享相同的名称空间。因此，伴生对象不能访问类的私有成员，而只能访问公共成员或通过类提供的方法来进行交互。
此外，伴生对象也可以独立于类存在，即可以定义一个只包含伴生对象的对象而不定义任何类。

PS：在Scala中有很多情况是用伴生对象来创建对象

Scala特质：
Scala 的特质（Traits）和 Java 的接口（Interfaces）在功能和用法上有一些相似之处，但也存在一些关键的差异。
相似之处：
1. 定义方法但不实现它们： Scala 的特质和 Java 的接口都可以定义方法，但不实现它们。它们的主要目的是为类提供一个契约，类必须实现这些方法来满足这个契约。
2. 多重继承： 在 Scala 中，一个类可以继承多个特质。同样，在 Java 8 及以后的版本中，一个类也可以实现多个接口。这允许开发者组合多个接口或特质的功能，实现更灵活的编程。

不同之处：
1. 实现细节：Scala 的特质不仅可以定义方法，还可以包含字段和方法的实现。这意味着特质可以包含更丰富的功能，而不仅仅是方法的声明。
相比之下，Java 的接口在 Java 8 之前只能定义方法的声明，不能包含字段或方法的实现。
从 Java 8 开始，接口可以包含默认方法和静态方法，但字段仍然只能是常量（即，public static final）。

2. 继承与混入：在 Scala 中，特质可以通过混入（mixin composition）的方式与类结合，而不仅仅是像 Java 中的接口那样通过继承。
这提供了更大的灵活性，允许开发者将特质与类以任何顺序组合，从而创建出具有所需功能的对象。

3. 类型检查：Scala 的特质可以用于类型检查，而 Java 的接口也可以。但是，Scala 的特质在这方面提供了更强大的功能，因为它们可以包含更多类型的成员（例如字段和方法）。

4. 特质可以扩展其他特质：Scala 的特质可以扩展其他特质，这使得特质可以像类一样形成一个层次结构。而 Java 的接口则不能继承其他接口（在 Java 8 之前），从 Java 8 开始，接口可以扩展其他接口。

总结来说，Scala 的特质在功能性和灵活性上超过了 Java 的接口。它们提供了更丰富的功能，包括字段和方法的实现，以及更灵活的混入机制。
然而，这并不意味着特质总是优于接口。在选择使用特质还是接口时，应根据具体的需求和场景来决定。

//定义特质
trait Greetable {  
  // 抽象方法，需要在实现该特质的类中定义  
  def greet: String  
  
  // 具体方法，可以直接在特质中实现  
  def sayHello: Unit = {  
    println("Hello, " + greet)  
  }  
}

//创建类，实现特质
class Person(val name: String) extends Greetable {  
  // 实现特质中的抽象方法  
  override def greet: String = "Mr. " + name  
  
  // Person类还可以定义自己的方法  
  def introduce: Unit = {  
    println("My name is " + name)  
  }  
}

//创建对象，并调用它的方法
class Person(val name: String) extends Greetable {  
  // 实现特质中的抽象方法  
  override def greet: String = "Mr. " + name  
  
  // Person类还可以定义自己的方法  
  def introduce: Unit = {  
    println("My name is " + name)  
  }  
}

//输出
Hello, Mr. Alice  
My name is Alice


//Scala特质混入
object Main {  
  def main(args: Array[String]): Unit = {  
    val person = new Person("Alice")  
      
    // 动态混入Greetable特质  
    val greetablePerson = new {  
      val name: String = person.name // 捕获外部person的name字段  
    } with Greetable {  
      override def greet: String = "Ms. " + name // 实现greet方法  
    }  
      
    // 调用混入特质的方法  
    greetablePerson.sayHello  
      
    // 调用原始Person类的方法  
    person.introduce  
  }  
}

关于特质叠加的执行顺序，Scala遵循一定的规则。当特质中存在方法调用super.xx时，Scala会从右到左执行这些调用。
此外，Scala的特质叠加还涉及到了初始化顺序的问题。当实例化一个类时，Scala会按照从左到右的顺序初始化混入的特质。

Scala自身类型：
Scala的自身类型（self-type）是一种特性，它用于声明一个特质（Trait）必须混入（mixin）其他特质，即使该特质没有直接扩展其他特质。这使得所依赖的成员可以在没有导入的情况下使用。
它提供了一种细化`this`或`this`别名类型的方法。自身类型在Scala的类型系统中起到了重要作用，它产生的效果与继承或混入颇为相似。

在Scala中，如果一个类或特质指明了自身类型，它的子类型或对象也必须是相应的类型。这提供了一种类型安全的机制，确保在特质或类之间建立正确的依赖关系。

使用自身类型时，需要写一个标识符，跟上要混入的另一个特质，并使用`=>`符号（例如`someIdentifier: SomeOtherTrait =>`）。
这意味着如果一个特质继承了另一个特质，并且需要混入第三个特质，它必须按照规定的自身类型进行声明。

在实际应用中，自身类型通常用于拆分大型的特质或类，以便更好地组织和管理代码。通过将大型特质拆分为多个小特质，并使用自身类型将它们组合在一起，可以提高代码的可读性和可维护性。

总的来说，Scala的自身类型是一种强大的机制，用于在特质和类之间建立明确的依赖关系，并确保类型安全。它允许开发者以更灵活和可维护的方式组织和管理代码。

trait Logger {  
  def log(message: String): Unit  
}  
  
trait Loggable {  
  this: Logger => // 声明自身类型，表示Loggable的使用者必须混入Logger特质  
  
  def doSomething(): Unit = {  
    // 使用Logger特质中的log方法  
    log("Doing something important")  
    // ... 执行其他操作 ...  
  }  
}  
  
// 一个具体的Logger实现  
class SimpleLogger extends Logger {  
  override def log(message: String): Unit = {  
    println(s"Logging: $message")  
  }  
}  
  
// 一个类，它混入了Loggable特质，并隐式地也混入了Logger特质（通过自身类型的要求）  
class MyService extends SimpleLogger with Loggable {  
  // 由于MyService混入了Loggable，它必须也混入Logger（通过继承SimpleLogger实现）  
  // 因此，doSomething方法中的log调用是有效的  
}  
  
object Main {  
  def main(args: Array[String]): Unit = {  
    val service = new MyService()  
    service.doSomething() // 输出 "Logging: Doing something important"  
  }  
}

Scala集合：
Scala集合主要可以分为两大类：可变集合和不可变集合。

不可变集合一旦创建，其内容就不会再改变，因此可以安全地在多线程应用程序中共享其引用。这种特性使得不可变集合在多线程环境下表现出色，无需担心数据竞态条件。

Scala中的集合类型还包括列表、序列、集等。列表有不可变列表（List）和可变列表（LinkedList）两种。
序列则包括常用的Vector和Range，其中Vector是ArrayBuffer的不可变版本，是一个带下标的序列；Range则表示一个整数序列。
集（Set）则是不重复元素的集合，与列表不同，集并不保留元素插入的顺序。

不可变（Immutable）List
不可变List是Scala集合库中的默认实现，一旦创建，其内容就不能被修改。这并不意味着你不能改变一个不可变List的引用，而是你不能改变List本身的内容。
当你对一个不可变List进行添加、删除或更新操作时，实际上会创建一个新的List，这个新的List是原始List的一个变种，包含了所需的更改。

不可变List的主要优点包括：

线程安全：由于不可变List的内容在创建后不会改变，因此它们天生就是线程安全的。多个线程可以共享同一个不可变List，而无需担心并发修改的问题。
持久化数据结构：不可变List提供了持久化的数据结构，即每次操作都会返回一个新的List，而原始List保持不变。这使得历史数据可以被保留，并可以用于构建复杂的算法或逻辑。
易于推理：由于每次操作都返回新的List，因此你可以更容易地通过链式调用或组合多个操作来构建复杂的集合转换，而无需担心中间状态或副作用。

List:
可变（Mutable）List
可变List允许你在创建后修改其内容，例如添加、删除或更新元素。与不可变List相比，可变List提供了更高的灵活性，但也带来了一些潜在的问题。

可变List的主要特点包括：
可修改性：你可以直接修改可变List的内容，而无需创建新的List。这使得在需要频繁修改List的场景下，可变List可能更加高效。
线程不安全：如果你在多线程环境中使用可变List，并且至少有一个线程修改它，那么你需要确保对它的访问是同步的，以避免并发修改异常或数据不一致。
线程安全性的维护通常需要额外的同步机制，这可能会增加代码的复杂性和性能开销。
状态变化：由于可变List的内容可以被修改，因此其状态可以在运行时发生变化。这可能会导致难以预测和推理的行为，特别是在复杂的算法或逻辑中。

Set:
不可变（Immutable）Set
不可变 Set 是Scala集合库中的默认实现，它提供了线程安全且不可变的集合。一旦一个不可变 Set 被创建，你就不能修改它（例如添加、删除或更新元素）。
但是，你可以通过创建一个新的 Set 来“修改”它，这个新的 Set 是原始 Set 的一个变种。
不可变 Set 的主要优点是线程安全且易于推理。由于它们是不可变的，所以你可以在多个线程之间共享它们，而无需担心并发修改的问题。
此外，由于每次操作都返回一个新的集合，所以你可以很容易地通过链式调用或组合多个操作来构建复杂的集合转换。


可变（Mutable）Set
可变 Set 允许你在创建后修改它（例如添加、删除或更新元素）。与不可变 Set 相比，可变 Set 提供了更高的灵活性，但也带来了线程安全和复杂性方面的问题。
如果你在多线程环境中使用可变 Set，并且至少有一个线程修改它，那么你需要确保对它的访问是同步的，以避免并发修改异常或数据不一致。
此外，由于可变 Set 允许就地修改，所以它们的行为可能更难以预测和推理，特别是在复杂的算法或逻辑中。

Map:
不可变（Immutable）Map
不可变Map是Scala集合库中的默认实现。一旦一个不可变Map被创建，你就不能修改它（例如添加、删除或更新键值对）。如果你试图修改一个不可变Map，Scala会返回一个新的Map，这个新的Map是原始Map的一个变种，包含了所需的更改。

不可变Map的主要优点包括：
线程安全：由于不可变Map的内容在创建后不会改变，因此它们天生就是线程安全的。多个线程可以共享同一个不可变Map，而无需担心并发修改的问题。
持久化数据结构：每次对不可变Map进行操作时，都会返回一个新的Map，而原始Map保持不变。这使得你可以保留历史数据，并构建复杂的算法或逻辑。
易于推理：由于每次操作都返回新的Map，你可以更容易地通过链式调用或组合多个操作来构建复杂的集合转换。

可变（Mutable）Map
可变Map允许你在创建后修改其内容，例如添加、删除或更新键值对。与不可变Map相比，可变Map提供了更高的灵活性，但也带来了一些潜在的问题。

可变Map的主要特点包括：
可修改性：你可以直接修改可变Map的内容，而无需创建新的Map。这使得在需要频繁修改Map的场景下，可变Map可能更加高效。
线程不安全：如果你在多线程环境中使用可变Map，并且至少有一个线程修改它，那么你需要确保对它的访问是同步的，以避免并发修改异常或数据不一致。这通常需要额外的同步机制，可能会增加代码的复杂性和性能开销。
状态变化：由于可变Map的内容可以被修改，因此其状态可以在运行时发生变化。这可能会导致难以预测和推理的行为，特别是在复杂的算法或逻辑中。

Scala隐式转换：
Scala中的隐式转换（Implicit Conversions）是一种强大的特性，它允许编译器在必要时自动地将一个类型的值转换为另一个类型的值。
这种转换是隐式的，也就是说，它不需要程序员显式地调用转换方法。隐式转换在Scala中非常有用，因为它可以帮助简化代码，提高可读性，并减少样板代码。

隐式转换主要通过隐式参数和隐式类来实现。

隐式参数
隐式参数是定义在作用域内的隐式值，它们可以被编译器自动用于方法调用中缺少的显式参数。这意味着你可以定义一个方法，该方法有一个或多个隐式参数，然后在调用该方法时省略这些参数，编译器会自动查找作用域内的相应隐式值来填充这些参数。

例如：
implicit val myImplicitValue: String = "Hello, World!"

def greet(implicit message: String): Unit = {
  println(message)
}

// 调用时省略了隐式参数message
greet() // 输出: Hello, World!


在上面的例子中，`greet`方法有一个隐式参数`message`。当调用`greet()`时，我们没有提供任何参数，但编译器找到了作用域内的隐式值`myImplicitValue`，并将其作为参数传递给`greet`方法。

隐式类
隐式类允许你定义一个类，该类可以在需要时自动将其他类型的值转换为该类的实例。隐式类必须满足以下条件：

1. 隐式类必须在其他对象或类的伴随对象中定义。
2. 隐式类只能有一个参数。

当编译器遇到一个期望的类型，但它得到了一个不匹配的类型时，它会查找是否有可用的隐式类可以将不匹配的类型转换为期望的类型。

例如：
object RichInt {
  implicit class IntWithTimes(x: Int) {
    def times(f: => Unit): Unit = {
      for (_ <- 1 to x) f
    }
  }
}

import RichInt._

// 使用隐式类定义的times方法
5.times { println("Hello") } // 输出Hello五次


在上面的例子中，我们定义了一个名为`RichInt`的对象，并在其中定义了一个隐式类`IntWithTimes`。这个隐式类接受一个`Int`类型的参数，并为其添加了一个`times`方法。
当我们在代码中写`5.times`时，编译器会自动找到并使用这个隐式类来将`Int`类型的值`5`转换为`IntWithTimes`的实例，从而可以调用`times`方法。

注意事项：
虽然隐式转换非常强大，但过度使用它也可能导致代码难以理解和维护。因此，在使用隐式转换时应该谨慎，并遵循一些最佳实践：
- 尽量将隐式定义放在与它们最相关的地方，比如伴随对象或特定的作用域内。
- 避免在全局范围内定义过多的隐式转换，以免产生意外的副作用。
- 隐式转换应该是可预测的，避免产生歧义或令人困惑的行为。
- 在团队开发中，使用隐式转换之前应与团队成员进行沟通和讨论，以确保大家都能理解和接受这种用法。

Scala上下文限定：
Scala的上下文限定（Context Bounds）是一种语法糖，它允许在类型参数上定义隐式参数的约束。这种机制使得在类型层面上引入额外的约束成为可能，而无需显式地传递额外的参数。
上下文限定的语法形式为T: M，其中T是类型参数，M是另一个泛型类或特质（trait）。这表示必须存在一个类型为M[T]的隐式值。

在Scala中，上下文限定常常与隐式转换和隐式参数一起使用，以提供类型安全的扩展和转换。通过上下文限定，你可以要求使用某个类型参数的方法或类必须存在一个满足特定条件的隐式值。
这有助于在编译时捕获潜在的错误，提高代码的可读性和可维护性。

下面是一个使用上下文限定的简单示例：
import scala.math.Ordering  

class Pair[T: Ordering](val first: T, val second: T) {  
  def smaller(implicit ord: Ordering[T]): T = {  
    if (ord.compare(first, second) < 0) first else second  
  }  
}  

// 隐式定义的Ordering，用于Int类型  
implicit val intOrdering: Ordering[Int] = Ordering.Int  
  
// 使用Pair类，并自动使用隐式定义的intOrdering  
val pair = new Pair(3, 4)  
val smallerValue = pair.smaller // 调用smaller方法时，编译器会自动查找并使用intOrdering  
println(smallerValue) // 输出: 3
在上面的例子中，Pair类定义了一个上下文限定T: Ordering，这要求必须存在一个类型为Ordering[T]的隐式值。当创建Pair的实例并调用smaller方法时，编译器会自动查找作用域内是否存在合适的隐式Ordering[T]值，并用于比较first和second的大小。在这个例子中，我们为Int类型定义了一个隐式的Ordering，因此当Pair的类型为Int时，编译器能够自动找到并使用它。

上下文限定提供了一种灵活且类型安全的方式来扩展现有类型的功能，而无需修改这些类型的源代码。它使得代码更加模块化，并允许开发者以声明式的方式定义类型之间的约束和关系。
注意：Scala中的语法糖运用得非常普遍。Scala作为一种多范式编程语言，旨在融合面向对象编程和函数式编程的最佳特性，并通过语法糖来简化这些特性的使用，这些语法糖使得代码更加简洁、易读和易于维护。


大数据Spark：
Spark 是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。
Spark底层是由Hadoop实现，但是和Hadoop有明显不同。

Spark和MapReduce对比:
1.MapReduce多个作业间的数据通信是基于磁盘, 而Spark多个作业间的数据通信是基于内存
2.一个MapReduce程序只有map+reduce, 而Spark程序可以有多个算子
3.MapReduce是批处理, 而Spark是批处理 & 微批准实时处理
4.Shuffle中Spark不用落盘, 而MapReduce要磁盘
5.Spark有丰富的RDD算子, SQL操作, Streaming流处理, 还可以处理机器学习, 而MapReduce只有Mapper和Reducer
6.Spark有容错机制可以切断血缘, 避免失败后从源头数据开始全部重新计算

Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展。
MapReduce 框架采用非循环式的数据流模型，把中间结果写入到 HDFS 中，带来了大量的数据复制、磁盘 IO 和序列化开销。
且这些框架只能支持一些特定的计算模式(map/reduce)，并没有提供一种通用的数据抽象。因此Spark发展出了RDD这个概念。

Spark RDD：
RDD(Resilient Distributed Dataset)叫做弹性分布式数据集，
是 Spark 中最基本的数据抽象，代表一个不可变、可分区、里面的元素可并行计算的集合。

RDD单词拆解：
Resilient ：它是弹性的，RDD 里面的中的数据可以保存在内存中或者磁盘里面；
Distributed ： 它里面的元素是分布式存储的，可以用于分布式计算；
Dataset: 它是一个集合，可以存放很多元素。

Spark中的RDD（Resilient Distributed Dataset）具有以下几个主要特点：
1. 分布式
RDD是一个分布式的数据集合，数据被分割成多个分区，并分布在集群的各个节点上。通过这种分布式设计，RDD能够在集群上并行处理数据，充分利用集群的计算资源和存储资源。

从HDFS读取数据并进行分布式处理
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("Distributed Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 从HDFS读取数据
val logFile = "hdfs://path/to/logfile"
val logsRDD = sc.textFile(logFile)

// 分布式计算每个IP地址的访问次数
val ipCounts = logsRDD
  .map(line => (line.split(" ")(0), 1)) // 提取IP地址并计数
  .reduceByKey(_ + _) // 聚合计数

// 打印结果
ipCounts.collect().foreach(println)

// 关闭SparkContext
sc.stop()

2. 不可变
RDD是不可变的，一旦创建就不能修改。每个RDD上的转换操作都会生成一个新的RDD。这种不可变性简化了并行编程中的数据管理问题，避免了多个任务对同一数据集进行并发修改时的冲突和数据不一致问题。
RDD的转换操作生成新的RDD
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("Immutable Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val data = Array(1, 2, 3, 4, 5)
val originalRDD = sc.parallelize(data)

// RDD的转换操作
val squaredRDD = originalRDD.map(x => x * x) // 求平方
val filteredRDD = squaredRDD.filter(x => x > 10) // 过滤大于10的元素

// 打印结果
filteredRDD.collect().foreach(println)

// 关闭SparkContext
sc.stop()

在这个例子中，originalRDD、squaredRDD和filteredRDD都是不可变的。每个转换操作（map和filter）生成一个新的RDD，而不会改变原始的RDD。

3. 弹性（容错性）
RDD具有容错性，通过血统（Lineage）信息实现。当一个数据分区丢失时，Spark可以根据RDD的血统信息重新计算丢失的数据分区。这种基于血统的容错机制确保了在集群节点故障时，计算任务仍然能够继续运行。

基于血统信息的容错机制
import org.apache.spark.{SparkConf, SparkContext}
import scala.util.Random

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("Fault Tolerance Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val data = 1 to 1000000
val originalRDD = sc.parallelize(data, 10) // 分成10个分区

// RDD的转换操作
val transformedRDD = originalRDD.map { x =>
  if (Random.nextInt(1000) == 0) {
    // 模拟随机节点故障
    throw new RuntimeException("Simulated error")
  }
  x * 2
}

// 行动操作
try {
  val result = transformedRDD.collect()
  println("Transformation successful")
} catch {
  case e: Exception =>
    println("Transformation failed, but Spark will handle it.")
    // 重新尝试操作
    val result = transformedRDD.collect()
    println("Transformation retried and successful")
}

// 关闭SparkContext
sc.stop()


4. 惰性求值
RDD的转换操作（如map、filter）是惰性求值的，这意味着这些操作并不会立即执行，而是记录在RDD的血统信息中，只有在遇到行动操作（如count、collect）时才会触发实际的计算。惰性求值可以优化计算过程，减少不必要的数据传输和计算。

5. 分区
RDD由多个分区组成，每个分区可以并行处理。分区是RDD的基本并行单位，Spark对每个分区独立进行操作，这使得计算过程可以充分利用多核处理器和分布式计算资源。

import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("Partition Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val data = Array(1, 2, 3, 4, 5)

//设置分区数
val partitionRDD = sc.makeRDD(data，2)

// 打印结果
partitionRDD.collect().foreach(println)

// 关闭SparkContext
sc.stop()

6. 基于血统（Lineage）记录
RDD维护了数据的血统信息，记录了从初始数据集到当前RDD的所有转换操作。血统信息不仅用于容错，还用于优化计算，通过分析血统信息，Spark可以进行诸如操作合并、过滤条件下推等优化。

7. 丰富的API
RDD提供了丰富的操作API，包括转换操作和行动操作。转换操作包括map、filter、flatMap、groupBy、reduceByKey等，行动操作包括collect、count、saveAsTextFile等。通过这些API，用户可以方便地进行各种数据处理和计算。

8. 内存计算
RDD支持内存计算，可以将中间结果缓存到内存中，以加快后续的计算过程。通过缓存机制，重复使用的RDD可以避免重复计算，从而显著提高性能。

9. 支持多种数据源
RDD可以从多种数据源创建，包括本地文件系统、HDFS、S3、HBase、Cassandra等。这使得Spark能够处理不同格式和来源的大数据集。

10. 跨语言支持
RDD支持多种编程语言，包括Scala、Java、Python和R。无论开发者使用哪种语言，都可以利用RDD的强大功能进行大数据处理。

RDD的创建：
1.用集合创建RDD
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("Create RDD from Collection").setMaster("local[*]")
val sc = new SparkContext(conf)

// 使用本地集合创建RDD
val data = List("apple", "banana", "orange")
val rdd = sc.parallelize(data)

// 打印RDD的内容
rdd.collect().foreach(println)

// 关闭SparkContext
sc.stop()

2.从文件中创建RDD
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession

// 创建SparkConf和SparkSession
val conf = new SparkConf().setAppName("Read CSV File").setMaster("local[*]")
val spark = SparkSession.builder.config(conf).getOrCreate()
val sc = spark.sparkContext

// 从CSV文件读取数据
val filePath = "file:///path/to/local/file.csv"
val df = spark.read.format("csv").option("header", "true").load(filePath)
val rdd = df.rdd

// 打印RDD的内容
rdd.collect().foreach(println)

// 关闭SparkSession
spark.stop()



RDD算子：
RDD的算子分为两类:
1.Transformation转换操作:返回一个新的 RDD
2.Action动作操作:返回值不是 RDD(无返回值或返回其他的)

转换算子：
map算子：
在Spark中，map算子是一个非常重要和常用的转换操作，它用于将一个RDD中的每一个元素通过一个指定的函数进行转换，并返回一个新的RDD。
map算子的功能类似于常见的编程语言中的map函数，能够对集合中的每个元素应用一个函数并生成一个新的集合。

Spark map算子的特点
一对一映射：map算子将原RDD中的每个元素一对一地映射到新的RDD中，即每个输入元素都会产生一个对应的输出元素。
惰性求值：map算子是一个惰性求值的操作。它不会立即执行，而是记录这个转换操作，只有当行动操作（如collect或save）被调用时才会触发实际的计算。
返回新的RDD：map算子返回一个新的RDD，新的RDD包含了应用函数后的元素。

示例代码：
val conf = new SparkConf().setAppName("Operator").setMaster("local[*]")
val spark = SparkSession.builder.config(conf).getOrCreate()

val rdd = sc.makeRDD(List(1,2,3,4))
rdd.map(_*2)

sc.stop()
注意:
RDD 不实际存储真正要计算的数据，而是记录了数据的位置在哪里，数据的转换关系(调用了什么方法，传入什么函数)。
RDD 中的所有转换都是惰性求值/延迟执行的，也就是说并不会直接计算。
只有当发生一个要求返回结果给 Driver 的 Action动作时，这些转换才会真正运行。
之所以使用惰性求值/延迟执行，是因为这样可以在 Action 时对 RDD 操作形成 DAG有向无环图进行 Stage 的划分和并行优化，
这种设计让 Spark 更加有效率地运行。


mapPartiti算子：
mapPartitions算子是Spark中一个强大且灵活的转换操作，用于对RDD中的每个分区应用一个函数。与map算子不同，mapPartitions对每个分区的数据集进行处理，而不是单独对每个元素进行处理。

Spark mapPartitions算子的特点
对每个分区进行操作：mapPartitions对RDD中的每个分区应用一次给定的函数，该函数接受一个迭代器作为输入，并返回一个新的迭代器。
减少函数调用开销：因为mapPartitions一次处理一个分区的所有元素，所以相比map算子，它减少了函数调用的开销。
适用于需要初始化开销的操作：对于一些需要初始化开销的操作（例如数据库连接、网络连接等），mapPartitions特别有用，因为初始化操作可以在每个分区中执行一次，而不是在每个元素上执行。

import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("MapPartitions Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val data = List(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data, 2) // 将数据分为两个分区

// 使用mapPartitions算子将每个分区的元素加倍
val mapPartitionsRDD = rdd.mapPartitions(iter => {
  iter.map(x => x * 2)
})

// 打印结果
mapPartitionsRDD.collect().foreach(println)

// 关闭SparkContext
sc.stop()

map算子和mapPartition算子的区别：
map算子：
适用于不需要分区上下文的简单操作，比如元素的转换、计算等。
当每个元素独立处理时，map是最自然和直观的选择。

mapPartitions算子：
适用于需要在分区级别进行批量处理或初始化资源的操作。
可以减少函数调用的开销，提高性能，特别是当函数调用开销较大时。

使用场景
批量处理：对于需要批量处理的操作，mapPartitions更为高效，因为它能够一次处理一个分区的所有元素，而不是一个一个处理。
减少初始化开销：对于每个分区需要一次性初始化的操作（如数据库连接、缓存初始化等），mapPartitions非常适用，因为这些初始化操作只需在每个分区进行一次。
高性能计算：当函数调用的开销较大时，mapPartitions通过减少函数调用的次数提高性能。

mapPartitonWithIndex算子：
mapPartitionsWithIndex 是 Spark 中的一个高级转换操作，它不仅像 mapPartitions 一样对每个分区进行处理，还会提供每个分区的索引（index），使用户能够在转换过程中访问分区号。
这对于需要根据分区号执行不同逻辑的情况特别有用。

Spark mapPartitionsWithIndex 算子的特点
分区级操作：与 mapPartitions 类似，mapPartitionsWithIndex 对每个分区进行批量处理。
提供分区索引：每个分区的处理函数接收两个参数——分区索引和分区数据的迭代器。这样，用户可以根据分区号实现不同的逻辑。
返回新的 RDD：返回一个新的 RDD，其中包含对每个分区应用给定函数后的结果。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("MapPartitionsWithIndex Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val data = List(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data, 2) // 将数据分为两个分区

// 使用mapPartitionsWithIndex算子将每个分区的元素加倍，并附加分区号
val mapPartitionsWithIndexRDD = rdd.mapPartitionsWithIndex((index, iter) => {
  iter.map(x => s"Partition: $index, Value: ${x * 2}")
})

// 打印结果
mapPartitionsWithIndexRDD.collect().foreach(println)

// 关闭SparkContext
sc.stop()


flatMap算子：flatMap 是 Spark 中的一个常用算子，用于将一个RDD中的每个元素通过一个指定的函数进行转换，然后将多个结果元素“扁平化”成一个新的RDD。
与 map 不同的是，flatMap 允许返回一个包含多个元素的集合，并将这些集合中的所有元素展平为一个新的RDD。

Spark flatMap 算子的特点
多对多映射：flatMap 可以将一个输入元素映射为零个、一个或多个输出元素。
扁平化处理：flatMap 生成的结果是将多个集合展平后的单一集合。
灵活处理：适用于需要将输入数据分解为多个输出数据的场景。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("FlatMap Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val sentences = List("Hello World", "Apache Spark is awesome", "FlatMap is useful")
val rdd = sc.parallelize(sentences)

// 使用flatMap算子将每个句子分割成单词
val wordsRDD = rdd.flatMap(sentence => sentence.split(" "))

// 打印结果
wordsRDD.collect().foreach(println)

// 关闭SparkContext
sc.stop()

flatMap模式匹配：
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("FlatMap Pattern Matching Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建事件列表
val events = List(
  ClickEvent("user1", "http://example.com"),
  PurchaseEvent("user1", "item1", 19.99),
  UnknownEvent("some random data"),
  ClickEvent("user2", "http://example.org")
)

// 创建RDD
val eventsRDD = sc.parallelize(events)

// 使用flatMap和模式匹配提取ClickEvent并展开
val clickEventsRDD = eventsRDD.flatMap {
  case ClickEvent(user, url) => Some((user, url))
  case _ => None
}

// 打印结果
clickEventsRDD.collect().foreach(println)

// 关闭SparkContext
sc.stop()


flatMap使用场景
1.分词处理：如示例所示，将句子分割成单词。
2.日志处理：将日志文件中的每一行分解成多个日志条目。
3.数据展开：将嵌套的集合或数组展平为单一的集合。例如，将JSON数组展平为单个元素。

glom算子：
glom 是 Spark 中的一个转换算子，它将每个分区的数据转换为一个包含该分区所有元素的数组。
换句话说，它将 RDD 的每个分区中的数据打包成一个数组，这样就得到了一个新的 RDD，其中每个元素都是一个数组，数组包含了原来分区中的所有元素。

glom 的特点
分区级别的操作：glom 将每个分区的数据聚集成一个数组。
保持数据局部性：不会跨分区移动数据，因此是一种非常轻量级的操作。
简化处理逻辑：在某些情况下，特别是需要对整个分区进行操作时（如调试、分区统计等），glom 可以简化处理逻辑。

使用场景
调试和监控：通过 glom 可以方便地查看每个分区中的数据，帮助调试和监控。
分区级别操作：需要对每个分区的数据进行汇总或其他操作时，可以先用 glom 将数据转换为数组，然后进行处理。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("Glom Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val rdd = sc.parallelize(1 to 10, 3) // 将数据分为3个分区

// 使用glom算子将每个分区的数据转换为数组
val glomRDD = rdd.glom()

// 打印结果
glomRDD.collect().foreach(arr => println(arr.mkString(", ")))

// 关闭SparkContext
sc.stop()

输出：
1, 2, 3
4, 5, 6
7, 8, 9, 10

groupBy算子：
groupBy 是 Spark 中的一个转换算子，它根据指定的条件将 RDD 中的元素进行分组。
groupBy 返回一个新的 RDD，每个分组由一个键及其对应的所有元素组成。groupBy 在数据处理中非常有用，可以将数据根据某些特征进行分类和分组。

Spark groupBy 算子的特点
分组操作：根据指定的函数（或条件）将 RDD 中的元素进行分组。
返回键值对RDD：返回的RDD是一个键值对RDD，其中每个键值对的键是分组键，值是该分组键对应的所有元素的集合。
可能产生数据倾斜：如果分组后的某些键有大量元素，这些键可能导致数据倾斜，进而影响性能。

使用场景
分类统计：按某个特征对数据进行分类统计。
数据聚合：对分组后的数据进行聚合操作。
复杂查询：在大数据分析中，按特定条件对数据进行分组以便进一步处理。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

// 创建SparkConf和SparkContext
val conf = new SparkConf().setAppName("GroupBy Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// 创建RDD
val data = List("apple", "banana", "cherry", "blueberry", "avocado")
val rdd = sc.parallelize(data)

// 使用groupBy根据首字母进行分组
val groupedRDD = rdd.groupBy(word => word.charAt(0))

// 打印结果
groupedRDD.collect().foreach{ case (key, values) => 
  println(s"$key: ${values.mkString(", ")}")
}

// 关闭SparkContext
sc.stop()

输出如下：
a: apple, avocado
b: banana, blueberry
c: cherry

groupBy 是 Spark 中一个非常强大的算子，可以根据指定的条件将 RDD 中的元素进行分组，并生成一个新的键值对 RDD。它广泛应用于分类统计、数据聚合和复杂查询等场景。
然而，使用 groupBy 时需要注意数据倾斜问题，特别是在分组键分布不均匀的情况下。通过理解和合理使用 groupBy，可以更高效地处理和分析大规模数据。


filter算子：
filter算子是Apache Spark中的一个重要操作符，用于从RDD（Resilient Distributed Dataset）中筛选出满足给定条件的数据。它的作用类似于SQL中的WHERE子句。

filter算子的概念
功能：filter算子根据指定的条件函数对RDD中的每个元素进行判断，保留满足条件的元素，过滤掉不满足条件的元素。
类型：filter算子是一个窄依赖操作，这意味着每个输出分区只依赖于一个输入分区，不涉及数据的重新分区或洗牌（shuffle），因此通常具有较高的执行效率。

应用场景
数据清洗：filter常用于数据清洗，过滤掉不需要的数据。例如，去除无效的日志行或缺失值。
数据子集：从大数据集中提取满足特定条件的子集。例如，从用户行为日志中提取出某一天的日志。
预处理：在数据分析或机器学习任务之前，对数据进行预处理，筛选出符合特定标准的数据。

示例代码：
val sc = new SparkContext("local", "Filter Example")

// 创建一个包含1到10的RDD
val data = sc.parallelize(1 to 10)

// 过滤出所有的偶数
val evenNumbers = data.filter(_ % 2 == 0)

// 收集结果并打印
evenNumbers.collect().foreach(println)



sample算子：
sample算子是Apache Spark中的一个有用操作符，用于从RDD（Resilient Distributed Dataset）中随机抽取一个子集。这个操作在数据分析和机器学习中经常使用，例如抽取训练集和测试集。

sample算子的概念
功能：sample算子根据指定的采样率从RDD中随机抽取一定比例的数据。采样可以是有放回的（带重复的）或无放回的（不带重复的）。
类型：sample算子是一个窄依赖操作，这意味着每个输出分区只依赖于一个输入分区，不涉及数据的重新分区或洗牌（shuffle）。

应用场景
数据抽样：在处理大规模数据时，可以使用sample抽取子集进行快速分析和调试。
训练集和测试集：在机器学习中，可以使用sample算子从数据集中抽取训练集和测试集。
数据平衡：在不平衡数据集的处理中，可以使用sample算子进行欠采样或过采样，以平衡不同类别的数据量。

示例代码：
val sc = new SparkContext("local", "Sample Example")

// 创建一个包含1到100的RDD
val data = sc.parallelize(1 to 100)

// 无放回采样，采样比例为10%
val sampleWithoutReplacement = data.sample(false, 0.1)

// 有放回采样，采样比例为10%
val sampleWithReplacement = data.sample(true, 0.1)

// 收集结果并打印
println("Sample without replacement:")
sampleWithoutReplacement.collect().foreach(println)

println("Sample with replacement:")
sampleWithReplacement.collect().foreach(println)



distinct算子：
distinct算子是Apache Spark中的一个重要操作符，用于从RDD（Resilient Distributed Dataset）中去除重复元素，生成一个只包含唯一元素的新RDD。

distinct算子的概念
功能：distinct算子用于从RDD中去除重复元素，返回一个只包含唯一元素的新RDD。
类型：distinct算子是一个宽依赖操作，这意味着每个输出分区可能依赖于多个输入分区。通常会涉及数据的重新分区和洗牌（shuffle）。

应用场景
数据去重：在数据清洗和预处理过程中，distinct算子可以用来去除重复的记录。
集合操作：在进行集合操作时，可以使用distinct算子获取唯一的元素集合。

性能优化
由于distinct算子是一个宽依赖操作，涉及数据的重新分区和洗牌，因此在大规模数据集上使用时需要注意性能优化：

合理分区：使用repartition或coalesce对数据进行重新分区，确保分区数量合适，避免数据倾斜。
聚合操作：在去重之前进行必要的聚合操作，减少需要去重的数据量。
缓存数据：如果需要多次进行去重操作，可以将中间结果缓存起来，避免重复计算。

示例代码：
val sc = new SparkContext("local", "Distinct Example")

// 创建一个包含用户点击数据的RDD
val clicks = sc.parallelize(Seq(
  ("user1", "page1"),
  ("user2", "page2"),
  ("user1", "page1"),
  ("user2", "page3"),
  ("user1", "page4")
))

// 提取所有用户的访问页面，并去除重复页面
val distinctPages = clicks.map { case (user, page) => page }.distinct()

// 收集结果并打印
distinctPages.collect().foreach(println)


repartition算子：
repartition算子是Apache Spark中的一个重要操作符，用于重新分区RDD（Resilient Distributed Dataset）。它在数据分布和负载均衡方面起到了重要作用，尤其是在数据处理和性能优化时非常有用。

repartition算子的概念
功能：repartition算子用于增加或减少RDD的分区数量。它通过洗牌（shuffle）操作将数据重新分配到新的分区中。
类型：repartition算子是一个宽依赖操作，因为它涉及到数据的重新分配和洗牌，输出分区可能依赖于多个输入分区。

应用场景
负载均衡：在数据处理过程中，如果分区数据量不均衡，可能会导致某些节点过载而其他节点空闲。通过repartition可以重新分配数据，实现负载均衡。
性能优化：在进行一些需要大量计算的操作之前，可以使用repartition增加分区数量，从而提高并行度和计算性能。
数据存储：在保存数据到外部存储系统（如HDFS）之前，可以使用repartition调整分区数量，以便控制输出文件的数量和大小。

性能优化
由于repartition是一个宽依赖操作，涉及数据的重新分配和洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：根据具体情况设置合适的分区数量，避免过多或过少的分区。
配合使用coalesce：在减少分区数量时，优先考虑使用coalesce而不是repartition，因为coalesce不涉及洗牌操作，效率更高。

示例代码：
val sc = new SparkContext("local", "Repartition Example")

// 创建一个包含用户点击数据的RDD，并将其分成10个分区
val clicks = sc.parallelize(Seq(
  ("user1", "page1"),
  ("user2", "page2"),
  ("user1", "page1"),
  ("user2", "page3"),
  ("user1", "page4")
), 10)

// 过滤出某些条件的点击数据
val filteredClicks = clicks.filter { case (user, page) => user == "user1" }

// 使用repartition算子将分区数量增加到5
val repartitionedClicks = filteredClicks.repartition(5)

// 查看分区数量
println(s"Number of partitions before repartition: ${filteredClicks.getNumPartitions}")
println(s"Number of partitions after repartition: ${repartitionedClicks.getNumPartitions}")

// 收集结果并打印
repartitionedClicks.collect().foreach(println)



coalesce算子：
coalesce算子是Apache Spark中的一个重要操作符，用于减少RDD（Resilient Distributed Dataset）的分区数量。它在进行性能优化和资源管理时非常有用。

coalesce算子的概念
功能：coalesce算子用于减少RDD的分区数量。与repartition不同，coalesce只进行合并操作，而不进行重新洗牌（shuffle），因此效率更高。shuffle如果设置为true，coalesce会进行洗牌操作，这与repartition类似。
类型：coalesce算子是一个窄依赖操作，这意味着它的每个输出分区只依赖于一个输入分区，通常不涉及数据的重新分区和洗牌（shuffle）。

应用场景
性能优化：在执行完map或filter等操作后，可能会产生大量的小分区。通过coalesce减少分区数量，可以减少任务调度开销，提高性能。
资源管理：在进行某些操作之前（如保存到HDFS或进行进一步的聚合操作），可以使用coalesce减少分区数量，以节省资源。

性能优化
虽然coalesce是一个窄依赖操作，但在使用时仍需注意以下几点以优化性能：

合理分区：根据具体情况设置合适的分区数量，避免过多或过少的分区。
shuffle参数：默认情况下，coalesce不会进行洗牌。如果需要更加均匀的分区，可以将shuffle参数设置为true，但这样会增加开销。

示例代码：
val sc = new SparkContext("local", "Coalesce Example")

// 创建一个包含100个元素的RDD，并将其分成10个分区
val data = sc.parallelize(1 to 100, 10)

// 使用coalesce算子将分区数量减少到5
val coalescedData = data.coalesce(5)

// 查看分区数量
println(s"Number of partitions before coalesce: ${data.getNumPartitions}")
println(s"Number of partitions after coalesce: ${coalescedData.getNumPartitions}")

// 收集结果并打印
coalescedData.collect().foreach(println)


coalesce算子和repartition算子的区别：
区别总结
特性	coalesce	repartition
是否洗牌（shuffle）	默认不洗牌，可选洗牌	总是洗牌
主要用途	减少分区数量	增加或减少分区数量
依赖类型	窄依赖	宽依赖
性能	效率较高，适合减少分区数量的场景	开销较大，适合需要重新分布数据的场景
数据分布	数据可能不均匀	数据重新分布更加均匀

选择使用
使用coalesce：当需要减少分区数量且不关心数据分布是否均匀时，使用coalesce效率更高。
使用repartition：当需要增加分区数量或需要确保数据分布均匀时，使用repartition更合适。


sortBy算子：
sortBy算子是Apache Spark中的一个重要操作符，用于对RDD（Resilient Distributed Dataset）中的元素进行排序。它在需要对数据进行排序的场景中非常有用。

sortBy算子的概念
功能：sortBy算子用于根据指定的键对RDD中的元素进行排序。它会生成一个新的RDD，其中的元素按照键的顺序排列。
类型：sortBy算子是一个宽依赖操作，因为它会触发洗牌（shuffle），即数据会在各个分区之间重新分配，以保证排序的正确性。

应用场景
数据排序：在需要对数据进行排序时，例如在生成排行榜、报告或对数据进行进一步分析时，可以使用sortBy算子。
数据处理流水线：在数据处理流水线中，可以使用sortBy对数据进行排序，以便后续操作能够按顺序处理数据。

性能优化
由于sortBy是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过numPartitions参数设置合适的分区数量，避免数据倾斜。
缓存中间结果：在对大数据集进行排序前，可以将中间结果缓存起来，以减少重复计算。
减少数据传输：尽量减少需要排序的数据量，以减少数据传输的开销。

交集、并集、差集、拉链：

交集 (intersection)：返回两个RDD的交集，包含两个RDD中共有的元素。
val sc = new SparkContext("local", "Intersection Example")

val rdd1 = sc.parallelize(Seq(1, 2, 3, 4, 5))
val rdd2 = sc.parallelize(Seq(3, 4, 5, 6, 7))

val intersectionRDD = rdd1.intersection(rdd2)

intersectionRDD.collect().foreach(println)  // 输出: 3, 4, 5

并集 (union)：返回两个RDD的并集，包含两个RDD的所有元素（不去重）。
val rdd1 = sc.parallelize(Seq(1, 2, 3))
val rdd2 = sc.parallelize(Seq(3, 4, 5))

val unionRDD = rdd1.union(rdd2)

unionRDD.collect().foreach(println)  // 输出: 1, 2, 3, 3, 4, 5

差集 (subtract)：返回第一个RDD中不包含在第二个RDD中的元素。
val rdd1 = sc.parallelize(Seq(1, 2, 3, 4, 5))
val rdd2 = sc.parallelize(Seq(3, 4, 5, 6, 7))

val subtractRDD = rdd1.subtract(rdd2)

subtractRDD.collect().foreach(println)  // 输出: 1, 2

拉链 (zip)：将两个RDD按位置进行拉链操作，返回键值对的RDD。
val rdd1 = sc.parallelize(Seq(1, 2, 3))
val rdd2 = sc.parallelize(Seq("a", "b", "c"))

val zippedRDD = rdd1.zip(rdd2)

zippedRDD.collect().foreach(println)  // 输出: (1, a), (2, b), (3, c)

partitionBy算子：
partitionBy算子是Apache Spark中的一个重要操作符，用于对数据进行分区操作，通常用于Pair RDD（键值对RDD）。它在需要对数据进行分区处理以优化性能和资源利用时非常有用。

partitionBy算子的概念
功能：partitionBy算子用于对Pair RDD进行重新分区。它根据指定的分区器（Partitioner）将RDD的元素重新分配到不同的分区中。
类型：partitionBy算子是一个宽依赖操作，因为它会触发洗牌（shuffle），即数据会在各个分区之间重新分配。

应用场景
数据分区优化：在对数据进行操作（如聚合、连接等）之前，可以使用partitionBy对数据进行重新分区，以优化性能。
数据分布管理：在需要控制数据分布的场景下，通过指定分区器，可以将数据按照特定的逻辑进行分配。
持久化存储：在将数据保存到外部存储系统之前，可以使用partitionBy对数据进行分区，以便控制输出文件的数量和大小。

性能优化
由于partitionBy是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过分区器设置合适的分区数量，避免数据倾斜。
选择合适的分区器：根据数据的特点选择合适的分区器（如HashPartitioner或RangePartitioner），以优化数据分布。
缓存中间结果：在对大数据集进行分区操作前，可以将中间结果缓存起来，以减少重复计算。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.HashPartitioner

val conf = new SparkConf().setAppName("PartitionBy Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含一些键值对的RDD
val data: RDD[(Int, String)] = sc.parallelize(Seq((1, "apple"), (2, "banana"), (3, "cherry"), (4, "date"), (5, "elderberry")), 3)

// 使用partitionBy算子将RDD重新分区
val partitionedData = data.partitionBy(new HashPartitioner(2))

// 查看每个分区的元素
partitionedData.glom().collect().foreach(partition => println(partition.mkString(", ")))

reduceByKey算子：
reduceByKey 是 Spark 中的一个常用算子，用于对键值对 (Pair RDD) 进行聚合操作。它通过将具有相同键的值进行合并，从而生成一个新的键值对 RDD。

reduceByKey 算子的概念
功能：reduceByKey 算子用于对具有相同键的值进行聚合操作，并生成一个新的键值对 RDD。它会将每个键对应的值使用指定的函数进行合并。
类型：reduceByKey 算子是一个宽依赖操作，因为它会触发洗牌（shuffle）操作，将具有相同键的值发送到同一个节点进行聚合。

应用场景
数据聚合：在需要对数据进行聚合操作（如求和、计数、最大值等）时，可以使用 reduceByKey 算子。
数据统计：在进行数据统计分析时，可以使用 reduceByKey 对数据进行分组和聚合。
分布式计算：在需要对大规模数据进行分布式计算时，reduceByKey 是一个高效的工具。

性能优化
由于 reduceByKey 是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过 numPartitions 参数设置合适的分区数量，避免数据倾斜。
预聚合：使用 mapValues 等算子对数据进行预处理和聚合，以减少洗牌的数据量。
本地聚合：使用 combineByKey 算子进行本地聚合，以减少网络传输的数据量。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("ReduceByKey Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含一些键值对的RDD
val data = sc.parallelize(Seq(("a", 1), ("b", 1), ("a", 2), ("b", 2), ("b", 3)))

// 使用reduceByKey算子对具有相同键的值进行聚合
val reducedData = data.reduceByKey((x, y) => x + y)

// 收集结果并打印
reducedData.collect().foreach(println)  // 输出: (a, 3), (b, 6)

groupByKey算子：
groupByKey 算子是 Spark 中用于对键值对 RDD 进行分组操作的算子。它将具有相同键的所有值放入一个迭代器中，并生成一个新的键值对 RDD。

groupByKey 算子的概念
功能：groupByKey 算子用于根据键将 RDD 中的值进行分组。它会将具有相同键的所有值放入一个迭代器中，并生成一个新的键值对 RDD，其中键是原始 RDD 中的唯一键，值是一个迭代器，包含与该键相关联的所有值。
类型：groupByKey 算子是一个宽依赖操作，因为它会触发洗牌（shuffle）操作，以确保具有相同键的所有数据能够被发送到同一个节点进行分组。

应用场景
分组聚合：在需要对具有相同键的值进行聚合操作时，可以使用 groupByKey 算子。
数据整理：在需要按键对数据进行整理和准备用于后续分析时，可以使用 groupByKey 进行数据的分组和组合。
分布式计算：在需要进行分布式计算和数据处理时，groupByKey 是一个常用的工具。

性能优化
由于 groupByKey 是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过 numPartitions 参数设置合适的分区数量，避免数据倾斜。
使用combineByKey替代：在需要进行聚合操作时，可以考虑使用 combineByKey 算子，它可以在本地对每个分区的数据进行聚合，减少数据的移动和洗牌操作。
避免过度分组：在数据量较大时，避免对大量数据进行过度的分组操作，以减少网络传输和内存

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("GroupByKey Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含一些键值对的RDD
val data = sc.parallelize(Seq(("a", 1), ("b", 2), ("a", 3), ("b", 4), ("a", 5)))

// 使用groupByKey算子对键进行分组
val groupedData = data.groupByKey()

// 收集结果并打印
groupedData.collect().foreach { case (key, values) =>
  println(s"Key: $key, Values: ${values.mkString(", ")}")
}

reduceByKey和groupByKey的区别：

聚合方式：
reduceByKey：对具有相同键的值进行聚合操作，使用用户提供的聚合函数进行合并。因此，它需要一个二元操作的函数作为参数，以便在本地对每个分区进行局部聚合，然后再将结果进行全局聚合。
groupByKey：简单地将具有相同键的所有值放入一个迭代器中，生成一个新的键值对 RDD。不进行聚合操作，而是将所有值集中到一个地方。

性能影响：
reduceByKey：由于在每个分区内进行本地聚合，所以可以减少数据移动和网络传输。它的性能通常比 groupByKey 更好，特别是在处理大规模数据时，因为它可以减少数据的移动和洗牌操作。
groupByKey：需要将所有具有相同键的数据传输到相同的节点上，并存储在内存中。这可能导致内存消耗较大，特别是在键的数量较多或者键的分布不均匀时。

适用场景：
reduceByKey：适用于需要进行聚合操作（如求和、计数等）的场景，以及当数据集较大时能够提供更好的性能。
groupByKey：适用于简单的分组操作，当不需要进行聚合操作，仅仅是将具有相同键的数据集中在一起时。

总结：reduceByKey是在suffle前聚合，groupByKey是在suffle后聚合

aggregateByKey算子：
aggregateByKey 是 Spark 中的一个强大算子，用于在键值对 RDD 上进行聚合操作。与 reduceByKey 和 groupByKey 相比，它提供了更灵活的聚合方式，允许在每个分区内和分区之间分别指定不同的聚合操作。

aggregateByKey 算子的概念
功能：aggregateByKey 算子用于对每个键的值进行聚合操作。它允许用户分别指定在每个分区内和分区之间的聚合操作。
类型：aggregateByKey 是一个宽依赖操作，因为它会触发洗牌（shuffle）操作，将具有相同键的值发送到同一个节点进行聚合。

应用场景
灵活的聚合操作：在需要对数据进行复杂的聚合操作时，aggregateByKey 提供了比 reduceByKey 和 groupByKey 更高的灵活性。
分区内和分区间聚合分离：在需要分别指定分区内和分区间的聚合逻辑时，aggregateByKey 是理想的选择。
初始值的使用：在聚合操作需要一个初始值时，可以通过 aggregateByKey 来指定。

性能优化
由于 aggregateByKey 是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过 RDD 的 repartition 或 coalesce 方法设置合适的分区数量，避免数据倾斜。
优化聚合函数：确保 seqOp 和 combOp 的计算效率，以减少聚合操作的开销。
使用初始值优化：选择合适的初始值 zeroValue 以确保聚合操作的正确性和效率。

示例代码：
val conf = new SparkConf().setAppName("AggregateByKey Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含用户点击数据的RDD
val clicks = sc.parallelize(Seq(
  ("user1", 1),
  ("user2", 2),
  ("user1", 3),
  ("user2", 4),
  ("user1", 5)
))

// 使用aggregateByKey算子计算每个用户的点击总数和点击次数
val zeroValue = (0, 0)  // (sum, count)
val seqOp = (acc: (Int, Int), value: Int) => (acc._1 + value, acc._2 + 1)
val combOp = (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)

val aggregatedClicks = clicks.aggregateByKey(zeroValue)(seqOp, combOp)

// 计算每个用户的平均点击数
val averageClicks = aggregatedClicks.mapValues { case (sum, count) => sum.toDouble / count }

// 收集结果并打印
averageClicks.collect().foreach(println)  // 输出: (user1, 3.0), (user2, 3.0)

foldByKey算子：
foldByKey 是 Spark 中的一个算子，用于对键值对 RDD 进行聚合操作。它类似于 aggregateByKey 和 reduceByKey，但提供了更简洁的语法，同时具有类似于 fold 操作的特性。
foldByKey 使用一个初始值（或称零值）和一个合并函数对每个键的值进行聚合。

foldByKey 算子的概念
功能：foldByKey 算子用于对每个键的值进行聚合操作。它使用一个初始值和一个合并函数，在每个分区内对数据进行局部聚合，然后在分区之间进行全局聚合。
类型：foldByKey 是一个宽依赖操作，因为它会触发洗牌（shuffle）操作，将具有相同键的值发送到同一个节点进行聚合。

应用场景
数据聚合：在需要对数据进行聚合操作（如求和、计数、最大值等）时，可以使用 foldByKey 算子。
简化代码：foldByKey 提供了比 aggregateByKey 更简洁的语法，对于简单的聚合操作，使用 foldByKey 可以使代码更简洁易读。
分布式计算：在需要对大规模数据进行分布式计算时，foldByKey 是一个高效的工具。

性能优化
由于 foldByKey 是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过 RDD 的 repartition 或 coalesce 方法设置合适的分区数量，避免数据倾斜。
优化合并函数：确保 foldFunc 的计算效率，以减少聚合操作的开销。
使用初始值优化：选择合适的初始值 zeroValue 以确保聚合操作的正确性和效率。

示例代码：
val conf = new SparkConf().setAppName("FoldByKey Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含用户点击数据的RDD
val clicks = sc.parallelize(Seq(
  ("user1", 1),
  ("user2", 2),
  ("user1", 3),
  ("user2", 4),
  ("user1", 5)
))

// 使用foldByKey算子计算每个用户的点击总数
val zeroValue = 0
val foldFunc = (x: Int, y: Int) => x + y

val foldedClicks = clicks.foldByKey(zeroValue)(foldFunc)

// 收集结果并打印
foldedClicks.collect().foreach(println)  // 输出: (user1, 9), (user2, 6)

combineByKey算子：
combineByKey 是 Spark 中最灵活的键值对 RDD 聚合算子。它允许用户定义三个函数来分别处理每个分区内和分区之间的数据聚合，从而提供了极大的灵活性。

combineByKey 算子的概念
功能：combineByKey 算子用于对每个键的值进行聚合操作。它允许用户定义初始化函数、分区内聚合函数和分区间聚合函数。
类型：combineByKey 是一个宽依赖操作，因为它会触发洗牌（shuffle）操作，将具有相同键的值发送到同一个节点进行聚合。

语法：
def combineByKey[C](
    createCombiner: V => C,
    mergeValue: (C, V) => C,
    mergeCombiners: (C, C) => C
): RDD[(K, C)]

参数说明
createCombiner：用于将每个值初始化为组合值的函数。
mergeValue：用于在分区内将值合并到组合值中的函数。
mergeCombiners：用于在分区间合并组合值的函数。

应用场景
复杂聚合操作：在需要进行复杂聚合操作时，combineByKey 提供了灵活性，可以分别定义分区内和分区间的聚合逻辑。
自定义聚合逻辑：在需要对数据进行自定义的聚合操作时，combineByKey 是一个理想的选择。
性能优化：通过分别定义分区内和分区间的聚合函数，可以优化数据处理的性能。

性能优化
由于 combineByKey 是一个宽依赖操作，涉及数据的洗牌，因此在大规模数据集上使用时需要注意性能优化：
合理设置分区数量：通过 RDD 的 repartition 或 coalesce 方法设置合适的分区数量，避免数据倾斜。
优化聚合函数：确保 createCombiner、mergeValue 和 mergeCombiners 的计算效率，以减少聚合操作的开销。
使用初始值优化：选择合适的初始值以确保聚合操作的正确性和效率。

示例代码：
val conf = new SparkConf().setAppName("CombineByKey Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含用户点击数据的RDD
val clicks = sc.parallelize(Seq(
  ("user1", 1),
  ("user2", 2),
  ("user1", 3),
  ("user2", 4),
  ("user1", 5)
))

// 使用combineByKey算子计算每个用户的点击总数和点击次数
val createCombiner = (v: Int) => (v, 1)
val mergeValue = (acc: (Int, Int), v: Int) => (acc._1 + v, acc._2 + 1)
val mergeCombiners = (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)

val combinedClicks = clicks.combineByKey(createCombiner, mergeValue, mergeCombiners)

// 计算每个用户的平均点击数
val averageClicks = combinedClicks.mapValues { case (sum, count) => sum.toDouble / count }

// 收集结果并打印
averageClicks.collect().foreach(println)  // 输出: (user1, 3.0), (user2, 3.0)


join算子：
leftOuterJoin和RightOuterJoin算子：
cogroup算子：

行动算子：
count()
first()
take()
takeOrdered()

aggregate()
countByKey()
countByValue()
save()
foreach()

Spark闭包检测：
Spark闭包检查是Spark框架中一个重要的机制，主要用于确保在分布式计算环境中，闭包（Closure）内的对象可以被正确地序列化并传输到Executor节点上执行。闭包检查的主要目的是防止因为序列化失败而导致的任务执行错误。


Spark的Kryo序列化：
Spark Kryo序列化是Apache Spark框架中支持的一种高性能的Java序列化机制。Kryo是一个快速且高效的序列化框架，它通过使用自定义的序列化方式来避免Java标准序列化的一些性能问题，如序列化体积大、序列化速度慢等。
在Spark中引入Kryo序列化框架的主要目的是为了提高Spark作业的性能，特别是在处理大规模数据时，序列化的性能对整个作业的性能影响非常大。


Spark血缘分析：
在Apache Spark中，血缘分析（Lineage Analysis）是指追踪和记录RDD（Resilient Distributed Dataset）之间的依赖关系和操作链的过程。
血缘信息包含了从初始数据集到最终结果数据集的所有转换和操作步骤。血缘分析的主要目的是为了实现容错和数据恢复。

血缘信息的用途：
1.容错机制：当某个节点或任务失败时，Spark可以利用血缘信息重新计算丢失的数据。
因为每个RDD都知道它是由哪些操作生成的，以及这些操作的父RDD是什么，所以Spark可以从头开始重新计算出需要的数据，而不需要重新计算整个作业。

2.优化查询：Spark可以通过分析血缘信息来优化查询执行。例如，Spark可以识别并优化重复的计算，避免不必要的计算步骤。

3.调试和监控：开发人员可以通过查看血缘信息来了解数据处理的全过程，帮助调试和优化Spark作业。

查看血缘信息的方法：
在Spark中，可以使用toDebugString方法查看RDD的血缘信息：
println(wordCounts.toDebugString)

Spark RDD的依赖关系：宽依赖和窄依赖
对于窄依赖：
窄依赖指的是一个子RDD的每个分区最多依赖于父RDD的一个分区。换句话说，数据可以在分区之间直接传输，而不需要进行数据的重新分配或洗牌。这种依赖关系使得Spark能够在发生故障时高效地重算数据。
常见的窄依赖操作包括：
Map：每个输出分区依赖于一个输入分区。
Filter：每个输出分区依赖于一个输入分区。
Union：每个输出分区依赖于两个输入分区（但这仍然是窄依赖，因为它们之间没有重新分区）。

对于宽依赖：
宽依赖指的是一个子RDD的每个分区依赖于父RDD的多个分区。这通常需要数据的重新分配或洗牌（shuffle），使得计算变得更加复杂和昂贵。
宽依赖的操作通常会导致Spark作业的性能下降，因为洗牌操作涉及到网络传输、磁盘I/O等开销。
常见的宽依赖操作包括：
GroupByKey：每个输出分区依赖于多个输入分区的数据。
ReduceByKey：每个输出分区依赖于多个输入分区的数据。
Join：需要将两个RDD的数据按照键进行重新分配。


Spark DAG：
DAG(Directed Acyclic Graph 有向无环图)：指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；
原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)。
DAG 的边界:
开始：通过 SparkContext 创建的 RDD；
结束：触发 Action，一旦触发 Action 就形成了一个完整的 DAG。

划分stage可以运用在并行计算。一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，
才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。
那么我们按照 shuffle 进行划分(也就是按照宽依赖就行划分)，就可以将一个 DAG 划分成多个 Stage/阶段，
在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行。

总结：Spark 会根据 shuffle/宽依赖使用回溯算法来对 DAG 进行 Stage 划分，
从后往前，遇到宽依赖就断开，遇到窄依赖就把当前的 RDD 加入到当前的 stage/阶段中。


Spark持久化：
Spark 持久化（Persistence）是指将 RDD（Resilient Distributed Datasets，弹性分布式数据集）存储在内存或磁盘中，以便在后续的操作中重用，从而提高计算效率。
持久化可以避免重复计算，特别是在需要对同一个 RDD 进行多次操作时。

持久化方法
Spark 提供了两个主要的方法来进行持久化：
persist()：允许指定存储级别（Storage Level）。
cache()：默认将 RDD 存储在内存中（等同于 persist(StorageLevel.MEMORY_ONLY)）。

存储级别
Spark 提供了多个存储级别，用于控制 RDD 的持久化方式：
MEMORY_ONLY：仅将 RDD 存储在内存中。如果内存不足，会丢弃部分分区并在需要时重新计算。
MEMORY_AND_DISK：优先将 RDD 存储在内存中。如果内存不足，会将部分分区存储到磁盘中。
MEMORY_ONLY_SER：将 RDD 以序列化的形式存储在内存中，适用于需要节省内存的情况。
MEMORY_AND_DISK_SER：将 RDD 以序列化的形式存储在内存和磁盘中。
DISK_ONLY：仅将 RDD 存储到磁盘中。
OFF_HEAP：将 RDD 存储在堆外内存中（需要启用堆外内存支持）。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.storage.StorageLevel

val conf = new SparkConf().setAppName("Persistence Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个RDD
val data = sc.parallelize(1 to 1000000)

// 使用cache方法进行持久化
data.cache()

// 使用persist方法进行持久化，指定存储级别为MEMORY_AND_DISK
data.persist(StorageLevel.MEMORY_AND_DISK)

// 执行一些操作
val count = data.count()
println(s"Count: $count")

val sum = data.sum()
println(s"Sum: $sum")


持久化的优缺点
优点
提高计算效率：避免重复计算，提高性能，特别是在需要对同一个 RDD 进行多次操作时。
灵活性：提供多种存储级别，可以根据内存和磁盘资源情况灵活选择。
容错性：持久化的数据可以在节点故障时重新计算，保证了数据的可靠性。

缺点
内存开销：持久化需要占用内存和磁盘资源，如果资源不足，可能会影响其他计算任务。
管理复杂度：需要合理选择存储级别和管理持久化的生命周期，增加了一定的管理复杂度。


RDD 容错机制Checkpoint：
在 Spark 中，RDD（Resilient Distributed Datasets）是具有弹性和容错性的分布式数据集。Spark 提供了多种机制来确保 RDD 在故障情况下的可靠性和容错性，其中一种重要的机制是检查点（Checkpointing）。

检查点（Checkpointing）概念
检查点（Checkpointing）是将 RDD 的数据写入持久存储（例如 HDFS 或本地文件系统），从而将 RDD 的计算血缘（lineage）图进行截断。
通过这种方式，Spark 可以在故障恢复时直接从检查点数据重建 RDD，而不需要重新计算其血缘图中的所有依赖操作。

检查点的作用
容错性：在计算过程中，如果遇到节点故障或内存溢出等问题，Spark 可以使用检查点数据来重建 RDD，避免重新计算复杂的依赖操作。
优化：对于长时间运行的应用程序或包含复杂依赖关系的 RDD，使用检查点可以减少计算血缘图的长度，从而降低 DAG（有向无环图）的管理开销。

使用检查点的场景
长时间运行的应用：在长时间运行的应用中，使用检查点可以防止血缘图过长，从而避免过多的重新计算。
复杂依赖关系的 RDD：当 RDD 有复杂的依赖关系时，检查点可以截断血缘图，减少重新计算的开销。
迭代计算：在迭代计算中，每次迭代都会生成新的 RDD，使用检查点可以减少计算血缘图的长度，提升计算效率。

检查点的使用方法
设置检查点目录：首先需要为 Spark 应用设置检查点目录，用于存储检查点数据。检查点目录可以是 HDFS 或本地文件系统。
调用 checkpoint() 方法：对需要检查点的 RDD 调用 checkpoint() 方法。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("Checkpoint Example").setMaster("local")
val sc = new SparkContext(conf)

// 设置检查点目录
sc.setCheckpointDir("hdfs:///checkpointDir")

// 创建一个RDD
val data = sc.parallelize(1 to 1000000)

// 对RDD进行一些操作
val mappedData = data.map(x => (x, x * 2))

// 对RDD进行检查点
mappedData.checkpoint()

// 触发行动操作以执行检查点
mappedData.count()

// 执行其他操作
val filteredData = mappedData.filter(_._2 % 4 == 0)
filteredData.collect().foreach(println)

在这个例子中，我们设置了检查点目录，并对 mappedData RDD 进行了检查点操作。随后，通过行动操作 count() 触发检查点，使 RDD 的数据被写入持久存储。在之后的计算中，Spark可以直接从检查点数据重建 RDD。

持久化和 Checkpoint 的区别：
目的：
持久化主要用于提高计算效率，避免重复计算。
检查点主要用于提高容错性，截断血缘图，减少重新计算的开销。

实现方式：
持久化通过将 RDD 存储在内存或磁盘中，提供多种存储级别。
检查点通过将 RDD 的数据写入持久存储（如 HDFS），需要指定存储目录，并通过行动操作触发。

使用场景：
持久化适用于需要对同一个 RDD 进行多次操作的场景，以及迭代算法的中间结果存储。
检查点适用于长时间运行的应用和复杂依赖关系的 RDD，以及迭代计算中的血缘图截断。

性能开销：
持久化的性能开销较低，因为其主要涉及内存和磁盘的读写操作。
检查点的性能开销较高，因为其涉及将数据写入持久存储（如 HDFS），需要更多的 I/O 操作。



Spark I/O流：Spark I/O是指在Apache Spark中进行数据输入（Input）和输出（Output）操作的过程和机制。Spark I/O涉及从各种数据源读取数据（输入），以及将处理后的数据写入到各种存储系统（输出）。
Spark I/O的主要组件和功能
数据读取（Input）

1. 文件系统：
   - HDFS（Hadoop Distributed File System）：Spark可以从HDFS中读取数据，这对于分布式计算环境非常重要。    
     val rdd = sc.textFile("hdfs://path/to/file")
     
   - 本地文件系统：Spark也支持从本地文件系统读取数据，适用于开发和测试。     
     val rdd = sc.textFile("file:///path/to/local/file")
     
   - S3（Amazon Simple Storage Service）：Spark可以从S3中读取数据，适用于云计算环境。    
     val rdd = sc.textFile("s3a://bucket/path/to/file")
     

2. 数据库：
   - JDBC：通过JDBC连接，Spark可以从关系数据库中读取数据。
     
     val jdbcDF = spark.read
       .format("jdbc")
       .option("url", "jdbc:postgresql://hostname:port/dbname")
       .option("dbtable", "tablename")
       .option("user", "username")
       .option("password", "password")
       .load()
     

3. NoSQL数据库：
   - Cassandra：通过Spark Cassandra连接器，可以从Cassandra数据库中读取数据。
   - HBase：使用Spark HBase连接器，可以从HBase读取数据。

4. 数据流：
   - Kafka：通过Spark Streaming，可以从Kafka读取实时数据流。    
     import org.apache.spark.streaming._
     import org.apache.spark.streaming.kafka._

     val ssc = new StreamingContext(sc, Seconds(10))
     val kafkaStream = KafkaUtils.createStream(ssc, "zkQuorum", "groupId", Map("topic" -> 1))
     

数据写入（Output）
1. 文件系统：
   - HDFS：将数据写入HDFS，便于分布式存储和后续处理。  
     rdd.saveAsTextFile("hdfs://path/to/output")
     
   - 本地文件系统：将数据写入本地文件系统，适用于小规模数据处理和测试。 
     rdd.saveAsTextFile("file:///path/to/local/output")
     
   - S3：将数据写入S3，适用于云存储和分布式处理。
     rdd.saveAsTextFile("s3a://bucket/path/to/output")
     

2. 数据库：
   - JDBC：通过JDBC连接，将处理结果写入关系数据库。
     
     jdbcDF.write
       .format("jdbc")
       .option("url", "jdbc:postgresql://hostname:port/dbname")
       .option("dbtable", "output_table")
       .option("user", "username")
       .option("password", "password")
       .save()
     

3. NoSQL数据库：
   - Cassandra：通过Spark Cassandra连接器，将数据写入Cassandra。
   - HBase：使用Spark HBase连接器，将数据写入HBase。

4. 数据流：
   - Kafka：通过Spark Streaming，将处理结果写入Kafka。
     
     kafkaStream.map(record => (record.key, record.value)).foreachRDD { rdd =>
       rdd.foreachPartition { partitionOfRecords =>
         val producer = new KafkaProducer[String, String](kafkaParams)
         partitionOfRecords.foreach { record =>
           val message = new ProducerRecord[String, String](outputTopic, record._1, record._2)
           producer.send(message)
         }
         producer.close()
       }
     }
     

Spark I/O的特点和优势
1. 多种数据源支持：
   - Spark支持多种数据源，包括本地文件系统、分布式文件系统（如HDFS、S3）、关系数据库、NoSQL数据库、消息队列等。

2. 统一的API：
   - Spark提供统一的API来处理不同类型的数据源和数据格式，使得用户可以通过一致的编程模型进行数据读取和写入操作。

3. 高效的分布式处理：
   - Spark I/O操作天然支持分布式处理，能够充分利用集群资源进行大规模数据的高效读写。

4. 容错机制：
   - Spark I/O操作具备容错机制，通过数据分区和重算机制，确保在节点故障时能够恢复数据处理。


SparkI/O流和Java I/O流的比较：
Spark的I/O流和Java的I/O流在功能、用途和设计理念上有一些相同点和不同点。

相同点
1. 基本概念：
   - 两者都涉及输入和输出操作，即读取数据和写入数据。
   - 都需要处理数据流和数据块。

2. 数据格式支持：
   - 都支持多种数据格式的读写，包括文本文件、二进制文件等。

3. 抽象和接口：
   - 两者都提供了抽象和接口来简化I/O操作，例如，Java的InputStream和OutputStream，Spark的DataFrameReader和DataFrameWriter。

不同点
1. 应用层次：
   - Java I/O流：是底层I/O操作的基础类库，提供对文件、网络等的基本读写功能。它是Java标准库的一部分，广泛用于各种Java应用程序。
   - Spark I/O流：是用于大数据处理的高级抽象，专注于分布式数据处理和存储。它建立在Hadoop等分布式文件系统之上，支持大规模数据的并行读写。

2. 抽象级别：
   - Java I/O流：提供低级别的I/O操作，开发者需要手动管理流的打开、关闭和处理。例如，FileInputStream、FileOutputStream、BufferedReader、BufferedWriter等。
   - Spark I/O流：提供高级别的数据抽象，数据以分布式集合（如RDD、DataFrame、Dataset）形式进行操作。Spark自动管理数据的分区、调度和容错。用户主要通过API进行数据加载和存储，如spark.read、spark.write等。

3. 并行和分布式处理：
   - Java I/O流：主要是单机模式，处理文件或网络数据时通常是单线程或多线程的方式，但需要开发者自行管理并行和同步问题。
   - Spark I/O流：天然支持分布式和并行处理，能够在集群上运行，通过分布式文件系统（如HDFS）进行数据存储和读取。Spark自动处理数据的分区和并行计算，大大简化了大数据处理的复杂性。

4. 使用场景：
   - Java I/O流：适用于小规模数据处理、系统编程、文件处理、网络编程等场景。
   - Spark I/O流：适用于大数据分析、ETL（提取、转换、加载）、机器学习等需要处理大规模数据的场景。

5. API设计：
   - Java I/O流：设计上比较底层，需要处理较多的细节，如异常处理、资源管理等。
   - Spark I/O流：设计上更加高层次和面向用户，提供简单易用的接口，并且集成了许多大数据处理功能，如数据源连接器、数据转换、聚合等。


Spark广播变量：Spark广播变量用来把变量在所有节点的内存之间进行共享，在每个机器上缓存一个只读的变量，而不是为机器上的每个任务都生成一个副本。

import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object BroadcastVariablesTest {
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setAppName("wc").setMaster("local[*]")
    val sc: SparkContext = new SparkContext(conf)
    sc.setLogLevel("WARN")

    //不使用广播变量
    val kvFruit: RDD[(Int, String)] = sc.parallelize(List((1,"apple"),(2,"orange"),(3,"banana"),(4,"grape")))
    val fruitMap: collection.Map[Int, String] =kvFruit.collectAsMap
    //scala.collection.Map[Int,String] = Map(2 -> orange, 4 -> grape, 1 -> apple, 3 -> banana)
    val fruitIds: RDD[Int] = sc.parallelize(List(2,4,1,3))
    //根据水果编号取水果名称
    val fruitNames: RDD[String] = fruitIds.map(x=>fruitMap(x))
    fruitNames.foreach(println)
    //注意:以上代码看似一点问题没有,但是考虑到数据量如果较大,且Task数较多,
    //那么会导致,被各个Task共用到的fruitMap会被多次传输
    //应该要减少fruitMap的传输,一台机器上一个,被该台机器中的Task共用即可
    //如何做到?---使用广播变量
    //注意:广播变量的值不能被修改,如需修改可以将数据存到外部数据源,如MySQL、Redis
    println("=====================")
    val BroadcastFruitMap: Broadcast[collection.Map[Int, String]] = sc.broadcast(fruitMap)
    val fruitNames2: RDD[String] = fruitIds.map(x=>BroadcastFruitMap.value(x))
    fruitNames2.foreach(println)

  }
}

Spark Executor：
Executor是Spark运行时环境中的一个核心组件，它在集群中的每个工作节点上启动，并负责执行由Driver程序分发的任务。Executor在整个Spark应用程序生命周期内一直存在，并在该应用程序的各个阶段处理任务。

它们的主要职责包括：
1.执行代码：Executor接收并执行由Driver分配的任务。每个任务通常是对数据集的一部分进行计算。
2.数据存储：Executor在内存中缓存数据，这些数据可以是计算结果或中间结果，以提高后续操作的效率，避免重复计算和磁盘I/O。

Executor的主要功能：
1.任务执行：当Spark应用程序提交作业时，Driver会将任务分配给Executor。Executor接收到任务后，使用其分配的资源（CPU和内存）来执行这些任务。
2.数据缓存：Executor可以在内存中缓存数据，以便在同一应用程序的后续任务中重用。这有助于减少重复计算和I/O操作，提高应用程序的性能。
3.报告状态：Executor会定期向Driver报告其任务的执行状态，包括任务的进度、完成情况和遇到的错误。

Executor的配置：
在提交Spark作业时，用户可以通过多种配置参数来调整Executor的行为，以优化应用程序的性能和资源利用率。
常见的配置参数包括：
spark.executor.instances：指定集群中启动的Executor实例的数量。
spark.executor.cores：指定每个Executor分配的CPU核心数。这决定了每个Executor可以并行运行的任务数量。
spark.executor.memory：指定每个Executor分配的内存大小。这决定了Executor可以缓存的数据量和运行任务时可用的内存。

示例配置：
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \  # 启动10个Executor
  --executor-cores 4 \  # 每个Executor使用4个核心
  --executor-memory 8G \  # 每个Executor使用8 GB内存
  /path/to/examples.jar

在这个示例中：
1.我们启动了10个Executor。
2.每个Executor分配了4个CPU核心和8 GB内存。

Executor的生命周期
1.启动：当Spark应用程序启动时，Driver请求集群管理器（如YARN或Mesos）为应用程序分配资源。集群管理器在工作节点上启动Executor。
2.任务分配和执行：Driver将任务分配给Executor，Executor在其分配的资源范围内并行执行这些任务。
3.任务完成和资源释放：当所有任务完成后，Executor向Driver报告任务状态并释放资源。如果Executor崩溃或任务失败，Driver可以请求重新启动Executor并重新分配任务。

Spark DAG：
DAG是由节点和有向边组成的图，其中：
节点：表示计算操作，如数据读取、转换（如map、filter）和输出操作。
有向边：表示操作之间的数据依赖关系，指明数据流从一个操作到另一个操作的方向。

DAG的无环性（Acyclic）意味着图中没有循环，即数据流不会回到之前的节点。

在Spark中，DAG用于表示从数据读取到结果输出的整个计算过程。具体来说，当用户在Spark应用程序中编写一系列的转换（Transformations）和行动（Actions）操作时，Spark会将这些操作转化为DAG。

生成DAG的过程
用户操作：用户编写的代码中包含一系列的RDD（Resilient Distributed Dataset）转换和行动操作。
逻辑计划：Spark根据用户操作生成逻辑计划，逻辑计划是一组表示数据依赖关系的有向无环图。
物理计划：Spark将逻辑计划优化并转化为物理计划，物理计划是具体的执行计划，包括分阶段（stages）执行的任务。
任务调度：Spark将物理计划中的每个阶段转化为一组并行任务，并将这些任务分发到集群中的Executor执行。

转换和行动
在理解DAG之前，了解转换和行动操作是非常重要的。
转换（Transformations）：这些操作（如map、filter、flatMap）用于从一个RDD生成另一个RDD。转换是惰性的（Lazy），即这些操作不会立即执行，只有在遇到行动操作时，Spark才会计算这些转换。
行动（Actions）：这些操作（如collect、count、saveAsTextFile）会触发实际的计算，返回结果或将数据保存到外部存储。

示例
val lines = sc.textFile("hdfs://...")  // 读取数据
val words = lines.flatMap(_.split(" "))  // 转换：分割单词
val pairs = words.map(word => (word, 1))  // 转换：生成键值对
val counts = pairs.reduceByKey(_ + _)  // 转换：按键聚合
counts.saveAsTextFile("hdfs://...")  // 行动：保存结果

节点：表示各个RDD和操作。
有向边：表示RDD之间的数据依赖关系。
读取数据：lines
分割单词：words
生成键值对：pairs
按键聚合：counts
保存结果：最终行动操作

Spark 累加器：
累加器（Accumulator）是Spark提供的一种变量，用于在集群的所有节点上进行聚合操作。累加器主要用于在转换操作中对数值进行累加，且只支持“添加”操作。常见的使用场景包括统计计数和聚合信息。

Spark 提供了几种内置的累加器类型，例如：
LongAccumulator：用于累加 Long 类型的值。
DoubleAccumulator：用于累加 Double 类型的值。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.util.LongAccumulator

val conf = new SparkConf().setAppName("Accumulator Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个Long类型的累加器
val acc: LongAccumulator = sc.longAccumulator("My Long Accumulator")

// 创建一个RDD
val data = sc.parallelize(1 to 10)

// 使用累加器在转换操作中累加值
data.foreach(x => acc.add(x))

// 打印累加器的值
println(s"Accumulated value: ${acc.value}")  // 输出: Accumulated value: 55

自定义累加器：有时内置的累加器无法满足所有需求，Spark 允许用户自定义累加器。自定义累加器需要继承 AccumulatorV2 类，并实现相关方法。

示例代码：
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.util.AccumulatorV2

// 自定义字符串累加器
class StringAccumulator extends AccumulatorV2[String, String] {
  private var _value: String = ""

  // 判断累加器是否为空
  def isZero: Boolean = _value.isEmpty

  // 复制累加器
  def copy(): AccumulatorV2[String, String] = {
    val newAcc = new StringAccumulator
    newAcc._value = this._value
    newAcc
  }

  // 重置累加器
  def reset(): Unit = {
    _value = ""
  }

  // 添加值
  def add(v: String): Unit = {
    _value += v
  }

  // 合并两个累加器
  def merge(other: AccumulatorV2[String, String]): Unit = {
    other match {
      case o: StringAccumulator => _value += o._value
      case _ => throw new UnsupportedOperationException(
        "Cannot merge StringAccumulator with different type"
      )
    }
  }

  // 返回累加器的值
  def value: String = _value
}

// 使用自定义字符串累加器
val conf = new SparkConf().setAppName("Custom Accumulator Example").setMaster("local")
val sc = new SparkContext(conf)

// 注册自定义累加器
val stringAcc = new StringAccumulator
sc.register(stringAcc, "String Accumulator")

// 创建一个RDD
val data = sc.parallelize(Seq("Hello", " ", "World", "!"))

// 使用累加器在转换操作中累加字符串
data.foreach(str => stringAcc.add(str))

// 打印累加器的值
println(s"Accumulated value: ${stringAcc.value}")  // 输出: Accumulated value: Hello World!

自定义累加器方法说明
isZero：判断累加器是否为空。
copy：创建累加器的副本。
reset：重置累加器。
add：添加值到累加器。
merge：合并两个累加器。
value：返回累加器的值。


Spark工程化代码方法：
Controller Service DAO 

Spark SQL:Spark SQL 是 Spark 用来操作结构化数据的组件。
通过 Spark SQL，用户可以使用 SQL或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。

Spark SQL常用语句如下：
SELECT：用于从数据表中查询数据。
INSERT INTO：用于向表中插入数据。
UPDATE：用于更新数据表中的数据。
DELETE FROM：用于从数据表中删除数据。
CREATE TABLE：用于创建一个新的数据表。
ALTER TABLE：用于修改现有的数据表结构。
DROP TABLE：用于删除一个数据表。
LOAD DATA：用于加载本地文件系统中的数据文件到数据表中。
SHOW COLUMNS：用于显示表中的所有列。
COUNT：用于计算表中的行数。
GROUP BY：用于根据指定的列对数据进行分组。
ORDER BY：用于根据指定的列对数据进行排序。
WHERE：用于对数据进行过滤。

Spark SQL的数据抽象：
Spark SQL数据抽象可以分为两类：
① DataFrame：DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库的二维表格，带有 Schema 元信息(可以理解为数据库的列名和类型)。DataFrame = RDD ＋ 泛型 + SQL 的操作 + 优化
② DataSet：DataSet是DataFrame的进一步发展，它比RDD保存了更多的描述信息，概念上等同于关系型数据库中的二维表，它保存了类型信息，是强类型的，提供了编译时类型检查。调用 Dataset 的方法先会生成逻辑计划，
然后被 spark 的优化器进行优化，最终生成物理计划，然后提交到集群中运行！DataFrame = Dateset[Row]

RDD[Person]：以 Person 为类型参数，但不了解其内部结构。
DataFrame：提供了详细的结构信息 schema 列的名称和类型。这样看起来就像一张表了。
DataFrame = RDD - 泛型 + Schema + SQL + 优化

DataSet[Person]：不光有 schema 信息，还有类型信息。
DataSet = DataFrame + 泛型
DataSet = RDD + Schema + SQL + 优化


Spark Streaming:Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。
Spark Streaming 的特点：
易用：可以像编写离线批处理一样去编写流式程序，支持 java/scala/python 语言。
容错：SparkStreaming 在没有额外代码和配置的情况下可以恢复丢失的工作。
易整合到 Spark 体系：流式处理与批处理和交互式查询相结合。

Spark Streaming常用语句：
输出操作：writeStream.format("parquet")或writeStream.format("kafka")等输出操作。
定义输出路径：.option("path", "path/to/destination/dir")或.option("kafka.bootstrap.servers", "host1:port1,host2:port2")
等参数定义输出路径等参数。
指定输出topic：.option("topic", "updates")等参数指定输出对应的Kafka topic。
启动流计算：使用start()操作来执行查询语句。

Spark Streaming工作流程：
① Spark Streaming 中，会有一个接收器组件 Receiver，作为一个长期运行的 task 跑在一个 Executor 上，Receiver 接收外部的数据流形成 input DStream。
② DStream 会被按照时间间隔划分成一批一批的 RDD，当批处理间隔缩短到秒级时，便可以用于处理实时数据流（时间间隔的大小可以由参数指定，一般设在 500 毫秒到几秒之间）。
③ 对 DStream 进行操作就是对 RDD 进行操作，计算处理的结果可以传给外部系统。
④ 接受到实时数据后，给数据分批次，然后传给 Spark Engine 处理最后生成该批次的结果。

Spark Streaming的实时性：
Spark Streaming 将流式计算分解成多个 Spark Job，对于每一时间段数据的处理都会经过 Spark DAG 图分解
以及 Spark 的任务集的调度过程。
对于目前版本的 Spark Streaming 而言，其最小的 Batch Size 的选取在 0.5~5 秒钟之间。
所以 Spark Streaming 能够满足流式准实时计算场景，对实时性要求非常高的如高频实时交易场景则不太适合。

Structure Streaming：
Structured Streaming 是一个基于 Spark SQL 引擎的可扩展、容错的流处理引擎。
统一了流、批的编程模型，你可以使用静态数据批处理一样的方式来编写流式计算操作。
并且支持基于 event_time 的时间窗口的处理逻辑。
Structured Streaming 最核心的思想就是将实时到达的数据看作是一个不断追加的 unbound table 无界表，
到达流的每个数据项(RDD)就像是表中的一个新行被附加到无边界的表中，
这样用户就可以用静态结构化数据的批处理查询方式进行流计算，如可以使用 SQL 对到来的每一行数据进行实时查询处理。

Transform实时计算：
case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)
val df: DataFrame = ... // streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }
val ds: Dataset[DeviceData] = df.as[DeviceData]    // streaming Dataset with IOT device data
// Select the devices which have signal more than 10
df.select("device").where("signal > 10")      // using untyped APIs
ds.filter(_.signal > 10).map(_.device)         // using typed APIs
// Running count of the number of updates for each device type
df.groupBy("deviceType").count()                 // using untyped API
// Running average signal for each device type
import org.apache.spark.sql.expressions.scalalang.typed
ds.groupByKey(_.deviceType).agg(typed.avg(_.signal))    // using typed API

计算结果可以选择输出到多种设备并进行如下设定：
output mode：以哪种方式将 result table 的数据写入 sink,即是全部输出 complete 还是只输出新增数据；
format/output sink 的一些细节：数据格式、位置等。如 console；
query name：指定查询的标识。类似 tempview 的名字；
trigger interval：触发间隔，如果不指定，默认会尽可能快速地处理数据；
checkpointLocation：一般是 hdfs 上的目录。注意：Socket 不支持数据恢复，如果设置了，第二次启动会报错，Kafka 支持。

Spark MLlib:
MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

Spark GraphX:GraphX 是 Spark 面向图计算提供的框架与算法库。Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。

Spark运行流程：
1.SparkContext 向资源管理器注册并向资源管理器申请运行 Executor
2.资源管理器分配 Executor，然后资源管理器启动 Executor
3.Executor 发送心跳至资源管理器
4.SparkContext 构建 DAG 有向无环图
5.将 DAG 分解成 Stage（TaskSet）
6.把 Stage 发送给 TaskScheduler
7.Executor 向 SparkContext 申请 Task
8.TaskScheduler 将 Task 发送给 Executor 运行
9.同时 SparkContext 将应用程序代码发放给 Executor
10.Task 在 Executor 上运行，运行完毕释放所有资源


Flink 大数据批处理运算+流式运算：
Apache Flink 是一个开源的、分布式的大数据处理框架，可以处理批量数据和实时数据处理。它被设计成快速、可扩展和可靠，并提供统一的批量和流数据处理编程模型。

Flink 的一些关键特性包括：
低延迟处理：Flink 提供了实时数据流的低延迟处理，适用于实时分析、欺诈检测和网络监测等场景。
容错性：Flink 提供了内置的容错性，确保数据处理即使在节点故障的情况下也可以继续。
可扩展性：Flink 可以水平扩展，添加额外的节点来处理不断增加的数据。
高效数据处理：Flink 通过其流水线数据处理架构提供高效的数据处理，减少了中间数据存储的需求。
易于使用：Flink 提供了一个用户友好的编程模型，支持 Java 和 Scala，并提供类似 SQL 的查询语言进行流数据处理。

Flink的运行中必须依靠Hadoop组件吗？
Flink是一个开源的流处理和批处理框架，与Apache Hadoop不同，它不直接依赖Hadoop。
尽管Flink在某些情况下可以与Hadoop生态系统中的一些组件集成，但Flink本身并不依赖Hadoop。

Flink 应用程序结构：
Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、
基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、RabbitMQ 等，
当然你也可以定义自己的 source。

Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window 
/ WindowAll / Union / Window join / Split / Select等，操作很多，可以将数据转换计算成你想要的数据。

Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：
写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 
Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，
同理你也可以定义自己的 sink。

2. Flink 并行数据流
Flink 程序在执行的时候，会被映射成一个 Streaming Dataflow，一个 Streaming Dataflow 是由一组 Stream 和 Transformation Operator 组成的。
在启动时从一个或多个 Source Operator 开始，结束于一个或多个 Sink Operator。
Flink 程序本质上是并行的和分布式的，在执行过程中，一个流(stream)包含一个或多个流分区，
而每一个 operator 包含一个或多个 operator 子任务。操作子任务间彼此独立，在不同的线程中执行，甚至是在不同的机器或不同的容器上。
operator 子任务的数量是这一特定 operator 的并行度。相同程序中的不同 operator 有不同级别的并行度。

一个 Stream 可以被分成多个 Stream 的分区，也就是 Stream Partition。一个 Operator 也可以被分为多个 Operator Subtask。
每一个 Operator Subtask 都是在不同的线程当中独立执行的。一个 Operator 的并行度，就等于 Operator Subtask 的个数。
而一个 Stream 的并行度就等于它生成的 Operator 的并行度。

数据在两个 operator 之间传递的时候有两种模式：
One to One 模式：两个 operator 用此模式传递的时候，会保持数据的分区数和数据的排序；如上图中的 Source1 到 Map1，它就保留的 Source 的分区特性，以及分区元素处理的有序性。
Redistributing （重新分配）模式：这种模式会改变数据的分区数；每个一个 operator subtask 会根据选择 transformation 把数据发送到不同的目标 subtasks,比如 keyBy()会通过 hashcode 重新分区,broadcast()和 rebalance()方法会随机重新分区；

3. Task 和 Operator chain
Flink的所有操作都称之为Operator，客户端在提交任务的时候会对Operator进行优化操作，
能进行合并的Operator会被合并为一个Operator，合并后的Operator称为Operator chain，实际上就是一个执行链，
每个执行链会在TaskManager上一个独立的线程中执行。


4. 任务调度与执行
当Flink执行executor会自动根据程序代码生成DAG数据流图；
ActorSystem创建Actor将数据流图发送给JobManager中的Actor；
JobManager会不断接收TaskManager的心跳消息，从而可以获取到有效的TaskManager；
JobManager通过调度器在TaskManager中调度执行Task（在Flink中，最小的调度单元就是task，对应就是一个线程）；
在程序运行过程中，task与task之间是可以进行数据传输的。

Job Client：
主要职责是提交任务, 提交后可以结束进程, 也可以等待结果返回；
Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点；
Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。
执行完成后，Job Client 将结果返回给用户。

JobManager：
主要职责是调度工作并协调任务做检查点；
集群中至少要有一个 master，master 负责调度 task，协调checkpoints 和容错；
高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是standby；
Job Manager 包含 Actor System、Scheduler、CheckPoint三个重要的组件；
JobManager从客户端接收到任务以后, 首先生成优化过的执行计划, 再调度到TaskManager中执行。

TaskManager：
主要职责是从JobManager处接收任务, 并部署和启动任务, 接收上游的数据并处理；
Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点；
TaskManager在创建之初就设置好了Slot, 每个Slot可以执行一个任务。

5. 任务槽和槽共享
每个TaskManager是一个JVM的进程, 可以在不同的线程中执行一个或多个子任务。
为了控制一个worker能接收多少个task。worker通过task slot来进行控制（一个worker至少有一个task slot）。

1) 任务槽
每个task slot表示TaskManager拥有资源的一个固定大小的子集。
flink将进程的内存进行了划分到多个slot中。
图中有2个TaskManager，每个TaskManager有3个slot的，每个slot占有1/3的内存。
内存被划分到不同的slot之后可以获得如下好处:
TaskManager最多能同时并发执行的任务是可以控制的，那就是3个，因为不能超过slot的数量。
slot有独占的内存空间，这样在一个TaskManager中可以运行多个不同的作业，作业之间不受影响。

2) 槽共享
默认情况下，Flink允许子任务共享插槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个槽可以保存作业的整个管道。允许插槽共享有两个主要好处：
只需计算Job中最高并行度（parallelism）的task slot,只要这个满足，其他的job也都能满足。
资源分配更加公平，如果有比较空闲的slot可以将更多的任务分配给它。图中若没有任务槽共享，负载不高的Source/Map等subtask将会占据许多资源，而负载较高的窗口subtask则会缺乏资源。
有了任务槽共享，可以将基本并行度（base parallelism）从2提升到6.提高了分槽资源的利用率。同时它还可以保障TaskManager给subtask的分配的slot方案更加公平。

FLink的Time和Window：
流式：就是数据源源不断的流进来，也就是数据没有边界，但是我们计算的时候必须在一个有边界的范围内进行，
所以这里面就有一个问题，边界怎么确定？无非就两种方式，根据时间段或者数据量进行确定，
根据时间段就是每隔多长时间就划分一个边界，根据数据量就是每来多少条数据划分一个边界，
Flink 中就是这么划分边界的，本文会详细讲解。

实时：就是数据发送过来之后立马就进行相关的计算，然后将结果输出。这里的计算有两种：
一种是只有边界内的数据进行计算，这种好理解，比如统计每个用户最近五分钟内浏览的新闻数量，
就可以取最近五分钟内的所有数据，然后根据每个用户分组，统计新闻的总数。
另一种是边界内数据与外部数据进行关联计算，比如：统计最近五分钟内浏览新闻的用户都是来自哪些地区，
这种就需要将五分钟内浏览新闻的用户信息与 hive 中的地区维表进行关联，然后在进行相关计算。

Flink的Window API提供了以下几种窗口函数：
Tumbling Windows：这种窗口函数将数据分成固定大小的窗口。每个窗口内的数据是一组数据。
Sliding Windows：这种窗口函数将数据分成固定大小的窗口，并且窗口可以滑动。每个窗口内的数据也是一组数据。
Session Windows：这种窗口函数将数据分成会话窗口。每个会话窗口内的数据是一组数据。会话窗口是根据时间或数据之间的距离来定义的。
Global Windows：这种窗口函数将数据分成一个全局窗口。每个全局窗口内的数据是一组数据。全局窗口的大小取决于数据的数量。

此外，Flink还提供了以下几种窗口函数的方法：
countWindow(size)：将数据分成指定大小的窗口，每个窗口内的数据是一组数据。
timeWindow(size, slide)：将数据按照时间分成指定大小和滑动间隔的窗口，每个窗口内的数据是一组数据。
sessionWindow(size, slide)：将数据按照时间分成指定大小和滑动间隔的会话窗口，每个会话窗口内的数据是一组数据。
globalWindow()：将数据分成一个全局窗口，每个全局窗口内的数据是一组数据。


Time：
在Flink中，如果以时间段划分边界的话，那么时间就是一个极其重要的字段。
Flink中的时间有三种类型：

Event Time：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，
每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。
Ingestion Time：是数据进入Flink的时间。
Processing Time：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是Processing Time。

例如，一条日志进入Flink的时间为2021-01-22 10:00:00.123，到达Window的系统时间为2021-01-22 10:00:01.234，
日志的内容：2021-01-06 18:37:15.624 INFO Fail over to rm2
对于业务来说，要统计1min内的故障日志个数，哪个时间是最有意义的？—— eventTime，因为我们要根据日志的生成时间进行统计。

此外，Flink还提供了以下几种时间函数的方法：

timeWindow(size, slide)：将数据按照时间分成指定大小和滑动间隔的窗口，每个窗口内的数据是一组数据。
sessionWindow(size, slide)：将数据按照时间分成指定大小和滑动间隔的会话窗口，每个会话窗口内的数据是一组数据。
countWindow(size)：将数据分成指定大小的窗口，每个窗口内的数据是一组数据。
globalWindow()：将数据分成一个全局窗口，每个全局窗口内的数据是一组数据。


总之，Flink 是一个强大的大数据处理框架，适用于批量数据和实时数据处理。它提供了低延迟处理、容错性、可扩展性


ClickHouse 大数据列式存储：
ClickHouse是一个开源的，用于联机分析（OLAP）的列式数据库管理系统（DBMS-database manager system）, 它是面向列的，
并允许使用SQL查询，实时生成分析报告。ClickHouse最初是一款名为Yandex.Metrica的产品，主要用于WEB流量分析。
ClickHouse的全称是Click Stream，Data WareHouse，简称ClickHouse。

ClickHouse不是一个单一的数据库,它允许在运行时创建表和数据库，加载数据和运行查询，而无需重新配置和重新启动服务器。
ClickHouse同时支持列式存储和数据压缩，这是对于一款高性能数据库来说是必不可少的特性。
一个非常流行的观点认为，如果你想让查询变得更快，最简单且有效的方法是减少数据扫描范围和数据传输时的大小，
而列式存储和数据压缩就可以帮助我们实现上述两点，列式存储和数据压缩通常是伴生的，因为一般来说列式存储是数据压缩的前提。

ClickHouse的特性包括：
1.完备的DBMS功能：ClickHouse具备DDL（数据定义语言）、DML（数据操作语言）、分布式管理、权限控制、数据备份与恢复等功能。
2.数据压缩：ClickHouse具有数据压缩功能，可以减少数据存储空间和网络带宽的使用。
3.数据的磁盘存储：ClickHouse将数据存储在磁盘上，不同于其他列式数据库大多需要在内存中处理。
4.多核心并行处理：ClickHouse具有多核心并行处理能力，可以充分利用多核CPU的性能。
5.多服务器分布式处理：ClickHouse支持多服务器分布式处理，可以处理大规模的数据。
6.支持SQL：ClickHouse支持SQL语言，可以方便地进行数据查询和操作。
7.向量引擎：ClickHouse采用向量引擎，能够高效地处理大规模的向量数据。
8.实时数据更新：ClickHouse支持实时数据更新，可以快速地响应数据变化。
9.索引：ClickHouse支持索引，可以提高查询效率。
10.适合在线查询：ClickHouse适合在线查询，可以快速地返回查询结果。
11.支持近似计算：ClickHouse支持近似计算，可以在一定程度上减少计算误差。
12.支持数据复制和数据完整性：ClickHouse支持数据复制和数据完整性，可以保证数据的可靠性和一致性。
13.角色的访问控制：ClickHouse提供角色的访问控制功能，可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。

例：在clickhouse中进行模糊查询 SELECT * FROM table_name WHERE column_name LIKE '%keyword%';

Zookeeper 大数据分布式协调服务
Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，
它负责存储和管理大家都关心的数据，然后接受观察者的注册，
一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。

特点：
1.Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。
2.集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所 以Zookeeper适合安装奇数台服务器。
3.全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。
4.更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行。
5.数据更新原子性，一次数据更新要么成功，要么失败。
6.实时性，在一定时间范围内，Client能读到最新数据。

zookeeper的选举机制：
半数机制，超过半数的投票通过，即通过。
（1）第一次启动选举规则：
投票过半数时，服务器 id 大的胜出
（2）第二次启动选举规则：
①EPOCH 大的直接胜出
②EPOCH 相同，事务 id 大的胜出
③事务 id 相同，服务器 id 大的胜出

zookeeper可以保证数据的一致性。底层采用Paxos算法，对分布式系统中某个数据达成一致。

Paxos算法流程：
（1）Prepare: Proposer生成全局唯一且递增的Proposal ID，向所有Acceptor发送Propose请求，这里无需携带提案内容，只携带Proposal ID即可。
（2）Promise: Acceptor收到Propose请求后，做出“两个承诺，一个应答”。
➢ 不再接受Proposal ID小于等于（注意：这里是<= ）当前请求的Propose请求。
➢ 不再接受Proposal ID小于（注意：这里是< ）当前请求的Accept请求。
➢ 不违背以前做出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。
（3）Propose: Proposer收到多数Acceptor的Promise应答后，从应答中选择Proposal ID最大的提案的Value，作为本次要发起的提案。
如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。
然后携带当前Proposal ID，向所有Acceptor发送Propose请求。
（4）Accept: Acceptor收到Propose请求后，在不违背自己之前做出的承诺下，接受并持久化当前Proposal ID和提案Value。
（5）Learn: Proposer收到多数Acceptor的Accept后，决议形成，将形成的决议发送给所有Learner。

PS：如果系统中有一个以上的 Proposer，多个 Proposers 相互争夺 Acceptor，针对这种情况，一种改进的 Paxos 算法被提出：从系统中选
出一个节点作为 Leader，只有 Leader 能够发起提案。这样能避免活锁的情况出现。改进后的算法被称作ZAB协议。
Zookeeper保证了CAP理论中的CP，即一致性和分区容错性。Zookeeper不能保证每次服务请求的可用性，进行leader选举时集群不可用。

ZAB过程：
（1）客户端发起一个写操作请求。
（2）Leader服务器将客户端的请求转化为事务Proposal 提案，同时为每个Proposal 分配一个全局的ID，即zxid。
（3）Leader服务器为每个Follower服务器分配一个单独的队列，然后将需要广播的 Proposal依次放到队列中去，并且根据FIFO策略进行消息发送。
（4）Follower接收到Proposal后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向Leader反馈一个Ack响应消息。
（5）Leader接收到超过半数以上Follower的Ack响应消息后，即认为消息发送成功，可以发送commit消息。
（6）Leader向所有Follower广播commit消息，同时自身也会完成事务提交。Follower 接收到commit消息后，会将上一条事务提交。
（7）Zookeeper采用Zab协议的核心，就是只要有一台服务器提交了Proposal，就要确保所有的服务器最终都能正确提交Proposal。


Flume 大数据数据采集：Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。
Flume最主要的作用是实时读取服务器本地的文件，将数据写入到HDFS。

Flume基础结构：
1 Agent
Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。
Agent 主要有 3 个部分组成，Source、Channel、Sink。

2 Source
Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，
包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy。

3 Sink
Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。
Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。

4 Channel
Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。
Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作。
Flume 自带两种 Channel：Memory Channel 和 File Channel。
Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。
如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。
File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。

5 Event
传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。
Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组。

kafka 大数据消息队列

传统的消息队列的主要应用场景包括：缓存/消峰、解耦和异步通信。
两种模式：点对点模式（消费者主动拉取数据，消息收到后清除数据），发布/订阅模式（可以有多个主题，消费者通过主题消费数据，每个消费者互相独立，都可以消费的到数据）

kafka特点：
1.为方便扩展，并提高吞吐量，一个topic分为多个partition
2.配合分区的设计，提出消费者组的概念，组内每个消费者并行消费
3.为提高可用性，为每个partition增加若干副本，类似NameNode HA
4. ZK中记录谁是leader，Kafka2.8.0以后也可以配置不采用ZK

Azkaban 大数据任务调度
Azkaban是由Linkedin公司推出的一个批量工作流任务调度器，主要用于在一个工作流内以一个特定的顺序运行一组工作和流程，
它的配置是通过简单的<key, value>对的方式，通过配置中的dependencies来设置依赖关系。
Azkaban使用job配置文件建立任务之间的依赖关系，并提供一个易于使用的Web用户界面维护和跟踪你的工作流。


常见工作流调度系统:
1.简单的任务调度：直接使用 Linux 的 Crontab 来定义；
2.复杂的任务调度：开发调度平台或使用现成的开源调度系统，比如 Ooize、Azkaban、Airflow、DolphinScheduler 等。

Azkaban由三个关键组件构成：
元数据
AzkabanWebServer
AzkabanExecutorServer

元数据
Azkaban使用关系型数据库存储元数据和执行状态。

AzkabanWebServer
项目管理：项目、项目权限以及上传的文件。
作业状态：跟踪执行流程以及执行程序正在运行的流程。
以前的流程/作业：通过以前的作业和流程执行以及访问其日志文件进行搜索。
计划程序：保留计划作业的状态。
SLA：保持所有的SLA规则

AzkabanExecutorServer
访问项目：从数据库检索项目文件。
执行流程/作业：检索和更新正在执行的作业流的数据
日志：将作业和工作流的输出日志存储到数据库中。
交互依赖关系：如果一个工作流在不同的执行器上运行，它将从数据库中获取状态。

AzkabanWebServer
AzkabanWebServer是整个Azkaban工作流系统的主要管理者，它负责Project管理、用户登录认证、定时执行工作流、跟踪工作流执行进度等
一系列任务。同时，它还提供Web服务操作接口，利用该接口，用户可以使用curl或其他Ajax的方式，来执行Azkaban的相关操作。
操作包括：用户登录、创建Project、上传Workflow、执行Workflow、查询Workflow的执行进度、杀掉Workflow等一系列操作，
且这些操作的返回结果均是JSON格式。并且Azkaban使用方便，Azkaban使用以.job为后缀名的键值属性文件来定义工作流中的各个任务，
以及使用dependencies属性来定义作业间的依赖关系链。这些作业文件和关联的代码最终以*.zip的方式通过Azkaban UI上传到Web服务器上。

AzkabanExecutorServer
以前版本的Azkaban在单个服务中具有AzkabanWebServer和AzkabanExecutorServer功能，
目前Azkaban已将AzkabanExecutorServer分离成独立的服务器，拆分AzkabanExecutorServer的原因有如下几点：

某个任务流失败后，可以更方便将其重新执行
便于Azkaban升级
AzkabanExecutorServer主要负责具体的工作流提交、执行，可以启动多个执行服务器，它们通过关系型数据库来协调任务的执行。

作业流执行过程：
WebServer根据内存中缓存的各Executor的资源（WebServer有一个线程会遍历各个Active Executor，
去发送Http请求获取其资源状态信息缓存到内存中），按照选择策略（包括executor资源状态、最近执行流个数等）选择一个Executor下发作业流；
Executor判断是否设置作业粒度分配，如果未设置作业粒度分配，则在当前Executor执行所有作业；
如果设置了作业粒度分配，则当前节点会成为作业分配的决策者，即分配节点；
分配节点从Zookeeper获取各个Executor的资源状态信息，然后根据策略选择一个Executor分配作业；
被分配到作业的Executor即成为执行节点，执行作业，然后更新数据库。


Atlas 大数据元数据管理
Apache Atlas 为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对
这些资产进行分类和管理，形成数据字典。并为数据分析师和数据治理团队，提供围绕这些
数据资产的协作功能。
注：数据字典：可以查到 hive 库的释义，表的介绍以及字段的解释和说明。
1.表与表之间的血缘依赖
2.字段与字段之间的血缘依赖

Atlas的使用：
Atlas 的使用相对简单，其主要工作是同步各服务（主要是 Hive）的元数据，
并构建元数据实体之间的关联关系，然后对所存储的元数据建立索引，最终未用户提供数据血缘查看。

Apache Atlas为Hadoop的元数据治理提供了以下特性：

数据分类
- 为元数据导入或定义业务导向的分类注释
- 定义，注释，以及自动捕获数据集和底层元素之间的关系
- 导出元数据到第三方系统

集中审计
- 捕获与所有应用，过程以及与数据交互的安全访问信息
- 捕获执行，步骤，活动等操作的信息

搜索与血缘
- 预定义的导航路径用来探索数据分类以及审计信息
- 基于文本的搜索特性来快速和准确的定位相关联的数据和审计事件
- 对数据集血缘关系的可视化浏览使用户可以下钻到操作，安全以及数据起源相关的信息

安全与策略引擎
- 基于数据分类模式，属性以及角色的运行时合理合规策略
- 基于分类-预测的高级策略定义以防止数据推导
- 基于cell的属性和值的行/列级别的masking

Core
- Type System: Atlas 允许用户为他们想要管理的元数据对象定义一个模型。该模型由称为 "类型" 的定义组成。
"类型" 的 实例被称为 "实体" 表示被管理的实际元数据对象。类型系统是一个组件，允许用户定义和管理类型和实体。
由 Atlas 管理的所有元数据对象（例如Hive表）都使用类型进行建模，并表示为实体。要在 Atlas 中存储新类型的元数据，
需要了解类型系统组件的概念。

- Ingest/Export：Ingest 组件允许将元数据添加到 Atlas。类似地，Export 组件暴露由 Atlas 检测到的元数据更改，
以作为事件引发，消费者可以使用这些更改事件来实时响应元数据更改。

- Graph Engine：在内部，Atlas 通过使用图形模型管理元数据对象。以实现元数据对象之间的巨大灵活性和丰富的关系。
图形引擎是负责在类型系统的类型和实体之间进行转换的组件，以及基础图形模型。除了管理图形对象之外，
图形引擎还为元数据对象创建适当的索引，以便有效地搜索它们。

- Titan：目前，Atlas 使用 Titan 图数据库来存储元数据对象。 Titan 使用两个存储：默认情况下元数据存储配置为 HBase ，
索引存储配置为 Solr。也可以通过构建相应的配置文件使用BerkeleyDB存储元数据存储 和使用ElasticSearch存储 Index。
元数据存储用于存储元数据对象本身，索引存储用于存储元数据属性的索引，其允许高效搜索。

Integration
用户可以使用两种方法管理 Atlas 中的元数据：
- API： Atlas 的所有功能都可以通过 REST API 提供给最终用户，允许创建，更新和删除类型和实体。
它也是查询和发现通过 Atlas 管理的类型和实体的主要方法。
- Messaging：除了 API 之外，用户还可以选择使用基于 Kafka 的消息接口与 Atlas 集成。
这对于将元数据对象传输到 Atlas 以及从 Atlas 使用可以构建应用程序的元数据更改事件都非常有用。
如果希望使用与 Atlas 更松散耦合的集成，这可以允许更好的可扩展性，可靠性等，消息传递接口是特别有用的。
Atlas 使用 Apache Kafka 作为通知服务器用于钩子和元数据通知事件的下游消费者之间的通信。
事件由钩子(hook)和 Atlas 写到不同的 Kafka 主题。
- ATLAS_HOOK: 来自各个组件的Hook 的元数据通知事件通过写入到名为 ATLAS_HOOK 的 Kafka topic 发送到 Atlas
- ATLAS_ENTITIES：从 Atlas 到其他集成组件（如Ranger）的事件写入到名为 ATLAS_ENTITIES 的 Kafka topic



Metadata source
- Hive：通过hive bridge， atlas可以接入Hive的元数据，包括hive_db/hive_table/hive_column/hive_process
- Sqoop：通过sqoop bridge，atlas可以接入关系型数据库的元数据，包括sqoop_operation_type/ sqoop_dbstore_usage/sqoop_process/sqoop_dbdatastore
- Falcon：通过falcon bridge，atlas可以接入Falcon的元数据，包括falcon_cluster/falcon_feed/falcon_feed_creation/falcon_feed_replication/ falcon_process
- Storm：通过storm bridge，atlas可以接入流式处理的元数据，包括storm_topology/storm_spout/storm_bolt
- HBase: 通过hbase bridge

Atlas集成大数据组件的元数据源需要实现以下两点：

- 首先，需要基于atlas的类型系统定义能够表达大数据组件元数据对象的元数据模型
(例如Hive的元数据模型实现在org.apache.atlas.hive.model.HiveDataModelGenerator)；

- 然后，需要提供hook组件去从大数据组件的元数据源中提取元数据对象，实时侦听元数据的变更并反馈给atlas；

Applications
- Atlas Admin UI: 该组件是一个基于 Web 的应用程序，允许数据管理员和科学家发现和注释元数据。
Admin UI提供了搜索界面和类SQL的查询语言，可以用来查询由 Atlas 管理的元数据类型和对象。
Admin UI 使用 Atlas 的 REST API 来构建其功能。

- Tag Based Policies: Apache Ranger 是针对 Hadoop 生态系统的高级安全管理解决方案，与各种 Hadoop 组件具有广泛的集成。
通过与 Atlas 集成，Ranger 允许安全管理员定义元数据驱动的安全策略，以实现有效的治理。 
Ranger 是由 Atlas 通知的元数据更改事件的消费者。

- Business Taxonomy:从元数据源获取到 Atlas 的元数据对象主要是一种技术形式的元数据。
为了增强可发现性和治理能力，Atlas 提供了一个业务分类界面，允许用户首先定义一组代表其业务域的业务术语，
并将其与 Atlas 管理的元数据实体相关联。业务分类法是一种 Web 应用程序，目前是 Atlas Admin UI 的一部分，
并且使用 REST API 与 Atlas 集成。

 
ATlas类型系统
Atlas 允许用户为他们想要管理的元数据对象定义一个模型。该模型由称为 “类型” (type)的定义组成。被称为 “实体” (entities)的 “类型” 实例表示被管理的实际元数据对象。由 Atlas 管理的所有元数据对象（例如Hive表）都使用类型进行建模，并表示为实体。

- Type：Atlas中的 “类型” 定义了如何存储和访问特定类型的元数据对象。类型表示了所定义元数据对象的一个或多个属性集合。
具有开发背景的用户可以将 “类型” 理解成面向对象的编程语言的 “类” 定义的或关系数据库的 “表模式”。
类型具有元类型，元类型表示 Atlas 中此模型的类型：

- 基本元类型： Int，String，Boolean等
- 集合元类型：例如Array，Map
- Class，Struct，Trait

- Entities：Atlas中的 “实体” 是类 “类型” 的特定值或实例，因此表示真实世界中的特定元数据对象。
  回顾我们的面向对象编程语言的类比，“实例” 是某个 “类” 的 “对象”。
- Attributes：Atlas中的属性还有一些属性，其定义了与类型系统相关的更多概念，包括：
- isComposite - 是否复合
- isIndexable - 是否索引
- isUnique - 是否唯一
- multiplicity - 指示此属性是（必需的／可选的／还是可以是多值）的

Atlas 提供了一些预定义的系统类型：

- Referenceable：此类型表示可使用名为 qualifiedName 的唯一属性搜索的所有实体
- Asset：此类型包含名称，说明和所有者等属性
- Infrastructure：此类型扩展了Referenceable和Asset ，通常可用于基础设施元数据对象（如群集，主机等）的常用超类型
- DataSet：此类型扩展了Referenceable和Asset 。在概念上，它可以用于表示存储数据的类型。在 Atlas 中，hive表，Sqoop RDBMS表等
都是从 DataSet 扩展的类型。扩展 DataSet 的类型可以期望具有模式，它们将具有定义该数据集的属性的属性。
例如， hive_table 中的 columns 属性。另外，扩展 DataSet 的实体类型的实体参与数据转换，
这种转换可以由 Atlas 通过 lineage（或 provenance）生成图形。
- Process：此类型扩展了Referenceable和Asset 。在概念上，它可以用于表示任何数据变换操作。
例如，将原始数据的 hive 表转换为存储某个聚合的另一个 hive 表的 ETL 过程可以是扩展过程类型的特定类型。
流程类型有两个特定的属性，输入和输出。
 
Prometheus 大数据监控系统	
Prometheus 是一个开源的完整监控解决方案，其对传统监控系统的测试和告警模型进行了彻底的颠覆，
形成了基于中央化的规则计算、统一分析和告警的新模型。

Prometheus 基本原理是通过 HTTP 协议周期性抓取被监控组件的状态，这样做的好处是任意组件只要提供 HTTP 接口就可以接入监控系统，
不需要任何 SDK 或者其他的集成过程。这样做非常适合虚拟化环境比如 VM 或者 Docker 。

Prometheus 应该是为数不多的适合 Docker、Mesos、Kubernetes 环境的监控系统之一。

相比于传统监控系统，Prometheus 具有以下优点：

1 易于管理
➢ Prometheus 核心部分只有一个单独的二进制文件，不存在任何的第三方依赖(数据库，缓存等等)。唯一需要的就是本地磁盘，因此不会有潜在级联故障的风险。
➢ Prometheus 基于 Pull 模型的架构方式，可以在任何地方（本地电脑，开发环境，测试环境）搭建我们的监控系统。
➢ 对于一些复杂的情况，还可以使用 Prometheus 服务发现(Service Discovery)的能力动态管理监控目标。

2 监控服务的内部运行状态
Pometheus 鼓励用户监控服务的内部状态，基于 Prometheus 丰富的 Client 库，
用户可以轻松的在应用程序中添加对 Prometheus 的支持，从而让用户可以获取服务和应用内部真正的运行状态。

3 强大的数据模型
所有采集的监控数据均以指标(metric)的形式保存在内置的时间序列数据库当中(TSDB)。
所有的样本除了基本的指标名称以外，还包含一组用于描述该样本特征的标签。
每一条时间序列由指标名称(Metrics Name)以及一组标签(Labels)唯一标识。每条时间序列按照时间的先后顺序存储一系列的样本值。
➢ http_request_status：指标名称(Metrics Name)
➢ {code='200',content_path='/api/path',environment='produment'}：表示维度的
标签，基于这些 Labels 我们可以方便地对监控数据进行聚合，过滤，裁剪。
➢ [value1@timestamp1,value2@timestamp2...]：按照时间的先后顺序 存储的样本值。

4 强大的查询语言 PromQL
Prometheus 内置了一个强大的数据查询语言 PromQL。 通过 PromQL 可以实现对
监控数据的查询、聚合。同时 PromQL 也被应用于数据可视化(如 Grafana)以及告警当中。
通过 PromQL 可以轻松回答类似于以下问题：
➢ 在过去一段时间中 95%应用延迟时间的分布范围？
➢ 预测在 4 小时后，磁盘空间占用大致会是什么情况？
➢ CPU 占用率前 5 位的服务有哪些？(过滤)

5 高效
对于监控系统而言，大量的监控任务必然导致有大量的数据产生。
而 Prometheus 可以高效地处理这些数据，对于单一 Prometheus Server 实例而言它可以处理：
➢ 数以百万的监控指标
➢ 每秒处理数十万的数据点

6 可扩展
可以在每个数据中心、每个团队运行独立的 Prometheus Sevrer。
Prometheus 对于联邦集群的支持，可以让多个 Prometheus 实例产生一个逻辑集群，
当单实例 Prometheus Server 处理的任务量过大时，通过使用功能分区(sharding)+联邦集群(federation)可以对其进行扩展。

7 易于集成
使用 Prometheus 可以快速搭建监控服务，并且可以非常方便地在应用程序中进行集成。
目前支持：Java，JMX，Python，Go，Ruby，.Net，Node.js 等等语言的客户端 SDK，
基于这些 SDK 可以快速让应用程序纳入到 Prometheus 的监控当中，或者开发自己的监控数据收集程序。
同时这些客户端收集的监控数据，不仅仅支持 Prometheus，还能支持 Graphite 这些其他的监控工具。
同时 Prometheus 还支持与其他的监控系统进行集成：Graphite，Statsd，Collected，Scollector，muini，Nagios等。 
Prometheus 社区还提供了大量第三方实现的监控数据采集支持：
JMX，CloudWatch，EC2，MySQL，PostgresSQL，Haskell，Bash，SNMP，
Consul，Haproxy，Mesos，Bind，CouchDB，Django，Memcached，RabbitMQ，
Redis，RethinkDB，Rsyslog 等等。

8 可视化
➢ Prometheus Server 中自带的 Prometheus UI，可以方便地直接对数据进行查询，并且支持直接以图形化的形式展示数据。
同时 Prometheus 还提供了一个独立的基于Ruby On Rails 的 Dashboard 解决方案 Promdash。
➢ 最新的 Grafana 可视化工具也已经提供了完整的 Prometheus 支持，基于 Grafana 可以创建更加精美的监控图标。
➢ 基于 Prometheus 提供的 API 还可以实现自己的监控可视化 UI。

9 开放性
通常来说当我们需要监控一个应用程序时，一般需要该应用程序提供对相应监控系统协议的支持，
因此应用程序会与所选择的监控系统进行绑定。为了减少这种绑定所带来的限制，
对于决策者而言要么你就直接在应用中集成该监控系统的支持，要么就在外部创建单独的服务来适配不同的监控系统。
而对于 Prometheus 来说，使用 Prometheus 的 client library 的输出格式不止支持Prometheus 的格式化数据，
也可以输出支持其它监控系统的格式化数据，比如 Graphite。
因此你甚至可以在不使用 Prometheus 的情况下，采用 Prometheus 的 client library 来让你的应用程序支持监控数据采集。



分布式账本：
分布式账本是一种基于分布式计算和密码学技术的数据记录和交互方式。它是一种去中心化的数据库系统，由多个参与方共同管理和维护。

分布式账本技术的核心是区块链技术。区块链是一个由区块组成的链式结构，每个区块包含了一定的数据记录和相应的验证信息。
每个参与方维护一个完整的账本副本，并通过共识机制来确保所有账本之间的一致性。
通过分布式账本技术，参与方可以在无需信任中心的情况下进行数据交换和交易。数据记录被加密并保存在多个节点中，
这使得数据更加安全、透明和不可篡改。而且，由于数据的分布式特性，分布式账本具有较高的容错性和可扩展性。
分布式账本技术在金融领域的应用最为广泛，例如比特币和以太坊等加密货币就是基于分布式账本技术实现的。
此外，它还被应用于供应链管理、物联网、医疗保健等领域，以提高数据的安全性、可追溯性和可信度。

除了区块链技术，还有其他形式的分布式账本技术，例如分布式共享账本（Distributed Ledger Technology，简称DLT）。
DLT是广义的分布式账本技术，可以包括不仅限于区块链的其他形式。与区块链不同，DLT技术的数据记录可以采用不同的结构和共识机制。
DLT技术的一个重要特点是可选择性的共识，即参与方可以根据自身需求和场景选择适合的共识机制。
DLT技术的应用场景非常广泛，包括金融服务、供应链管理、物联网、数字身份认证等领域。
它可以解决传统中心化账本的信任问题，提高数据交换的效率和安全性。

总的来说，分布式账本技术是一种基于分布式计算和密码学技术的数据记录和交互方式，通过去中心化和共识机制实现参与方之间的数据交换和交易。
它具有数据安全、透明、不可篡改、容错性高和可扩展性强的优势，被广泛应用于各个领域。

用Java实现一个区块链：
区块链技术可以使用各种编程语言来实现，包括Java。下面是一个简单的示例，说明如何使用Java实现一个基本的区块链。

注意，这个示例只是为了教育目的，以帮助您理解区块链的基本概念。实际的区块链实现，如比特币或以太坊，会更加复杂和具有更多的功能。

首先，我们需要一个Block类：

public class Block {

    private String hash;
    private String previousHash;
    private String data;
    private long timestamp;

    public Block(String hash, String previousHash, String data, long timestamp) {
        this.hash = hash;
        this.previousHash = previousHash;
        this.data = data;
        this.timestamp = timestamp;
    }

    // getters and setters...
}
然后我们需要一个Blockchain类，它会存储一个链表，链表中的每个元素都是一个Block对象：

import java.util.ArrayList;
import java.util.List;

public class Blockchain {

    private List<Block> blockchain;

    public Blockchain() {
        blockchain = new ArrayList<>();
    }

    public void addBlock(Block block) {
        blockchain.add(block);
    }

    public Block getBlock(int index) {
        return blockchain.get(index);
    }

    // more methods...
}

现在我们有了基本的区块链结构，我们需要一种方法来创建新的区块并将其添加到链上。这可以通过一个名为"mining"的过程来完成，其中我们可以创建新的区块并将其添加到链的末尾。
为了简化，我们只包括一个简单的哈希计算：

public class Miner {

    private Blockchain blockchain;
    private static final int HASH_difficulty = 2; // made this up, but in reality it would be a large number like 20 or 32

    public Miner(Blockchain blockchain) {
        this.blockchain = blockchain;
    }

    public boolean mineBlock(String data) {
        String hash = calculateHash(data); // this should be a function that takes the data and calculates a hash (using SHA-256 or something)
        if (isHashValid(hash)) { // check if the hash starts with enough zeros for the difficulty level
            Block newBlock = new Block(hash, blockchain.getBlock(blockchain.blockchain.size() - 1).getHash(), data, System.currentTimeMillis()); 
            blockchain.addBlock(newBlock); 
            return true; 
        } else { 
            return false; 
        } 
    } 
   private boolean isHashValid(String hash) { 
       return hash.substring(0, HASH_difficulty).equals(String.format("%" + HASH_difficulty + "$0", "0")); 
   } 
   // here we would need a function that calculates a hash from the block's data - for simplicity, we'll leave it out here, but in a real blockchain implementation, this is crucial! 
}

在这个例子中，mineBlock() 方法尝试添加一个新的区块到区块链上。它首先计算数据的哈希值，并检查该哈希值是否满足难度要求（在本例中，哈希值的前两个字符是否为零）。如果哈希值满足难度要求，那么它将创建一个新的区块并将其添加到链上。
否则，它将继续尝试直到成功为止。这只是一个基本的示例，实际的区块链实现将更加复杂。

区块链的加密技术和传统密码加密技术有什么不同？
区块链的加密技术与传统密码加密技术有一些关键的不同之处，这些不同主要是为了满足区块链分布式、去中心化的特性。以下是一些区块链加密技术与传统密码加密技术之间的不同点：

1. 去中心化的信任模型：
   - 传统密码加密：在传统的密码学中，通常依赖于中央授权机构（例如，证书颁发机构）来验证和签署数字证书，确保公钥的合法性。
   - 区块链加密：区块链采用去中心化的信任模型，通过分布式账本和共识算法来验证和记录交易。公钥的合法性通常是通过区块链网络中的节点共同验证的。

2. 智能合约和可编程性：
   - 传统密码加密：传统加密主要用于点对点的通信加密，通常不涉及可编程逻辑。
   - 区块链加密：区块链技术引入了智能合约，这是一种可编程的、自动执行的合约。智能合约通常使用特定的编程语言（如Solidity），并通过区块链上的节点进行执行。加密在智能合约中发挥关键作用，以确保合约的安全性和不可篡改性。

3. 共识算法和网络安全：
   - 传统密码加密：传统网络通常依赖于中央授权机构和传统的网络安全措施，如防火墙和入侵检测系统。
   - 区块链加密：区块链网络通过共识算法（如工作量证明、权益证明）来保护网络安全。加密技术用于确保交易的保密性和完整性，以及在共识过程中的节点身份验证。

4. 不可篡改的分布式账本：
   - 传统密码加密：传统系统的数据通常存储在中心化的数据库中，容易成为攻击目标。
   - 区块链加密：区块链使用去中心化的、不可篡改的分布式账本，每个区块都包含了前一个区块的哈希值，确保数据的安全性和完整性。加密技术用于确保区块链的不可篡改性。

总体而言，区块链加密技术强调了去中心化、智能合约、分布式共识和不可篡改性等特性，以满足区块链的独特需求。传统密码学主要用于传统网络通信的保密性和身份验证。
